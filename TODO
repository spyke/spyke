- try memmaping the .wave file to save RAM
    - open in copy-on-write mode in np.load(mmap_mode='c')?
    - worry about how shifted waveforms are resaved to disk on demand. Does the memmap need to be closed first, save with np.save, then reopen memmap with np.load()?

- also add CAR to SurfStream and SimpleStreams, and also to lowpass streams

- do proper low pass filtering (not just decimation) to get LFP data from wideband data
    - when exporting to lfp.zip, do slicing and filtering and decimation in the same 10 sec chunks of wideband data as spike detection, to avoid memory errors

- do whitening of raw data? done by Kilosort and spyking-circus

- add t-SNE as a dimension reduction method

- BUG: in some cases when opening streams and/or sorts of one polytrode layout, closing them, then opening one of another layout, the displayed channel arrangement in at least the sort window is wrong, or you might get a KeyError while trying to index into a channel that doesn't exist, because the new layout has fewer channels than the old one

- add ISI histogram to Verify->ISIs tab in main window

- NVS: make NDsep less biased when comparing a pair of very unequally sized clusters, by sampling the same fraction of spikes from each cluster, not the same number of spikes

- do a screencast of core features on sample data: detection, initial clustering, renumbering, viewing clusters in various spaces, projecting multiple clusters simultaneously, exporting. Should give voiceover

- during detection, check for any DC offset (perhaps due to ADC issues?) by randomly sampling each channel (maybe do this during the noise estimation), and subtract the DC offset from the raw data
    - this would be done automatically by the above "common average referencing" preprocessing step?

- instead of keeping time as a separate dimension, try including spike time or ISI as input into dimension reduction (PCA and/or ICA)
    - or maybe use log(ISI), like Delescluse2006

- add option to export multiunit spikes (those that aren't classified as "good", but also aren't "junk" nid 0 spikes) to .ptcs files. This could greatly increase the spike count when calculating MUA in neuropy. These could be exported to a neuron with ID -1 in the .ptcs file. To prevent any potential conflict, should add check to make sure there aren't any multiunit neurons (-ve numbered) in spyke that are also classified as "good".

- disable LFP Window button when lpstream is None

- when selecting points in the cluster window, instead of only displaying in the sort window the first 200 waveforms selected, treat the plots like a stack, removing the oldest selected ones to make way for the newest selected ones, keeping the max displayed the same at 200. This would allow for easier exploration of waveform features of cluster lobes.

- add option to align to max -ve (or maybe +ve?) slope - good theoretical basis, since extracellular spike is the derivative of the intracellular spike, and the max extracellular slope should correspond to the max sodium current

- JMS: try optionally adding spike time to the input data matrix before sending it off for dimension reduction. This means that spikes that cluster in time may somewhat cluster in PCA/ICA space, and may reduce the need to plot against time. Could help when you have a cluster that stretches out over a long period of time, and for parts of that period, looks like it needs cleaning/splitting, whereas other parts are fine. If spike time and some waveform feature correlate, they might cluster out.
    - maybe add a checkbox next to the tis dropdown box to toggle this feature, to indicate that time, in addition to waveform data on the selected time range and channels, will go into dimension reduction. Or, maybe it should go in the cluster tab somewhere around the dimension reduction dropdown box

- try adding ISO distance (Harris2001) - need to calculate Mahalanobis distance, which requires the covariance matrix of the cluster, which assumes the cluster is Gaussian. Consider adding Lratio (Schmitzer-Torbert2005) as well

- consider using projection pursuit + Silverman's critical bandwidth (Menardi2012) instead of PCA or ICA to maximize clusterability during dimension reduction
    - this seems to be too slow, and so far hasn't shown any ability to improve clusterability on any of our data 

- consider implementing Ester1996 - use k-dist metric to help automatically decide the most useful value of sigma

- cluster window dimensional gain, maybe have spin box or combo box next to each of x, y, z in main spyke window - this would be eqv't to having independent sigmas per dimension

- add ability to restore a spike's channels to those chosen during detection: take maxchan, choose chans within the inclusion radius, reload wavedata. This would effectively allow undoing the replacement of spikes' chans with meanchans of the nid to which they belong.

- key bindings:
    - Ctrl+D in cluster window deselect the cluster under the cursor, if it's selected

- maybe each neuron's dt value (time difference between the two peaks) should be the mean of all its constituent spikes (a float), not simply the (integer) value of the mean template

- NVS: as a cleaning step (not alignment step): take rms difference between spike and template across all temporal shifts (in my case, 5 of them), find shift with min rms difference, as before. Then, take that shift and the two on either side (at least one of which is already calculated), and take the 2nd derivative of all 3 (left - 2*mid + right). Then, plot distribution of these 2nd dtive values, see if get some bimodality. NVS says that even with a threshold that's constant across clusters and even .srf files, this is a great way to pull out crap spikes that don't match the template very well at all, ie have very little sharpness in their rms fit, even at the best shift.

- maybe I should freeze scouts only if they haven't moved less than a threshold distance some threshold number of iterations, as NVS does. Currently, I freeze a scout as soon as it moves less than 10^-5 in a single iteration. This could be pretty easily done by changing the still boolean array into an int array, and then instead of checking whether it's true or false at certain parts of the code, check if its still count has reached threshold

- consider implementing NVS's 2nd derivative weighted measure of template center, which effectively aligns to the sharpest peak in the template. All the member spikes then go along for the ride. Good for accurately comparing templates.
    - also, consider using it on a single spike basis? maybe this could be a more principled replacement for my own sharpness measure

- add a "clusters only" checkbox or toggle button next to the cluster plot button, which plots only the selected clusters and no spikes, probably using some kind of spheres instead of just singular points

- add time in hours and time in fraction of stream trange to tooltips, as alternatives to time in us

- would be nice to have a translucent plane in the cluster window representing the current time position of the spike slider in the sort window, if time is one of the plotted dimensions

- I'm often duplicating my efforts, comparing the same pair of clusters over and over, even when they haven't changed their spike populations, like when say they've been renumbered and recoloured. This is especially bad when a pair isn't obviously different, and needs to be plotted on a specific set of chans, and maybe needs ICA and/or time. I need two things to solve this:
    1 - a way of designating that a pair of clusters is significantly different from each other. Then, when I go cycling through clusters most similar to current one, those that are significantly different should be highlighted differently somehow
        - maybe the S key in the sort window can be used to designate a pair as significantly different, shouldn't conflict with S key in cluster window for selecting spikes under cursor
        - maybe the "S" button on the sortwin toolbar should be togglable, and only enabled when exactly two clusters are currently selected. Then, the user looks at the state of the toggle button to find out if they've previously been found to be significantly different. Could also colour significantly different pairs say black when highlighted
    2 - a way of keeping track of pairs when their IDs change, and a way to stop keeping track of them when either of their member spikes change. Might use hashes for this somehow. Maybe take the hash of the set of sids of the neuron pair, use that as a key into a dict, and store a boolean to represent whether they're significantly different or not. Could also store the overlap index, overlap area ratio, what chans and what dimensions they were deemed significant on. How expensive is it to be constantly calculating hashes of thousands or hundreds of thousands of integers?
        - to be really careful, should hash each neuron's sids separately, then hash the hashes. That way, if two neurons have swapped some spikes, the hash is different, which is what we want
        - a package called joblib might be useful: http://packages.python.org/joblib/memory.html
        - looks like hashlib.md5(arr).hexdigest() is lightning fast, even on 1e6 length numpy int arrays. This is good! Looks like it only works on the data, not the shape or data type. md5 isn't secure against attacks, but it's fast and that's all I care about
        - note that order matters in a hash. All sids in a neuron are sorted, so that's fine, but then I need to be consistent about hashing the two hashes together. Maybe hash them in order of which has the lowest id spike.
        - store a dict with fields:
            - significant: overlap index/separation index? or just boolean? maybe just boolean, since overlap/separation index can always be recalc'd given info below
            - CA: component analysis (PCA, PCA/ICA, ICA)
            - dims: x, y, z dimensions
            - chans: arr or list of selected chans

- why don't I have an alignment rule during detection that says "if this spike's sharpest negative phase comes after the sharpest positive phase, align to the positive phase"? Maybe because for some spikes the sharpest -ve phase will vary between the first and second, and you'll misalign by a whole lot. But then, all I do now is align to the sharpest -ve phase anyway, which can result in the same misaglinment. So, maybe it would indeed be good to limit the misalignment to a shorter time span by choosing to align to the sharpest +ve phase if the sharpest -ve phase follows it. That way, you know you're at most only one phase away from the correct alignment. The way it currently stands, you can end up two phases away from the correct alignment.
    - ah, but this wouldn't work in the case where there isn't much of a +ve phase to speak of, ie, a mostly single -ve phasic spike. Or, the case where there are two significant +ve phases, but they alternate from one spike to the next in terms of which is sharpest

- would be nice if PCs/ICs weren't recalculated when you're going through and manually plucking spikes out using SHIFT+DEL. This doesn't seem to happen the first time, but it does the second time, for some reason. This doesn't make any difference to the display in PCA, because all the same points are still there, just differently coloured. But, it does make a difference during ICA, because ICA is not deterministic. Either way, if you have a lot of points plotted, recalculating can get annoyingly slow. Or, maybe it's the recalculation of the mean waveform that's slowing things down?

- might also be nice to not auto-select cluster 0 after doing a SHIFT+DEL. Just leave the original cluster selected, so you can continue cleaning it out

- highlight currently selected spikes in cluster window somehow, assuming they're plotted in cluster window. Maybe make them 2x2 pix instead of just a single pixel?
    - this would also make manual spike selection in cluster window much more useful. You'd have visual feedback about which points you have and haven't selected!
    - or maybe invert the point's colour, or change its shape or something?
    - maybe just use a colour that isn't already being plotted. Make a list of bright colours, and cycle through them until you find one that isn't being used. Could sort existing colour table by brightness, sum RGB values for each entry to get brightness
    - then of course when you hit ESC to clear the selection, you have to restore the original colours/shapes
    - maybe when selecting specific spikes, desaturate the unselected ones all by a set amount, and leave the selected ones saturated. Then when you clear the selection, set them all to be saturated again

- make brush visible when the "S" select key is held down in the cluster window
    - adjustable size brushes for spike selection might be nice too

- if num selected spikes is <= 1 and you hit cluster, print an error and return

- NVS says Vpp**2 fits a Gaussian better than Vpp. Maybe I should try this during spatial localization

- to improve axes contrast against data points, especially for mini axes, first draw three fat black lines, then clear depth buffer, then draw the 3 coloured lines on top

- make separation index a proper metric somehow? From -1 to 1, and if it's < 0, the distribution pair has poor seperation?

- to prioritize pairwise comparisons of similar clusters, find all rms error values between all local pairwise clusters (local could be constrained to something really small, like 50um), and then sort them in increasing error. This could be controlled via CTRL+> and CTRL+<, and I guess would have to be recalculated on every keypress, just as it normally is, since in between you might merge/split/renumber clusters

- sometimes you get spikes with rather different maxchans, and therefore very different sets of chans, that belong together in a cluster, yet it's hard to ensure they really do because they share so few chans. This seems to happen most often for spikes clost to a dead channel on the polytrode. Have a button that reloads all the currently selected spikes such that they all share the same superset of chans. However, there is a max num chans you can include for each spike, and that's hardcoded into the wavedata and spikes array. So you'll have to prioritize the chans to store. Sort the potential chans in order of increasing distance from the mean of the selected spikes, or the chan closest to that mean. Then, ensure that each spike's maxchan is included in this new superset. If not, for any such spikes, replace the most distant chan from the mean with the maxchan. That way you ensure that you never lose the spikes maxchan as determined during detection.
    - don't forget to also update chans and nchans for each spike
    - the easier, though less correct thing to do, would be to consider spikes to have nothing but zeros on chans outside of their chanlist
    - even easier would be to increase the inclusion radius a bit and redo detection
    - easiest of all would be to always load all maxnumchans of data into the wavedata array for all spikes

- when loading a .sort and .spike file, in case of missing .wave file, have ability to reload all wavedata from .srf file(s). This way, huge .wave files need not be backed up, they're really only there for speed and convenience.

- I think for num changed spike waveforms > some threshold, say 50k, it might be faster to resave the whole .wave file, instead of updating just the individual entries one at a time

- semi-automate pairwise cluster comparison, making use of one or more of the overlap measures
    - designate an overlap threshold. Go through all pairs. For the first pair that exceeds the threshold, print a message, select them in the interface, and leave the user to examine them in more detail. Maybe first try each pair in PC space, and if they exceed overlap, try again in IC space, and if they still exceed, turn it over to the user, who will perhaps select a different set of channels to help distinguish the clusters.
    - whatever overlap metric I use, once it works, I can use it to optimize first for the channel selection, and then for value of sigma that best separates the two clusters

- make ENTER in nstds field activate the clean button

- when calculating matches, calculate not just against unclustered spikes, but also against all multiunit spikes, ie all spikes with nid < 1

- need a quantitative criteria for deciding if a cluster should be classified as multiunit
    - some kind of Vpp of template to stdev ratio, something akin to signal to noise ratio

- seems the Qt nlist labelling bug, where 3 digit nids are replaced by ... when there are -ve numbers in the list, seems to go away once you hit values around 120, maybe cuz the two takes up more space?
    - or, maybe it's when the negative numbers start hitting 2 digits

- JMS: add time to PCA to help with clustering over time with drift

- maybe I should designate a cluster's physical position in x and y by running localization on its mean waveform, instead of relying on averaging the physical positions of its constituent spikes. Might make sorting in order of y position a little more reliable wrt keeping clusters with similar mean waveforms close to each other

- maybe I should mark sids as dirty for the .spike file too
    - would be more difficult to mark spike file entries as dirty, since they become so so much more often and in so many different ways. But, maybe the only code change required would be in one or two places, like in apply_clustering.
    - seems like .sort file saving is the limiting factor now, not .spike file saving. The time and space hogs are probably the neuron.sids arrays. I could try excluding those during save, and then regenerate them from the .spike file on load. Would inevitably lose any neuron 0 though.

- make right click on spike(s) in nslist bring up context menu with delete, or just make CTRL+Del delete the currently explicitly selected spike(s). This wouldn't be undoable though, with the current undo/redo framework being so based around creation/deletion of clusters. It would also screw up the undo/redo stack.

- if exactly 2 clusters are selected, and one or more spikes are selected in the nslist, have some control that lets you swap the cluster membership of those spikes. Maybe the tab button, since that has a bidirectional arrow. This wouldn't be undoable though, with the current undo/redo framework being so based around creation/deletion of clusters. It would also screw up the undo/redo stack.

- something wrong with find next/prev most similar cluster, channel overlap calc is wrong? not finding obvious candidates. Might be a new bug from adding multiunit clusters?

- should I really be scaling all data to have unit variance before feeding to PCA? PCA only requires 0 mean, according to wiki
    
- WORRY?: climbing.climb() returns int32 nids, while the spikes['nid'] field is int16

- add recalc for current cluster button to match tab - useful when you merge in some spikes and want to see what the plot has changed to, and what the next best usids are

- finish implementing discretized cluster space using huge sparse matrix

- NVS: try using x nearest neighbours instead of sigma, and to keep scout points from getting stuck in local density maxima, add a bit of noise (but how much? some fraction of sigma?) when they stop moving (or just by default on every iter)

- WISHLIST: add vertical line threshold indicator to match error histogram, and make it user settable, and keep it synchronized with the match thresh spin edit in match tab

- WISHLIST: maybe if you're plotting in PC space and you change cluster selection in nlist, the cluster plot should automatically replot (saves you from having to constantly go back to spyke frame and hit plot button when cycling through neurons). But, doing so might be slow on big sort sessions

- renumbering is quite slow for track wide sorts with millions of spikes, not sure why

- maybe to get around drift in a track, remove (or somehow discount) time gaps between recordings, so that gradient ascent doesn't stop at recording edges. Also, maybe doing maxchan based PC clustering with time as a 3rd or 4th dimension could help with drift. Might be difficult to tease out the difference between a multiunit cluster and a single unit cluster that's drifted. Could use a check of some kind that chops a cluster up into appropriate time chunks, and tests for multiunit vs singleunitness in each time chunk somehow

- move plotting controls to their own plot tab, free up space on cluster tab for Nick's automated subclustering using range of sigmas and calculating clusterscores
    - or, might be more convenient to have plotting controls as toolbar in cluster window

- on .gdf export, popup dialog asking for which DIN value(s) represent start of stimulus condition(s) (including pauses/blank screens), so their times can be injected at the correct lines in the .gdf file. List all DIN values by default

- when trying to decide which chans and which timepoints on each chan to use for clustering (or for feeding into PC), for each timepoint, plot a histogram of the voltage values for all the spikes, normalize it to get p values, and then calculate entropy. The max entropy comes from a Gaussian distribution, so any entropy values less than that are less Gaussian, and probably more clusterable. Isn't all this work just replicating what the KS test does?
    - actually, this won't work, because sum(plogp) doesn't care what order the p values come in in the distribution. So, if you took a Gaussian (which has max entropy of all distribs), split it in half, and flipped one half, you'd get the same entropy, but you'd now have a sort of bimodal distrib. Looked at distribution of noise in a nice cluster, at both t=0 and t=0.5ms (start and midpoints), and both fit a gaussian really well. So, to test for clusterability of channels/timepoints, probably best to use ks test and compare distribution of candidate point/chan to that of a gaussian. Probably have to normalize both to 0 mean and unit variance.

- WISHLIST: keep a record of all cluster operations performed (and all params used for each clustering), and store them in the .sort file, in sequence in a list, so the history can be examined and reconstructed. This is already somewhat implemented via the cluster changes, although those don't store clustering params

- convert all line endings to Unix style

- BUG: opening a sort from one track, and then opening a sort from a different track causes channel display problems. There must be some channel info that isn't cleared between sort openings. Maybe some attribute of the main window or the sort window? Maybe something to do with self.windows['Sort'].panel.enabled_chans?

- what happens when a recording has more than 1 experiment in it? I know it'll have as many textheaders (displayrecords) as there are experiments, but what about din (digitalsvalrecords)?

- NVS: to choose at what point to align each spike during detection, take the 2nd derivative along the spike, weighted by the voltage, weighted by the waveform point index, add all those up and normalize by the number of points, and you get the point index to align to. This tends to be right around the 0 crossing. What's nice is that it's paramaterizable/linear - a small change in some voltage value results in a small change in how you align - there's no 2AFC between align to one peak or the other. He claims it results in better spike alignments within clusters right off the bat.

- WISHLIST: numpy 1.6 now has float16 support! Could conceivably switch from int16 to float16 and get rid of all the conversion stuff, but then other programming languages don't necessarily have float16 support, and so the .wave files wouldn't really be compatible. Plus, I'd have to convert all of my existing int16 .wave files into float16. And, integer math is faster, so I should really stick with int16s for waveform data

- WISHLIST: consider max/min/best aligning each channel separately, or aligning according to selected chan. This would mean shifting chans relative to each other

- WISHLIST: if single selected neuron is the same colour as the channel selection colour, change to different channel selection colour (alternate between red and green)

- WISHLIST: allow undo/redo of renumbering, and only trim cluster change list when it exceeds some maximum, like 50 changes, or after saving, to preserve memory

- WISHLIST: allow loading of .srf file *after* loading of .sort, with proper association of .srf to stream.srff.f
    - also allow closing of .srf file, while leaving .sort open - for when you need to disconnect external drive

- possible waveform error measures:
    - rms - seems to work well enough
    - 2nd derivative at peaks (or of the whole waveform?)
    - distance measure, which would take both voltage and time difference of waveforms into account - sort of need to measure the area between each waveform pair

- NVS: store density function lookup table (with arbitrary fineness) for speed in move_scouts() - that way, you don't need to calculate d**2, or exp(), or anything. You just give it distance as integer index into an array, and you get your density f'n value as output

- color raster lines according to neuron color, assuming neuron id isn't -1. If so, then just leave it as same color as the max chan. Or, maybe color it grey? If a clustering has been done, maybe yes. If not, leave it colored by maxchan

- use QActionGroup in the code to turn sampfreq menu checkboxes into exclusive radio items (can't do this directly in designer apparently)

- themed icons - can do it now using QIcon.fromTheme(), but should be much easier once it's integrated into Qt Designer in 4.8: http://bugreports.qt.nokia.com/browse/QTBUG-7777

- turn uslist (and maybe nslist) into multicolumn table, or just use tooltips to display other params, and a context menu to allow you to sort by those params

- make data and sort pane tooltips instantaneous, ie use mouseMoveEvent like for cluster pane
    - this would also remove need to set focus to the appropriate window to get the tooltip. All you'd need to do is hover

- maybe autoscroll nlist to most recently programatically selected cluster?
    - no need, now that they're more efficiently displayed in LeftToRight flow mode

- remove "cid" as much as possible, maybe even altogether, maybe even from spikes array. Replace with "nid".

- populating an nslist with 100s of thousands of entries is slow, and uses 100% CPU. setting UniformItemSizes to True and layoutMode to Batched helps. Messed around with fetchMore and canFetchMore and rowCount, but that's just a bad hack (see fetchMore Qt example - scrollbar doesn't work properly, can't easily scroll to the end). It seems there's no way to display a huge list immediately, irrespective of the number of entries in it, like you can with Wx's virtual list. Very disappointing.
    - why does nslist take longer to populate than a same-sized demo listview in listview_test.py?

- when hitting 'C' or 'X' to focus current cluster or spike, if more than one is selected (for either type), take average position of all those selected and focus that average position

- QT bug: setting both QListView.setLayoutMode(QtGui.QListView.Batched) and QListView.setResizeMode(QtGui.QListView.Adjust) at the same time often causes endless recalculating of layout with flickering and 100% CPU, until you happen to resize the list to a point where it stops doing so
    - maybe setting batch size to something bigger will work around this
    - yes, this works, but it's just a hack

- QListView: keyboard navigation doesn't wrap when Wrapping is enabled
    - see http://www.qtcentre.org/threads/38355-QListView-keyboard-navigation-doesn-t-wrap-when-Wrapping-is-enabled
    - posted to list, didn't get a reply

- why are neuron.sids int64? shouldn't int32 be enough?

- consider deleting all lowpassrecords from srff when saving to .sort. They take up half the space in a .sort file. Could do the same for highpass and lowpass records

- add DeleteAll for clusters, maybe Shift+Del. Same for Shift+Del GUI button?
    - this should also delete all undo/redo history, to save memory and time

- TrackStream: when exporting, keep the offsetted timestamps intact, and offset the corresponding .din files appropriately?
    - problem: you'd need two sets of .din files if you also wanted to sort just a single or a few .srf files at a time, or if you wanted to sort a track starting from different .srf files
    - might be best then to export spike times not offsetted, ie relative to their respective .din files? Could also export the int64 us offset time for each recording in a separate file, say t0.bin. Then, if you really wanted to combine spikes from all recordings in a track in neuropy, you could use the relative t0.bin values to add the appropriate offsets back to the spike times

- TrackStream: to deal with different intgains, find recording with biggest intgain, call that value maxintgain. For each recording, scale its values by its intgain/maxintgain when returning a slice from its stream. Note that this ratio should always be factors of 2, so all you have to do is bitshift, I think. Then, have a single converter for the trackstream whose intgain value is set to maxintgain

- check that the sort file you're opening came from the srffname, or list of srffnames that are currently open

- would be nice to have drag and drop of .srf and .track files onto the main spyke frame perform identically to the manual File/Open scenario, especially for files that are on an external drive, that File/Open doesn't default to

- localization during detection makes detection take 60% longer, which seems like a lot. Maybe use Cython to speed up fitting of each spike's waveform? The passing of a Python f'n to LM and constantly calling it as it iterates is probably the slow bit. So, this means all of LM would have to be rewritten in Cython?
    - yes, but only because the SciPy implementation is from LMPack, which is Fortran. If I could find a C version, I could easily interface into that with Cython, and not have to rewrite the algorithm.
    - could also try specifying an analytical Jacobian instead of forcing it to be estimated on each call (ie for each spike that needs to be localized)

- autosave .sort file every 10 min or so during detection? and then what, how would you resume detection if it messed up previously? Doesn't seem very feasible

- NVS: Nando said it would be good to get an energy f'n going for the gradient ascent clustering, so you can quantitatively evaluate which clustering is best, and therefore run the algorithm with multiple values of sigma (and maybe with different dimensions) and automatically choose the one with the lowest energy
    - sounded like it could be some kind of measure like the sum of all the distances from each point to its scout point/cluster center

- sort window status bar: for current cluster: cluster number, num spikes, SD on maxchan

- do test for double triggers (sometimes a spike with a weird delay across channels will cause this): check each ISI across the whole list of sorted spike times. For each ISI that falls below, say, 0.5 ms, check the spatial separation of those two spikes, and if that falls below, say 100 um, or maybe 1 channel distance, then flag that as a double trigger, and maybe delete the second spike? Or keep the spike that's aligned to the biggest phase? This means resizing the spikes array each time, and probably some other stuff. All of this should really be done right after detection, and before any clustering
    - another way to test for this would be to do best fit alignment on all spikes in each cluster, and then see if the waveform difference between any two temporally adjacent spikes is exactly 0. That would indicate you've counted the same spike twice.
    - see spikes 247217 and 247218 in ptc15.92 detection as an example (at t=1286824840 and t=1286824900 respectively). Should turn on detection debugging and figure out why the phasetis for 2ndary chan 21 for the first spike are only 3 timepoints apart (28 and 25), which almost certainly messes up the lockout on that chan, and causes the double trigger

- seem to still be triggering off the latter part of some spikes whose first real phase is, I guess, not sharp enough, or not big enough, and then these are thrown out during cluster cleaning

- (semi) automate cluster cleaning somehow

- maybe integrate STA into spyke for quick look at RFs of what you just clustered, and especially, how the RFs may have changed with the last clustering change - ie, keep previous RF plots open for comparison

- set y site coords to be -ve, ie still starting from 0 at top of probe, but then decreasing to -ve numbers as you go down the probe - this would make plotting simpler - top of probe is at top, and bottom is at bottom - no need to reverse the y axis

- sometimes weird spatial localizations going on - try Cauchy distrib instead of Gaussian
    - why not try adding an amplitude offset param to Gaussian model? This could be all the heavy tail I need...
        - actually, I think I tried this (soff and toff), and presumably it didn't work very well

- if doing a subclustering results in an overall reduction in the number of clusters, try and merge into the existing available cluster numbers - don't just delete the old ones and add new ones to the end

- after happy with all clusters and their cleanliness, maybe go through all thrown out spikes, and do template matching of each one against each of the clusters. Set a very strict error threshold, but if an unclustered spike falls below it, move it to the cluster that matches best. Sometimes during waveform clustering (cleaning), good spikes can get thrown out by accident, though I don't think this is a very huge problem

- add feature where you do a clustering of only the unclustered points in the data (unclustered is defined as cid=-1 or nid=-1, or both?
    - refinement would be to do clustering of only those unclustered points near the current focus

- in addition to y location, add ability to renumber clusters according to:
    - Vpp
    - nspikes

- add ability to cycle through plotting all of a cluster's waveforms, 50 or 100 at a time, ala NVS, to more thoroughly check how clean the cluster really is
    - maybe select a range of spikes to display, and then have a keyboard command that lets you scroll that selection range up and down by half its length. So, for example, you select spikes 50-100 by hand, and then hit Alt+Pgdn (or Alt+down) to change selection to spikes 75-125. Or there could be no overlap too. If initial selection is less than say 25, make Alt+down select at least 25 at a time.
    - going Ctrl+PgDwn or PgUp in the sort window does the same thing, 50 at a time. Works well enough

- try leaving spikes aligned to their sharpest peak by default instead of their min peak, this might make cleaning by maxchan waveform shape more effective

- clicking on blank part of listctrl, thereby deselecting all, doesn't trigger a selection event
    - try binding OnClistSelect and OnNlistSelect to each and every click in the list?

- error measure ideas:
    - weight each spike in cluster by 2nd derivative of the cluster mean
    - take shortest distance (in voltage-time space) to template, instead of just voltage
    - or simply weight each spike by the cluster mean, so error in signal far from 0 is more important than same error close to 0

- temporal spike modelling
    - try doing piecemeal modelling ala spatial modelling, see if that gives better values (or have I tried this already?)
    - if either of the sigmas are say > 200 us, assume that such a sigma should be ignored, and that the spike is effectively monophasic
        - luckily, it always seems to be the 2nd sigma (s1) that sometimes is fit at a really big value, so for clustering, the s0 value should be fairly reliable as is - is this really true?

- normalize spike density by Gaussian volume, and measure density across all spikes within rneigh, not just those that belong to the cluster - this should make densities more comparable across multiple climb() runs
    - not necessary now that I've turned off density thresh

- to ignore correlated ripple, add something during detection that checks if signal is too distributed over too many chans to qualify as a spike - that way you could lower the median noise multiplier a bit and catch smaller spikes, while ignoring bigger ripple

- add ability to add notes to a .sort file, where you can mention sorting details, like problems encountered, or where the set of neurons were imported from (if at all)

- need dynamic scaling of dimensions for visualization (some sort of GUI for it), and also need to fix problems with changing scaling when a cluster has a non zero ori - could be tricky
    - actually, maybe all you have to do is make ori represent ori in real space (unscaled, not visualized), and then when you scale any of the dims, take the arctan of the scaled dims to get the visualized ori, but make sure to always store the real ori in the .sort

- Nick's idea: represent Vpp on a log scale - this might make things more clusterable in the Vpp dim, especially since some of the spikes can reach really big amplitudes due to instabilities, and those stretch out clusters (maybe non-linearly) in the Vpp direction
    - doesn't seem to improve clusterability, although Nick says it does for him

- rev646 (switch cluster list to a virtual listctrl) broke spyke in linux
    - hangs whenever you call clist.RefreshItems(). If you change clist from a normal wxLC_list style to wxLC_report style, the problem goes away. Maybe RefreshItems() doesn't need to be called at all for a non-report style?

- check if num plot slots has reached some ridiculous value (say 100), and stop adding any more, raise an error - this prevents the user from accidentally selecting 1000s of spikes for plotting, by going shift+end in the nslist

- add random numbers (up to 1/2 a dphase bin width: 20 us/2 == 10us) to the dphase values to dither them out a bit, and make it more obvious how many fall into each dphase bin, so you can actually tell what the distribution looks like in dphase space, instead of having lots of overlap at each of the discrete dphase values

- might be useful to hide clustered points, so you can focus on just the unclustered ones that are left
    - not sure if this can even be done, given that all points are part of the same single glyph

- colour items in nlist and clist - declare OnGetItemAttr method (see wx demos)
    - do it in the cluster list as well - maybe make that a proper listctrl
    - this would require a black background for the listctrl, otherwise white wouldn't show up

- nid tooltips are a bit slow and not always reliable

- maybe keep rendering disabled during the whole .sort restore process - it's currently disabled when calling mlab.points3d

- should really set up neuropy to import the spikes from the .sort directly, but can make it still rely on an existing .din and .textheader file, since those would have to be pulled from the .srf file anyway

- rethink the whole "best" thing for marking which sort to use - maybe just leave it be with no best marker any more, and add a sort level to the command line neuropy interface - nevermind, its already there, but the sorts are only numbered, not named
    - also, you can't go sortN.e0.sta(), since sort is below exp in the hierarchy - this is awkward - rethink the hierarchy?
    - tree printout should show the sorts and experiments?
- get neuropy+dimstim working in Python 2.6

- MAYBE: add ability to turn on/off a cluster (and its coloured points) temporarily, so you can see the uncoloured density of points within it, and maybe better decide how to adjust the cluster to match
    - cluster button: hide unselected clusters
        - or just a hide/show button, which toggles the visibility of each of the selected clusters
        - or best: separate show and hide buttons that set the visibility of each of the selected clusters
            - should this leave point colours unchanged? probably, otherwise toggling would get really slow
        - this would also prevent distracting tooltips from popping up

- overplotting speed

- replace unnecesary use of list(set(array)) with np.unique(array)

- open .sort with no .srf, close it, open a .srf, get no sort.st attrib error when updating rasters - no slider shows up as a result?
    - maybe soln is to turn off rasters every time you open a .srf, and then enable them every time you open a .sort?

- detect, cluster, do new detection just following first one in time, cluster, then delete first one, get an IndexError

- deleting a detection doesn't clear neurons' waveforms, but maybe that's not such a bad thing?

- selecting a block of spikes in list doesn't plot them - might be due to bugfixes in latest wx, might not need my hack workarounds anymore?

- clear/update neuron mean waveform and replot it immediately (if currently plotted), when updating its cluster members, ie moving/modifying the cluster and then hitting "c"
    - close .srf file, open test.sort with no waves in it, modify cluster, apply it, yet mean neuron waveform remains unchanged from when it was last calculated when the .srf file was open



------------------------
OLDER:

- reduce number of chans included in a spike according to amount of signal on that signal, maybe based on median signal for that chan for entire file, or randomly sampled subset of the file

- order colours consecutively according to cluster mean y location, to make neighbouring clusters in X-Y space less likely to be assigned the same colour

- WARNING! TODO: not sure if say ADchan 4 will always have a delay of 4us, or only if it's preceded by AD chans 0, 1, 2 and 3 in the channel gain list - I suspect the latter is the case, but right now I'm coding the former

- add NVS's one pixel row per channel display window, with greyscale of colour hue indicating voltage - see more recognizable patterns, can see spikes over the span of just a few pixels (look like gabors over space and time), yet can also look across a wide swath of time to find similar looking gabors elsewhere - interesting

- Nick's suggestion: try smoothing across channels. For each timepoint, for each chan, weight the signal at that timepoint with 1, and all the others for other channels as a function of a gaussian. Then normalize to get the smoothed signal value for that channel for that timepoint. Then, repeat, centered on each of the channels, and repeat for all timepoints. This preserves signal they have in common, and gets rid of channel-independent noise. Drawback is that you lose spatial resolution for your spike: a spike will appear to spread across more channels than it did before smoothing. This tradeoff can be tweaked by changing the sigma of the 2D spatial smotthing gaussian. Looks pretty good in his implementation.
    - might be a bit slow, but could just be implemented as an extra step before or after interpolation. is equivalent to convolving with some kind of spatial filter? maybe this is already implemented somewhere in numpy or scipy

- for convenience in neuropy, copy all stimulus data from .srf to .sort file

- think there's an issue with the cursor width not updating when the window it represents (the one to the left) changes its zoom level

- add CSD calculation and display

- get tooltips to work on main spyke frame widgets

- make ESC in tree and event list clear the current selection

- when assigning an event to a plot, change the zorder of the lines such that the maxchan is always on top, only really need to deal with the maxchan, which can sometimes be big enough to span over to the screen real estate of other chans
    - can make this a template method too, such that it sets the zorder for all lines in constituent events to be the same as it is for the template mean

- hovering over waveforms should highlight the closest datapoint on the closest chan with a circle point or something, while displaying its time and exact voltage in the tooltip - this would probably be more useful than displaying the voltage in the potentially empty space under the mouse
    - should do this both for spike window and sort window (not really necessary for chart or lfp, might be slow, but might be nice too), although in sort window it wouldn't be clear which waveform to do it on, for overlapping waveforms

- plot and control thresh level on spike and chart plots
    - use picker - when a thresh line is picked (within a tolerance of a couple pix), change mouse cursor to hand or vertical sizer, and while button is down, adjust position of thresh lines and value of the manual thresh level in Events tab

- separate groups of 3 digits in spin edit and end labels with commas (time in us)

- add 1ms wide chart window to sort window, so you can see chan layout of events in both real space and chart space, might make it easier to sort

- combined message and stimulus DIN window:
    - messages: maybe just a combo box with timestamp:message entries. Choosing one of them skips you to that timestamp. Changing current file position with other methods should automatically scroll you through the combo box at appropriate times
    - stimulus din: similar to above: drop down combo box with timestamp:DIN value entries. Again, changing current file position with other methods should automatically scroll you through the combo box at appropriate times

- stop forcing the main window to position itself at (0, 0), check what the upper leftmost available position is - for someone like Nick with the taskbar along the top of the screen, it's more like (0, 30)

- this might speed up plotting even further: have only one actual Plot object per plot panel (the quickRemovePlot), and then another that's just a shell for temporarily holding plot attribs (data, colour, style) which are updated, drawn to buffer, updated, drawn to buffer, etc. and then blitted (this is actually a lot like having axes.hold = True)
    - I think this would vastly reduce the number of Line2D objects needed - initing them takes a long time, and once they've been inited, they sit there taking up memory
    - not sure how this would affect used_plots and available_plots
    - seems like a good solution for overplotting thousands of plots in the error threshold window
        - or maybe better thing there would be to only overplot on top of the template mean say the 10 or 20 events with the highest errors that fall below the current error threshold

------------------------------------
INTERPOLATION

    - when resamplex > 1 and shcorrect == False, you only need resamplex - 1 kernels. You don't need a kernel for the original raw data points. Those won't be shifted, so you can just interleave appropriately

    - take DIN channel into account, might need to shift all highpass chans by 1us, see line 2412 in SurfBawdMain.pas

    - could use speed improvement

    - only bother displaying interpolated data for spike frame - not necessary for chart and lfp frames, and will conserve update speed to a large extent compared to uninterpolated mode

------------------------------------
EVENT DETECTION:

    - LOCKOUT: do I propagate any spatiotemporal lockouts from the end of one searched block to the start of the next?  I'm taking slightly overlapping blocks of data to make sure I don't miss any spikes that fall on the borders, but I'm not sure if I'm propagating lockouts...
        - pretty sure the answer is no right now

    - maybe instead of locking out, I should subtract the modelled waveform from the raw data, and allow the event loop to test all its detected events
        - this might help in dealing with overlapping spikes
        - maybe I should retest for new events in the subtracted data that might come out
        - somehow make sure that when subtracting off the model from the raw data, that you're always bringing the raw data values closer to zero, no matter what. Have to do some abs()-ing somewhere, I suspect, or at least some checking of values > 0 or < 0

    SPIKE MODELLING:

        SHORT TERM:
            - check to make sure that if model gives you a single phase spike (one of the phases is pretty much 0 amplitude), that you don't dismiss it as a false event, just because the two phases (negligibly) have the same sign in their amplitudes - ie, don't mistake what might be essentially single phase spikes for double phase spikes with the same sign (up up, or down down)
            - check modelled y0 - if outside the y coords of given chans, throw the spike out - this is probably better than putting constraints on the modelled y0
            - is x0 unfairly weighted according to how many chans each col has? eg, if left col has more chans, is x0 biased to the left?
            - store param names in a list of param strings sorted the same way as values in sm.p, override SpikeModel.__getattr__, check if attr is in the param string list, use its index to return the value in sm.p. That way you can go sm.mu1 without having to worry about param order changing in the future
            - log detection and modelling printouts to a file, search through that instead of having to search through lousy winxp terminal


        - except for the initial threshold test, instead of directly checking raw voltage values to see if they exceed certain values and when that occurs (like a window discriminator), how about this:
            - when you find a spot that exceeds the initial threshold, fit a spike model to the signal on that channel and each of the surrounding channels within slock. The spike model will be biphasic (or maybe triphasic). Now, of your modelled channels, find the one with the maxchan at the peak of the initial phase, model any further channels that might fall within the slock of that new maxchan, and search one last time for the maxchan within slock
            - model parameters: overall sign (+ve sign would be say a normal initially downward going spike), overall width (up to 1.5ms say), and amplitude of each phase (phases will be forced to alternate in sign)
            - once you've found your best fit model (which will necessarily be smooth), use it to decide if it qualifies as a spike (ie, use it to check amplitude of its second phase), and if so, to decide how long to lock out for
            - this would be a kind of denoising, and would get around weird cases where the spike is distorted, which if you look at raw values, screws up your decision of where the peaks are and how long to lock out for
                - this would probably get rid of the spike alignment problems when matching templates
            - should the model be a Gaussian windowed polynomial of order nphases + 1?
            - how best to go about actually fitting the model? what's the numpy/scipy algorithm for this?
                - there's scipy.optimize, which has a .leastsq() f'n
                - there's also matplotlib.mlab.polyfit
            - then, if you've decided it qualifies as a spike, you can save the model for it, and use that for template building and matching
        - Nick says: Levenberg-Marquadt algorithm (LMA) is least squares, and so is simplex? What differs is just the cost f'n?
            - ah, scipy.leastsq is LMA
            - Nick: instead of doing template matching after modelling the spike, you can just plot the model parameters in a multidimensional space and do normal clustering - skip the whole matching step!
            - says best to just use sum of Gaussians, no need to multiply a N+1 order polynomial with N gaussians
                - tried this doing some manual fitting, sum of Guassians looks great, and is very malleable to get you the spike shape you want
            - also, in the same go, you can model the position of the spike in space - just add a 2D gaussian model for space (that's another 4 params: two means + two stdevs) - then, when you cluster, you're in a very intuitive space (even if its, say 6+4 = 10 dims) that involves the means, stdevs, and amplitudes of two spike phases, plus the position and spread in space.
                - this brings us all the way back to what was Phil's original idea: use physical space as your clustering space
            - should actually have another param theta for angle of 2D gaussian
        - I should probably try this all out in pure Python to start, including the intial threshold detection step (using dynamic thresholds)
            - nothing in this whole modelling method precludes changing anything about the spatiotemporal lockout - that can stay as is. What changes is that the lockout becomes more accurate, since you're modelling the spike peaks in both space and time, which is less affected by spatiotemporal noise than the raw data values
            - for initial spike detection, instead of just looking back one data point to see if signal is diverging from 0 (which itself is susceptible to noise), maybe look back two or more points, or average across the last two or more points to make that decision - this would make you less susceptible to suspect a spike due to a little mini up-down event on the down slope of the final phase of the previously detected spike
                - or, if you see an up-down event, check if it exists on other chans at the same time - although, this assumes that the spike is multichannel, which it isn't necessarily
            - might also be able to reduce num params by fixing all spike phase gaussian means and stdevs to be the same (assuming S+H correction is turned on, and there aren't any backpropagating spikes)
        - spike at ptc15.87.23700, chan=3 could definitely use separate vertical and horizontal spatial sigmas - the vertical one would be greater
            - same goes for spike at ptc15.87.21940, chan=10. Maybe sigma vector amplitude and angle would be better
            - having two spatial sigmas probably makes the problem underconstrained for a 2 column probe, but it might work on a 3 column probe - I think Tim concluded this as well
            - problem is that to compensate for something that obviously isn't circularly symmetric in space, it sets the source way off the probe to the left or the right. This then messes with the spatial lockout, which is centered on the modeled source location, so you end up getting double triggers.

        - another strategy might be to give up on the spatial modelling, fix temporal gaussian means and sigmas across chans, but give each chan its own two amplitudes, then just triangulate to find spatial mean, or then run the model in a separate independent run, using the modelled output values in time from the previous run, to decide on the spatial means and sigmas and orientation. This will help prevent some of the parameters from confusing the others. In other words, maybe the first run should be concerned with finding the spike's point in time, and the second run with finding its point in space.
            - tried this, didn't help under LM. Not sure if I tried it again under R-alg. Anyway, it was complicated and confusing, and really, you should try to fit everything at once

        - maybe I should do the full 3D modelling that Tim did, this would only require adding one extra parameter: z0. sigmaz would be the same as sigmax. Also, maybe theta should just be plugged into the model as a constant, determined by the user, and defaulting to 0
            - might also try a 3D Guassian instead of the 3D 1/r. Gaussian is smoother and well defined around 0, leastsq might be more stable with it
            - thought I tried this, and model wasn't converging well. Maybe this was for the Levenburg-Marquadt, before I tried the R-alg.

        - could also try feeding just the main peak or the two peak amplitudes of each channel to the spatial model, instead of all the points in the spike. The peak values might be more consistent with the spatial model, and therefore easier to fit and less likely to result in ridiculous spatial parameter outputs
            - benefit: treats normal and backprop APs equally - otherwise a backprop
            - or maybe just fit the biggest abs(peak) instead of both peak and valley, since the two sometimes give conflicting info -- one chan might be max for the first peak, another will be max for the second - see ptc15.87.28880
            - or maybe model both peaks, but in separate runs, and use the results of both runs for clustering

        - better way of dealing with backprop rather than just taking the peak amplitudes, is to leave it full waveform and add an AP velocity parameter (um/us)
            - could assume that propogation delay is up (or along the user-supplied theta), or could have two different delays, one in the x and one in the y direction (probably unnecessary). Either way, the current spike time at the x, y coords of the spatial model would be the 0-point time reference

        - rotate probe xy axes slightly (say by 10 deg in some direction), but leave the origin at the center top of the probe - this change of coords would give each chan a unique value for both x' and y', and all chans would contribute equally to both x' and y' related parameter tuning - currently, the chans in the same col all have the same x values, and among themselves they tell you nothing about x related params (position, sigma, or velocity)
            - Nick doesn't think this would help - besides this is redundant with providing a spatial theta for each spatial Gaussian envelope to be rotated by

        - interpolating to 100 kHz would double the number of data points the model could work with - this might make it perform better?
            - interpolation is very fast compared to the R-algorithm, so this might be worth trying

    - could speed things up by only interpolating say each 1ms of data following an initial threshold crossing, maybe interpolate to 100 kHz while we're at it, and leave everything else at 25 kHz without any S+H correction - you don't really need interpolation or S+H correction for the initial threshold detection, I think...

    - create a new thread or process for each searchblock() call

    - dynamic spatiotemporal lockout: some spikes span greater space and time than others, yet they're all given the same lockout. I guess on thresh xing and again on finding a spike, you could search locally in time and space and within the fixed slock and tlock, and see if signal drops below thresh in something less than slock and tlock, and adjust the lockout accordingly for that thresh xing or found spike

    - doubleclick detection_list row brings up textctrl in a (modeless?) dialog with entire 2D events array for that item

    - change event array returned by detector.search() to be many rows, 2 cols, instead of 2 rows, many cols. Would be more natural that way:
        - printouts, no matter how long, would always have maxchan and timepoint right next to each other
        - length checking (of number of events) would only need len, which works on rows

    - maybe consider looking for some kind of multichannel threshold crossing instead of testing each channel separately at the start of the inner loop: if say sum or sumsquared of all chans exceeds some (potentially dynamic) threshold (say 3.5*median for each chan * number of chans), then maybe you've stumbled across some event that's hard to see on a single chan basis, but is more obvious if you squint your eyes and look across a few chans at once (see ptc15.87.102000520, chans=[30,22,31], most easily visible in chart window)
        - or maybe for each timepoint, grab triplets (or quads or something) of neighbouring channels and look at their sum, instead of the sum for all chans, but then for each timepoint you'd have to do this for all possible neighbouring triplets, which might start getting slow
        - this will help find small spikes that might otherwise be missed, but fixing false negatives is less important than reducing false positives

    - Nick: find fit errors between each modelled spike and its raw data, apply some threshold for fit error (normalized by number of chans? or by mean voltage across all chans), reject modelled spikes as invalid if their fit error exceeds your threshold - threshold might be same as or based on noise threshold used for initial event detection?

------------------------------------
CLUSTERING:

    - should probably stop normalizing x0 and y0, even if you then reweight them somehow yourself, because normalizing destroys the physical distance metric between spikes

    - I want clustering to first consider x0 and y0 position exclusively, and only after the fact, use all the other spike model params to further divy up the spatial clusters. Throwing all the params into a big multidimensional dataset and clustering it all in one go won't accomplish this, because there's nothing sequential about the consideration of the parameters. Some can have more influence than others according to how you weight them, but you can't specify sequential treatment of params. The result is that often you get clusters that don't correspond particularly well to their x0 y0 values, even after manual reweighting. Instead, they correspond to the other param values
        - the idea is that the secondary params should only be there to refine the x0 y0 clusters
        - I think the only solution is to do multiple cluster runs, with different param sets in each. First, cluster using only x0 and y0. This could result in undersplit clusters, but shouldn't have any obvious errors such that a spike from the top of the probe is clustered with one from the bottom, for example. Next, cluster all the spikes again, but use only the other params. You now have two sets of clusters. The second set will probably have fewer significant clusters in them, and could be useful for stuff other than spike sorting - ie, looking at all cells that have similar spatial sigmas, or similar thetas, or something. Now, take the intersection of the two sets of clusters somehow. Some set() method should be able to do this. This way, you potentially break down a cluster defined by x0 y0 into subclusters defined by all the other params. There could also be more than just two sets of clusters
        - to get intersection of two cluster sets, take first cluster in first set, find intersection of it with each cluster in the second set. Each non-empty intersection becomes a new cluster (or subcluster if you like). Repeat for all clusters in the first set.

    - PCA and ICA should be tried at some point during clustering - these wouldn't automatically solve my sequential clustering desires, would they? Maybe for the second set of clustering, I should just throw all the remaining params into a single multidimensional dataset, run PCA/ICA on it, pick the few most significant components, and work on those
        - use mdp package for PCA and ICA
        - note there's also a py_ica module that does infomax ICA

    - perhaps SPC doesn't work very well with a small example dataset of only a few dozen (82) spikes because it uses neighbourhood density to cluster, and in this case, density is quite low everywhere...
        - lowering Knearestneighbours (!=q) seems to increase the number of non-unitary clusters it's willing to produce

    - Nick: work on way more spikes, few thousand, and evaluate using similarity matrix, forget about checking each and every spike manually. Also suggests to try PCA/ICA first to help you throw out the not so useful params
        - similarity matrix between all spikes would probably be of modelled spikes in voltage space (single trace per spike) - would hope that the error between modelled voltage traces within a cluster is much less than that between clusters
            - actually, I'm not so sure about doing this in voltage space, since that can be somewhat variable - modelling it with params abstracts out that variability, that's the point - should probably do similarity matrix in parameter space instead, or something

    - read through Fee 1996 paper a bit, got idea that, if I do hierarchical clustering, I should try a really low T threshold, and then increasingly bigger ones, and at each resulting hierarchy flattening, check to see which of the clusters at the previous level have agglomerated, and for those that have, check if any of the agglomerated clusters violate single-unit stats (see Fee 1996). If so, should probably go no further than the previous T level
        - Fee 1996 also says to reject clusters that are multiunit, according to R2/10 = 8.8/0.8 * F(2)/F(10) - some kind of ratio of ISI distribs

------------------------------------
TEMPLATES:

    - Nick: add kmeans to quickly generate templates
        - maybe not so hard, since kmeans is built into scipy I think, but it's still going to require arbitrary parameter values that you'll have to set, and it might give you a false sense of completeness/correctness

    - replace event listctrl with a virtual listctrl, for faster populating of (and scrolling through?) many thousands of detected events

    - unsorted event list text background should be coloured according to maxchan
        - second list control below it for trash?

    - hitting ENTER on a single event in event list should seek the dataframes to that timepoint (just as clicking on the event in the sort window does the same, sort of by accident) - right now it toggles selection of currently focused item, as does SPACE

    - subtemplates - recognize that two distinct events might be from the same cell, but in a different mode of operation
    - toggle the display of all the templates (their means) at once (button, or Ctrl-A?), with just their active channels displayed. This way, you can quickly see if some templates are very similar, and how the cells are distributed across the electrode. This also lets you decide which templates need more/fewer channels enabled to help distinguish between them. Each template should be a different colour (up to say 16 or so).


    - MANUAL SPIKE ALIGNMENT/DURATION:

        - activation (double-click/enter) of either an event list item or a tree item will bring up a dialog box (one that doesn't prevent access to its parent while it's open. modeless?) with two spin ctrls which let you set the trange of the event/template. Plots should update dynamically while the dialog is still open.
            - modifying a template's trange should do the same for all its member events
            - modifying the trange of a template's member event should also modify the template's trange and that of all other member events
            - for events, add a 3rd spin ctrl to modify the events .t timestamp, for aligning misaligned spikes (triggered off the wrong phase, etc.). Again, should have this dynamically update the event's plot on every spin ctrl event
                - every time you modify the timestamp to your liking, maybe when you hit OK it should check to make sure you haven't duplicated some other existing event with exactly the same maxchan and timestamp. This can happen if two events which are really one and the same are detected by the detector, and you modify one to match the phase of the other

        - do this on a per spike basis as a feature, if automated alignment can't always get it right
        - when doing alignment on a template, move all member events too. Duh. We'll just be adjusting tref for the template, which should then trigger a re-cut of all member spikes?

        - custom spike duration per template
            - Need to make the spike duration (currently 1ms) user settable for every template. Some spikes are broad, others narrow, and it's best to confine the templates to just those times with useful information about the spike. Otherwise, if there's too much flatline on either side, then you're basically saying "it ain't a spike unless stuff doesn't happen immediately before or after it", which isn't true. You might get a spike very soon after from another cell, or even from the same cell if it's a fast spiker.
            - in other words, we want the ability to crop a template in time. This complements the ability to crop a template in channel space

    - for Sort frame, catch Paint event or whatever so that instead of doing a full redraw after de-occlusion, you just restore the saved .background

    - TODO: yet another wxTreeCtrl | wxTreeMultiple flag bug to report to trac: when moving down from the last tree item, you lose highlight (but not focus). Then when you press up, focus and higlight jump to second last item. Similar situation at the top of the tree.

    - when a combination of templates and events are selected in the tree and you hit Delete, leave the templates selected, and only delete the events
        - this should then update the plots for the templates, because their member spikes have just changed

    - cycle between next/previous set of 10 or so member events for current template (or first selected template) on > and < keypresses. Or maybe + and - keypresses

    - hover over event in tree should give you tooltip with event time and maxchan

    - BUG: after opening a .sort file, if your first click is on the first template, no selection event happens, and template mean isn't plotted. Have to move to another one first, then back. Not a problem if first click is on something other than first template
        - smells like another wxWidgets bug



------------------------------------
MINI MATCH/RIP:

    - while building up templates, have ability to do sort of a mini rip of your current templates across the current subset of spikes.
    - this would allow you to quickly increase the number of spikes in each template, and would reveal which spikes in your subset are still very different from any templates you've generated so far, so you can prioritize and focus on those next
    - more importantly, this will aid in sweeping up spikes from those cells that fire a lot and distract you from those that fire rarely
    - a degenerate version of this would be to do a self rip, where you rip across only the set of spikes that are currently members of the template. This would allow you to adjust the error thresh until the self rip just barely returns all of the current members of the template
    - just as for a normal rip, you'll first need to set an error thresh for the template
    - as you progress in your template building, you can occasionally do a mini rip and look at the error histogram, to see how well that template separates spikes out from the subset. Should be able to adjust the error thresh such that you get exactly the spikes that are already members

'''
------------------------------------
MATCHING/RIPPING:

    - instead of ripping against all the raw data, spike detect the whole file, and then rip against the detected spikes. If you center both your templates and your events on the peak, then comparing a template to an event is a one-shot process - no timeshifting required. 2 benefits:
        - much faster
        - error histograms will be more sharply bimodal, since you're not comparing templates to noise ever, just to events
            - also, no longer have to worry so much about selecting as few flat chans as possible in your template, in the worry that it'll start triggering against flatish lines of noise in the raw data
    - to reduce spike variability due to correlated noise, for any given comparison between template and data (be it against any timepoint in data, or against a detected event), take the channels that aren't selected in the template (purportedly because they have no significant signal on them), and at every timepoint, take their mean, and subtract that from _all_ channels. This should make spikes in the raw data (or in the detected events) look more like one of the templates
        - mean might not be ideal - maybe take their covariance and subtract it or something (see hyperellipsoidal method ref'd in 200x Blanche)
        - this should be done during any comparison between template and data, including during min rips (see below)
'''

------------------------------------
VERIFICATION:

    - after completing a rip of the whole file, assemble the spike times from all the templates into a single list. Compare it to the list of spike times generated by applying the spike detection algorithm to the whole file. See how the two sets of spike times overlap. Imagine a venn diagram: if there are spikes that the detection algorithm found that the MTM algorithm did not find, then you need to examine those spikes. Either:
        A) you set your error thresh too stringently for the template that should have matched that spike;
        B) you have bad templates, or you're missing templates

    - plot autocorrelograms for each template, and cross-correlograms for all template pairs, make sure the bin heights are ~0 for abs(deltaT) < 1ms for autocorrs, make sure you don't have ridiculously huge peaks in xcorrs
        - wasn't there some other related test that we should be doing?
    - Nick: create a distance/similarity metric that, for all pairs of templates, for all overlapping channels, measures the difference between the signals, normalized by, say, the number of overlapping channels and the number of points per channel. This would help confirm that two templates are indeed two separate cells

    - plot spikes as fat bright vertical lines or something underneath the chart window so you can scroll through and see just what was classified as a spike and what wasn't. Vertical line colour should match template colour

    - limit template generation to specific part of file
        - Nick's idea: randomly sample events from, say, only the first 10 minutes of a recording, build up your templates with those. Then, rip across the whole file, and watch if fit gets worse the further away you get in time.
            - plot goodness of fit over time - Goodness of fit might change over time, especially over hours, and in recordings different from which the templates were generated. Either divide each recording up into pieces say 10 minutes long, and plot that, or have a sliding window that calculates the goodness of fit for the last 10 minutes (that would give you lots more data points)


------------------------------------
LOW-PRIORITY:

- add mouse-controlled time range selection to slider

- CTRL-B to bookmark a position (plot a red vline at that timepoint in all data frames? and/or add that timepoint to file position drop-down combo box), CTRL-F3 and CTRL-F2 to skip forward/back to next/previous bookmark (or select it from list of bookmarked times in file pos combo box)
    - good for skipping back and forth quickly between two (or more) spikes or otherwise interesting regions

- add file pos spin button, and catch the spin button's events (see wx manual pdf page 240) to step through file
    - or just use pgup/pgdown or slider paging

- add scroll wheel support for data frames, for scrolling through data
    - scroll by 40us in spike frame, 1ms in chart frame, 50ms in lfp frame
        - add a .tres attrib to each data frame?

- add vertical zoom controls to all DataFrames

- toggle between garish rainbow colours and normal green

- toggle plot points

- make moving of main spyke window retain existing spatial relations to data frames, instead of resetting to original spacing
    - worked on this for quite a while, couldn't figure it out, had recursive calling problems and stuff
    - ah, I was looking for something like this for a long time: wx.PostEvent(window, event) lets you send any event to any given window. Should try this out...

- goto feature: CTRL+G brings file position spin control into focus where you can enter the time that you want to jump to
    - maybe add ability to choose units (us or ms or s) from a combo box

- parsing progress bar with pause/stop button
    - use threading, so widget draw events aren't blocked during parsing

- load progress bar

- save progress bar

- event detection progress bar with pause/stop button

- matching/ripping progress bar with pause/stop button






------------------------------------
OLDER:

- move simple matplotlib plot() method from Stream class to WaveForm class, that way, you no longer need to pass a trange argument
    - instead of:
        f.highpassstream.plot(chanis=None, trange=(135e6, 135.1e6))
      you'd slice instead:
        f.highpassstream[135e6:135.1e6].plot(chanis=None)
    - would have to add more stuff as attribs to WaveForm class, such as .records and .rts

- spike (and lfp? hardly worth it?) data need to be corrected for sample and hold (S+H) delay offsets
    - do this during interpolation as in Surfbawd?

- LFP data need to be corrected for offsets induced by low-pass filters
    - what are the spectral phase and amplitude characteristics of the filters?
    - Hilbert transforms are useful for this?
    - want to do CSDs as well - I guess these don't need to be filter offset corrected, since you're just comparing between lfp chans, not between lfp and spike chans

- could eventually add monopole and dipole modelling for 3d localization and to distinguish between pyramidal and stellate cells

- to deal with spike adaptation and attenuation during bursts, for each template, rip not just its mean across the file, but also the biggest and smallest (in terms of something like sum of peak-to-peak voltage across template channels, or variance across channels, or...) spike assigned to that template.
    - or, for each template, plot the distribution of spike sizes for all spikes in that template. For some templates, you might get a skewed distrib in which case: A) you've got a polluted template that needs to be split
                                    B) you need to do something, can't remember what I was gonna write, maybe increase the error thresh for that template?
    - maybe have ability to go from bins in spike size histogram (say its tail, or some weird peak) back to a display of the individual spike(s) that contributed to that bin


- need to be able to select which channels to enable in the template before doing a rip
    - for each template, use mouse controlled bounding box/clicking to pick out significant channels
    - if there's another template nearby in channel space, it's good to also select a few surrounding flat channels to help distinguish the fits of the two templates during the rip

- hard to do?: would be good to have undo/redo for all the template membership manipulations

- what kind of window should we have around the spike time? Does the spike time represent the time of absolute value of peak voltage on the channel with the biggest signal? Then the temporal window around that is -0.25 ms and +0.75 ms for a total of 1 ms window? I think that's roughly the standard in surfbawd. Temporal window parameters (-ve and +ve offset from spike time) should be a global setting that everything uses, but one that you can change at will (at least when spyke isn't currently doing a rip) to accomodate the display of the occasional long slow spike

- do we want to allow templates to be generated from spikes from multiple recordings? this could add annoying complexity, but might be handy for getting enough spikes from those rarely firing neurons - some neurons might fire only during one stimulus type and not the other. Neurons might have opposing preferences, such that you never see them all fire within the same recording

- low probability: in case of electrode drift within a recording, allow user to make executive decision that two templates that are significantly different in voltage+channel space due to drift, yet sufficiently similar to the eye, that they should be assigned the same neuron id (keep their template ids separate?)
    - should there be a distinction between template and neuron ids just in case we want to do something like the above? Maybe the template id should become alphanumeric, with a number followed by a letter, say 12a, 12b, etc.

- OLD Cython event detection: for each searchblock() call, I only allocate enough memory to store event times for 54 cells (one per chan) with an average firing rate of 1 kHz - perhaps I should check in the cython code if I'm about to exceed the bounds of the eventtimes array, and grow it when need be, or at least print an error, instead of segfaulting!






----------------
SPIKE MODELLING:
        - I think the real answer to most of my fitting problems is that I need to be able to place restraints on the parameters while the fit is running. Does this mean delving into leastsq fortran code? Rewriting leastsq in Cython? Is there another function I should use instead? Should do some searches, maybe mail the scipy list. - solution was to use openopt, which allows all kinds of constraints
        - disable grounded channels
        - lock each chan out separately wrt to its phase2ti, since all chans can now be potentially delayed wrt source - this will require calculating each channel's phase2ti separately...
        - take weighted spatial mean of chanis at phase1ti to better estimate initial (x0, y0)
        - if giving phase1 and phase2 different AP velocity delays, should probably constrain them to be of the same sign - scratch that: see ptc15.87.26940 where the 1st phase increasingly leads the spike time as a f'n of distance, and the 2nd phase increasingly lags the spike time as a f'n of distance. You need to allow v1inv and v2inv to be of opposite sign to successfully model this
        - constrain that sx and sy need to be within some factor of each other, ie constrain their ratio
        - improve estimation of window limits relative to threshold crossing
        - add AP velocity param(s) - make that inverse velocity to avoid singularity
            - seems to be causing some x localization problems...
        - improve spatiotemporal lockout accuracy
            - for every fit spike, check if the max fit val is a peak (ie has values on either side that are lower than it). Or, just check if the means of both phases fall within the window limits. If not, chances are good that this thresh event was triggered by a late channel that's only now crossed thresh, just outside of the spatial lockout of the actual spike, which had already been modelled. Therefore, the currently fit spike should be thrown out
                - maybe a better way to deal with this late channel situation is to do the AP velocity modelling - that way each modelled chan can be locked out to a different timepoint according to the modelled propogation delay
        - test if constraints are actually making any difference, or if ralg is just amazing
        - search for maxchani within chanis, and recenter on it, generating a new more appropriate set of chanis. Then take weighted spatial mean of these chanis at phase1t to get best estimate of source. Feed this best estimate as (x0, y0) into model - ie get away from maxchan centric thinking
        - nah: regenerate chanis list based on this spatial mean based best estimate
            - don't think this will make any significant difference to model results
        - NAH: try 1/r model again, with initial guess located at weighted spatial mean instead of at maxchan, to avoid singularity at maxchan
            - or, replace param r with rinv, or "nearness" param, to get rid of singularity, like I did for AP velocity v and vinv ("slowness")
        - run profiler - R algorithm is just plain slow - solution is to try another one
        - should probably initialize model (x0, y0) to weighted spatial mean of chans, instead of just the maxchan coords - this would also help to reduce singularity/differentiability problems with 1/r model

end SPIKE MODELLING
-------------------



----------------
QT rewrite DONE:


- replace all list RefreshItems() calls with reset()?
    - reset clears any current selection, in addition to updating the list
    - update() doesn't seem to do anything.
    - here it is:
        model = self.windows['Sort'].slist.model()
        i1 = model.createIndex(row1, 0)
        row2 = model.rowCount(None) - 1
        i2 = model.createIndex(row2, 0)
        model.dataChanged.emit(i1, i2)
- delete all SetItemCount() calls
- should really rename slist to uslist
- replace all clist with nlist
- search for all occurrences of getSelection()
- rename sf to sw, cf to cw, etc (if possible)
- test .sort file saving and loading
- bind sort window buttons
    - keyerror when undoing deletion of cluster - has to do with cluster selection and plotting, which is slightly different from what it was under wx
    - split a cluster, works fine, undo works fine, on redo, new clusters are restored and selected, but for some reason, the mean waveform of the last of the new clusters isn't plotted. This causes a plot keyerror on subsequent selection changes (such as another undo)
        - in fact, even happens when you split, undo, then leaving the selection as is and split again (as opposed to redoing). If instead of leaving selection as is, you change selection, then change back, then resplit or redo, it works fine. Very mysterious. Qt bug? Something to do with focused item, not just selected items?
        - if you have another unrelated cluster selected, or if you have no clusters selected whatsoever, you can undo/redo the split as many times as you like without the keyerror. Mysterious.
- cluster tooltips
    - do i really need to use mouse events? can't I just grab the current mouse position every time a tooltip is requested for the cluster window? ie, can't i just override the tooltip event, set the toltip text, and then allow the tooltip event to continue?
    - alternative is catch mouse movement events, set tooltip text, then raise tooltip event. This would have the benefit of raising the tooltip immediately, instead of having to wait for the delay to expire. Delay isn't adjustable it seems.
        - If you want to show a tooltip immediately, while the mouse is moving (e.g., to get the mouse coordinates with QMouseEvent::pos() and show them as a tooltip), you must first enable mouse tracking as described above (setMouseTracking(True)). Then, to ensure that the tooltip is updated immediately, you must call QToolTip::showText() instead of setToolTip() in your implementation of mouseMoveEvent().
- keyboard commands for clustering and cluster selection
    - ever since implementing keyPressEvent in SpykeMayaviScene, standard vtk/mayavi key commands aren't being caught properly. +/- no longer zoom, and f for focus does a pan and zoom, instead of just a pan
    - using direct base class calling instead of super() fixed this
    - on R keypress, select random sample of current cluster's spikes. This way you don't have to go manually selecting spikes to evaluate cluster cleanliness. Also, this way, every time you hit R, you get a new random sample of spikes plotted, which potentially makes cluster evaluation less biased
- make cluster points single pixel in full screen (they seem to be 2x2 for some reason)
    - actually, none of my interactivity code works in the fullscreen window, seems it's a distinct window from the normal cluster window
    - programatically resize the cluster window to take up the full screen on F11. Maybe catch double click on the titlebar. This way, you don't need to use Mayavi's separate fullscreen window
    - for ease of setting fullscreen or maximized state, might need to change window type to proper window, ie QDialog, or widget with Qt::Window flag set
- update cluster plot immediately on dim change
- add Apply Clusters button, to recolour cluster points after hitting Plot button after hitting Cluster button
    - hm, or why can't you just check for clusters and auto colour them each time plot is hit? Indeed!
- maybe change default size and position of cluster window to be to the right of sort window, full width horizontally to edge of monitor, along the top, and half the height of the sortwindow (ie half screen height). This way, you can still see the terminal window below spyke, in the bottom right corner of the screen (best to resize terminal window to fit fully within this space. Might be able to set up a terminal profile for this)
- if the nearest glyph is a point and not an ellipsoid, and if that point belongs to a cluster, select that cluster - getting too many mis-hits when selecting clusters, especially dense clusters
    - same goes for cluster tooltips
- remove paths to .srf filenames from .parse files. Same goes for the parse filenames too. This way, you assume that the .srf file exists in the same directory as the .parse file, and it lets you use .parse files across devices with different mount points
- hell, move all .track files into the same folder as their constituent .srf and .parse files too - this makes everything much simpler - no paths at all
- allow display of multiple neurons' spikes in nslist: rename .neuron to .neurons, and combine their sids, and present them (either in neuron order, or in sid order) in the nslist, so you can select any and all of the spikes of all of the selected neurons. Neuron id can be a sortable field, along with the others (sid, time, x, y, etc)
- if number of points > some threshold, disable cluster tooltips on hover, and only trigger them with a keyboard command or something
    - simpler to just require keyboard command for all tooltips
- maybe now that I'm no longer plotting ellipsoids, switching from the whole mess of mayavi/vtk/etc to OpenGL might be quite easy, and might perform a lot better, and use less RAM, and have fewer dependencies?
    - imagine how fast sorts will load when I no longer have to wait around for mayavi/vtk to do init!
    - plotting and rotating 7 million points in opengl in qt is super fast!
- BIG BUG: many of the weird artifactual spikes are wrong - they're not there when you go take a look in the raw data - something wrong with spike detection in linux? Get them even in first 30 sec of file 87, which I definitely wasn't getting before
    - a symptom of this is that I get a lot of "spatial location way off" messages, and this seems to slow down detection a whole lot when it happens. In first 30 sec, only seems to happen in the first detection process
        - actually, sometimes it happens on multiple processes simultaneously
    - when I click on the spike to go to its position in the raw data, it goes to a raster line, but the raw data at that time doesn't match the spike data in the sort window
    - duplicated on jervis, running rev 999 of dev branch, while searching first 30 sec of file 87
    - also, occasionally getting "Exception OSError: (4, 'Interrupted system call') in <Finalize object, dead> ignored" at end of detection, may or may not be related
    - could this be a new version of some library that started causing this? Cython? numpy?
    - as of rev 935 in dev branch in win7 (using wx with non-working mayavi), bug doesn't seem to exist
    - maybe need to look at code changes in climbing.pyx or detect.py between rev 935 and tip
    - except when it happens on all chans, only seems to happen on chan 53, ie the last chan, or on chan 1, ie the 2nd chan, and both of those happen to be adjacent chans
        - try testing on different file
        - try running detection in debug mode
        - problem disappears in debug mode, always comes up with 7085 spikes instead of the 7105 it comes up with otherwise - this is a multiple process problem?
        - using numpy 1.5.1 in win7, multiprocessing 0.70a1 in both win7 and ubuntu
        - upgraded cython to 0.14.1 and numpy to 1.5.1 in ubuntu, didn't help
    - is pyopengl-accelerate being used in linux? installed in windows, seems to cause problems?
    - do a hack test for abs(spike signal) > 25000, drop into debugger, though this won't work when multiprocessing
    - try revert to rev 935 on ubuntu, see if you can get it to work in wx
        - will have to do some hacking, like disabling a certain wx list style or something
    - why is this a probabilistic problem?
    - might it have something to do with ipython being loaded up and ready to pounce in case of an error? I guess everything, including ipython, is duplicated across processes, and that could cause some kinda race conditions?
        - nope, removing ipython altogether doesn't help
    - might try limiting the number of processes to 2 or 3 (or restoring hyperthreading), see if that changes anything
        - leaving multiprocessing turned on, but searching only one block of data at a time, there's only ever one child processes at a time, and that works flawlessly across whatever single blocksize you choose
    - aha! got wx spyke dev rev 935 working in linux, get the same problem during multiprocessing detection. So it must be a problem with a linux library, and qt or opengl aren't to blame.
        - set up the simplest possible example code that duplicates this problem, and try it on different platforms. Seems this might be due to a recent update in some library. See http://stackoverflow.com/questions/4952247/interrupted-system-call-with-processing-queue
        - need a more realistic demo that does actual spike detection in multiple processes, but maybe without the gui stuff - ie, loads off the disk, runs cython, runs scipy.leastsq, etc.
    - try running in python 2.5 with processing installed
    - maybe the specific atlas libraries I'm using could be to blame - in win7, I'm using christoph's intel mkl libraries
        - try downgrading numpy/scipy, or compiling them without atlas
    - maybe instead of using Pool, with its weird initializer hack, I should subclass Process, make DetectionProcess.run() call searchblock nblocks/4 times, and launch 4 of these processes manually
        - if that doesn't work, could spawn my own processes manually using linux specific os calls?
    - SOLUTION: needed to deepcopy the detector for each process to prevent artefactual spikes, not sure why this seems to only be required in linux

-------------------
end QT rewrite DONE





----------------
GL rewrite DONE:

- replace cw.glyph.mlab_source.update() with cw.updateGL() or cw.qglwidget.updateGL()
- search for mlab, glyph, disable_render, scene
- for some reason, plotting 7m actual data points gives really slow rendering
    - it's not the colouring - decolouring all has same problem, and setting them all to white doesn't help either
    - zooming in or out, or rotating so that most points are obscured, also doesn't seem to help - quite mysterious
    - must be the values themselves - nope, scaling them all down so they're between -1 and 1 doesn't help.
    - strange, hiding and then showing the cluster window helps speed it up a little, but it still sucks
    - reducing npoints to 10k speeds it up a fair bit, but its still strangely slow. Something in the python process is using 100% CPU during idle
    - better on restart, but still strangely slow.
    - switching back to 7m cube data after plotting real data restores normal rendering speed. Setting to exactly 7282567 points in cube doesn't change that. Very strange.
    - it's actually slower than mayavi
    - ah, maybe it's cuz array X isn't contig! yes, it isn't contig! Yes, copying solved it!
- zoom and pan should be more consistent speed
    - solve both of these by multiplying factor by distance (length of translation vector)
- implement tooltips and point selection
    - need reverse mapping from viewport coords to point coords (and ideally, to point index)
- implement focus to point
- implement saving and restoring of camera view (search for view and roll keywords)
    - just save and restore the modelview matrix and the -self._dfocus as the focus
      
end GL rewrite DONE
-------------------



----------------------------------------------------
DONE:

- move code to /spyke subfolder, make a setup.py in root, keep TODO in root
- how hard will it be to generate highpass, lowpass, chartwindow, and fistogram widgets in wxpython?
    - email wxpython list on displaying time series data.
    - wxpy 2.8 should be faster, doesn't need to copy data, can use numpy data directly?
    - borrow something from MPL?
- need ability to take a spike that's assigned to an existing template and remove it and generate a new template from it
- NOPE: destory data windows instead of hiding them - should be simpler in the end
    - set up config system to, among other things, remember window positions and sizes when you re-open a window, whether within the same session or across sessions
- try making data frames MiniFrames (see manual) instead of passing them the TOOL_WINDOW flag
    - consider making data frames children of the main spyke frame again - maybe this could be done to automatically iconize and activate the data frames when main spyke frame gets the event - yup, this is exactly the case. hooray!
- chart window channel layout is wrong
- arrange data windows so they overlap as little as possible
- end range problem for chart window, allow plotting of nothing...
- remove hard coding in data window positioning
- have a dock mode, where moving one window, whether main spyke frame or any of the data frames, moves all windows the same amount
    - I think all you need to do is give all frames an OnMove method (or something), call all other windows' OnMoves, pass them the event, and then event.Skip()
    - or better yet, make moving the main spyke window always move in dock mode, and moving the dataframes not do so (er, this last one seems hard to do, leave it out)
- make seek move chartwin's center to that position
    - that way you can look at stuff before and after the spike win data
    - also, a centered seek will allow for natural centered zooming in and out in time
- for laying out channels in space, should really be doing a um to uV conversion vertically, and a um to us conversion horizontally. Instead, right now I'm just maintaining vertical and horizontal order, and assuming all chans are equally spaced within those ordered lists
- shade the 1ms (or whatever the spike frame width is) on the chart window with a slightly lighter background than pure black, to indicate what part of the chartwin data you're currently looking at in the spike window
- make spike frame use same colours and chan to colour mapping as chart frame - chart frame is rainbow, spike frame will be similar, but not exactly the same for probes with chans that line up horizontally (like the 3 col probes)
- make spike frame width depend on number of columns in probe, so you get consistent channel width for all probes
- replace default slider position text (which keeps corrupting anyway) with proper spin edit and dedicated text boxes to indicate current position and start and end of recording
- nah: add chan id toggle button to display chan id directly underplotting (or shown next to?) each channel in all the dataframes
    - instead: show timestamp and chan id tooltip on hover
- add faint 0 uV horizontal lines for spike and LFP windows (not necessary for chart win)
    - add faint vertical lines at center of each column in spike window
- add horizontal zoom controls to all DataFrames
- when searching for maxchan, limit search to chans within spatial lockout
      - prevents unrelated distant cells firing at the same time from triggering each other
- add ability to search backwards in time:
    - maybe just slice data in reverse order, Cy loop won't know the difference?
    - would also have to reverse order of block loop, can prolly do this by
      simply making blocksize BS -ve
- BUG: searching for next/last spike with only some channels enabled does weird multiple triggering off of bigger spikes on that channel, and sometimes segfaults if F3/F2 is held down
    - doing the same using search button alternates between two results when some chans are disabled
    - was due to a mistake in indexing into distance matrix in cython code
- create a win32 package installer, or just do python setup.py install, don't really need to bother with the whole py2exe thing, just manually install python, numpy, scipy, wxpy, and mpl on clients
- deal with case where you've got just a .sort file open, and you try adding an event with no wave.data to a template. Prevent user from doing this! Right now it removes the event from the event list and gets as far as trying to update the template mean, at which point it runs into an AssertionError. Need to handle this properly
    - maybe you shouldn't even be able to select an event when its wave.data is unattainable...
- be able to specify time range(s?) within a file to limit spike detection to - this could be useful for limiting detection and template generation to parts of the file during different levels of animal wakefulness, as indicated by the dominant frequencies in the LFP
- specify chans that you want to search for events on. 2 benefits:
    - helps you increase n for a template that rarely fires
    - say you notice a shortage of cells in a certain part of the probe - you want to be sure there aren't any cells there that you've missed
- F3 and F2 to search forward and back one event
- toggle chan selection by Ctrl+click on chan in any of the data frames - this either makes them invisible (but that could cause problems for tooltip when it's looking for the nearest line) or toggles their colour to dark grey or something.
- have a log window at bottom that details everything going on behind the scenes
    - this could simply be an integrated pyshell window (see manual), so you can do things from command line instead of being forced to use the GUI
    - also, use it to examine and mess with internals, including wxpy internals
- keep spyke importable as a library, not just as a standalone program. This way, you can parse files, spike sort, etc directly from interpreter, or neuropy, or any other python project.
- Template.chans should be a property. Every time the user changes the channel selection for a template, you should do Template.chans = Template.get_maxchan() to get the new maxchan from the new set of enabled chans
- introduce a weighted error per template chan, where the template's maxchan is weighted the most and surrounding chans are weighted less as you get further away, maybe a 2D gaussian distance weighting function with standard deviation of one spatial lockout radius
    - use a 2D matrix?, generate it from the actual distances between channels in the probe layout?
    - could also weight points with a gaussian in time, centered on t=0, with stdev=tlock
- Detection deletion:
    - allow if none of the Detector's events have a non-None .template attrib
    - if we allow deletion, maybe session.detections should be a dict instead of a list
- need to handle OnResize in DataFrames like I do in the SortFrame - background needs to be regenerated on every resize event
- in detect_cy.pyx, get_maxchan and set_lockout both have awkwardly long arg lists, maybe make up a struct type whose fields point to all the variables used in this method, and pass the struct to get_maxchan and set_lockout
    - or, just make most of the variables globals, using global keyword
- change lockp from being a counter to holding the absolute timepoint until which the chan is locked out, that way you don't have to decr.
- waveform widgets should display data with the current level of interpolation (surfbawd only ever displayed raw data)
- if matplotlib is used for waveform widgets, will get subpixel rendering and antialiasing for free, which should make interpolated data visibly different from raw data even at low zoom
- randomly sample the srf file to get some (hopefully) representative subset of all the multichannel spike waveforms
- static and dynamic thresholds, independent for each chan, based on stdev or median (or other measures?)
- recreate Detector with current setting whenever user clicks on method or threshold radios
    - clicking on other options can just update the current Detector
    - or better, Detector is updated from other options only when you hit search or F3 or eq'v
- unsorted event list control should have sortable columns: id, maxchan, timestamp.
- use .GetTopLevelParent() from (any?) widget to get a reference to spykeframe, safer than doing a bunch of Parent and GrandParent stuff
    - actually, that only seems to return the frame the widget is in, but then from there you can take the Parent to get tye spykeframe
- tabbing between tree and list ctrl works, but there's something sort of in between that the tab lands on, yet after landing on that something, if you hit up/down, you realize you're actually focused on the event ctrl, just can't tell cuz the current selection for the event list ctrl doesn't highlight where you're in that twilight focus zone
    - i think the twilight zone is actually one or both of the sort panels, cuz the keydown printout happens when you hit a character in the twilight zone
    - MoveAfterInTabOrder and MoveBeforeInTabOrder should help
    - OK, now I've figured it out. It's the stupid .plot_pane that's getting the focus. By deleting the plot_pane, I get rid of the problem. So the trick will be to enforce that the plot_pane can't get focus. Normally, you'd override AcceptsFocusFromKeyboard, but that doesn't work for C++ classes, so you have to do something else. See: http://lists.wxwidgets.org/pipermail/wxpython-users/2008-April/074292.html
    - captured tab keypress in both tree and list controls, use them to transfer focus to the other control
- BUG: splitterwindow doesn't actually set itself to desired size until after you move the mouse for the first time
- make SPACE alone, without CTRL, toggle selection of currently focused item - DONE, but there's an annoying flicker due to the hack around a wx bug, probably can't be eliminated til the wx bug is fixed
- Nick's suggestion: add ability to sort event list by match error, as an alternative to sorting spatially by maxchan. Ie, do a mini-match between the currently selected template or event in the tree (make sure only one item is selected) and all the unsorted events in the event list, and sort the event list by their err. Maybe add an err column to the event list that gets filled in when you ask for a mini-match, and then click the column header to sort by err. Note that many of the entries in the err column will be blank because many events won't have enough similarity in overlapping channels/maxchan distance away to merit even generating an err value.
- try parsing by restoring from .parse files again for speed, especially when it comes to testing...
- rename Session object to Sort object
    - shorter
    - corresponds better to .sort file
    - corresponds better with Sort level (proposed renaming of Rip) in neuropy object hierarchy
- transparency would be good for overplots
- neuron ids should be incrementing integers. When you delete a template, the following templates' ids and colours should remain unchanged, until you hit a toolbar button (or something) to indicate that you want to renumber (and recolour) the whole lot, which will effectively only renumber and recolour those templates following the one that was deleted
- template files
    - contain python syntax that can be eval'd, or config file format, to get you everything you might ever want to know about the template:
        - file (files?) it was generated from
        - member spike times (and their associated files?)
        - date time it was last modified
        - ...
    - store all templates in one file ala surfbawd? or one file per template?
    - store it all in a single SQL database file?
    - THIS ONE: or, pickle the whole Sort class instance to file, but also make it possible to export to separate binary spike files (plus the binary .din file)
- are there really two different record flags (M and MS) that both indicate a MessageRecord (see surf.py)? What .srf files do either (or both) occur in? Which one is more common? The MS one?
    - no, they're MS and MU, for Surf and User generated messages. At least, there shouldn't be any .srf files out there that have only M as the record flag

- instead of simply searching for max and min V within window, search in window forward from thresh for a peak, then in either from that peak for up to another 200us for another one of opposite sign. If you don't find a 2nd one, it ain't a spike. How to find peaks? Look for change in diff? Nah, use inline C.
- profile pickling and unpickling of both .parse and .sort files by temporarily using the Python pickle module instead of the cPickle module, to hopefully figure out why the surf.File and sort.Sort objects take so damned long to process - too many references?
- assigning 64 bit range values to the wx slider on startup overflows if they're outside 32 bit values - this is a problem for 7 GB .srf files. Might have to use the slider as a rougher metric - instead of us, have it represent 10us steps? Or interpolated Xus steps? This then needs translation between slider values and actual time values...
- abstract out spike detection and obvious rejection from spike modelling, do all detection first in its own loop, followed by all modelling later in its own loop
- Nick's suggestion: consider leaving voltages in AD units, as signed 16 bit integers (centered around 0 instead of 2048 as in the .srf files) instead of float32's. This would use half the memory, and might also speed up computation: integer math should be faster than float math. Whenever you actually need uV, just convert on the fly
    - I think I decided to stick to float32's because during Nyquist interpolation, I have to switch to them anyways, and I'm always doing interpolation
- does disabling a channel really prevent it from being searched on? yes
- just a note: user should probably disable any grounded out chans in the dataframes, before beginning detection/sorting/matching. Any disabled chans in the dataframes should propagate through the whole process as disabled by default (not sure if this is the case right now)
- when searching for maxchan, new one should exceed current in Vpp, not just in Vp at phase1t. See ptc15.87.35040. Best to use Vpp instead of just size of a single phase when deciding on maxchan
- Another problem at ptc15.87.35040 is that it did actually detect the original teal chan 5 as a spike, but then when it went to look for phase2 on the new purple maxchan 6 and couldn't find it, it gave up completely instead of reverting back to the previous maxchan. Maybe I should save things that are definitely spike-like before looking for new maxchan, and revert to those if a PeakError is raised
- Neuron.update_wave() could be optimized by not looping over each ri separately, instead dealing with all ris at once
    - first loop took 9.417 sec, 2nd loop took 48.399 sec for a cluster with 92358 spikes - this is way too slow
    - consider aligning spikes in time in wavedata arrays, wrt their phase1 timepoint - can figure out which timepoint the spike starts at (go back phase1ti points from the reference point) and when it ends (count nt points from the start of the spike)
- better strategy for neuron chan selection: at least 1/2 the spikes have to contribute to a channel for it to be considered a neuron chan. Why are some obviously important chans being left out of some spikes? this is due to a full channel lockout due to a near simultaneous spike nearby
- add binary export of spike times and din for quick interoperation with neuropy
- store common surf record types (highpass, lowpass, and digitalsval) in structured ndarrays instead of long lists of Python objects
- when saving a .sort, and the only changes you've made are to the Sort itself (and maybe the nids in the spikes array), and no changes have been made to the spikes that have been detected and therefore to the wavedata, then just save the Sort object and the spikes array
    - this means you have to put the wavedata in the .sort file first?
    - might be best to split .sort file up into a .sort file (with spikes and Sort in it) and a .wave file (with wavedata in it)
        - make save .wave normal menu item, instead of the weird check box thing I have right now
        - this would make it easier to import a Sort and its clusters and apply them to a new set of spikes with their own wavedata
        - this would also make it more efficient to save multiple .sort files when testing various sorting methods (although not various detection methods) - each one wouldn't also have its huge associated wavedata duplicated unnecessarily
        - saving repeatedly while sorting would be much faster
- add file export item for exporting spikes and din and textheader, with a directory chooser
- tree is still really slow when expanding a neuron with 10s of 1000s of spikes - this is because it's not a true virtual tree like the virtual list is truly virtual - the tree populates all the children of all the currently expanded nodes, not just the currently visible children - that's cuz VirtualTree is implemented in Python - wx needs to do this at a lower level in C
    - best soln is probably to replace tree with 2 more virtual list ctrls: one listing all the neurons (clicking neurons displays their mean), and one listing the currently selected neuron's spikes (clicking selects that spike)
        - only downfall is you could no longer directly compare overplots of spikes of one neuron vs another, but this is already quite difficult to do with the multiselect tree
        - benefit would be doing away with the complexity of bugs in the tree, and most of all, speed!
        - another benefit is you could then sort neurons, and the spikes in each neuron, however you like by clicking appropriate columns in the appropriate virtual listctrl
        - another benefit is that listctrls can colour their items, to match neuron colour
- bug: mean neuron waveforms are being ignored on restore
- add a tooltip that displays nid, nspikes, etc when you hover over an ellipsoid
    - see https://svn.enthought.com/enthought/browser/Mayavi/trunk/examples/mayavi/data_interaction/select_points.py
        - also shows how to outline a selected point
    - easiest would be to use the picker and retrieve the scalar value for each ellipsoid. Need to set the value to something other than the default (1.0) when you generate it, make its value match nid (it's not being used for color or size or anything, so it won't affect the visualization) - problem is, dunno how to set it - really, you need to set the scalar value for all the 100s of points that make up the ellipsoid surface, because its any single one of those that you actually end up picking
- try if making get_spatial_mean take the peaks on each chan separately (but close in time to the peak on the max chan) improves clusterability - this would lump backprop spikes with their normal versions
    - yes, this seems to improve clusterability, in both ptc15 and ptc18
- apply a .sort file's neurons/clusters to the current sort
    - make this a File-Import neurons item?
- bug: clicking on a spike-less neuron in the nlist raises an error, which is fine, but then you can't plot any other neuron, which is bad - here's the traceback:
- add keyboard controls for cluster parameters in cluster window
- big bug: using sort.SCALE only in the visualization of the ellipsoid seems to have messed up sort.apply_cluster for clusters with nonzero orientations
- stop adding of new ellipsoid (which are now glyphs instead of parametric surfaces) from resetting the camera view
- save camera view (pos, angle, zoom) in .sort, so you come back to the same spot on restore
- restoring from .sort with no .srf open is broken again
- split .sort file one more time into a .sort and a .spike file - this way, you can have multiple clustering variations, and save each one as a separate small .sort file (few MBs), all of which reference the same .spike file - that way you don't have to rely on just the exported .spk files to document your sorting iterations, plus you get to keep all relevant sort data indefinitely for all iterations - you can always go back and see how spikes were sorted at a given point in the sorting history
    - need each .sort file to reference a specifically named .spike file in the same folder - they can't all obviously have the same root name, since multiple .sort files have to have different names
    - .sort, .spike, and .wave files should now all be stored a couple levels down from the .srf file, in the appropriate track/recording/ folder
- make srffname, sortfname, spikefname, and wavefname be pathless (relative to cwd) for portability
- implement dynamic noise calc - do it on a blocksize (10s) basis, right before calling get_edges() - this will reduce excess loading time, and take advantage of the multiple processes already present for detection
- fix LFP
- automated clustering:
    - select dims to use for clustering - multiselect listctrl
    - set climb params
    - normalize appropriately (differently than for plotting, or maybe should do it the same, to make things more apparent)
    - merge and delete clusters
    - control density fraction of max outside of which you throw away noise points (just a global control, or per cluster?)
    - plot ellipsoids representing the position of each cluster and 1 std out in each clustered dimension
- get rid of multiple detection ability - this would greatly simplify indexing into spikes array. All spikes would always be labelled contiguously from 0 according to their row index - wouldn't need to mess with ris stuff anymore - the row index would become the spike index, since you can now never add or delete spikes
    - or actually, only need to enforce inability to delete spikes, can still add spikes to the end
    - would get rid of a whole lot of .searchsorted calls and np.where calls. To find a spike's spot in the spikes array, all you need is its sid == rid
- add split method for a single cluster, but need to be able to tell it how many you want to split it to, which dims to cluster along, what params to use (perhaps one that isn't commonly used, but would be beneficial for this cluster), etc
- see if it's possible to make sx and sy free variables (or just make them a single free variable sigma) when doing spatial localization - currently they're fixed at 30 um each. Changing this value changes the resulting localization, which is a bit worrisome. What happens when you decrease or increase this fixed value? If it has to remain fixed, is 30 the optimum value? How do I know this?
    - solution was to make a single spatial sigma free variable, and to fit it first using the spatial mean as the fixed x and y positions, followed by fitting a 2nd time with x and y free, and sigma fixed. Don't get convergence problems, and sigma comes out to something realistic on every spike. Unfortunately, the sigma dimension doesn't seem to improve clusterability
- for simplicity, rename Vs back to V0 and V1, and phasetis back to phaset0i and phaset1i
- combine merge and split into re-climb
- stop using grey as a cluster colour - grey points are hard to tell apart from unsorted semi-transparent white points

- New check_edges() strategy:

1. For each chan, find 3 most extreme extrema, where an extremum is the biggest max > 0 between zero crossings, or the smallest min < 0 between zero crossings. Endpoints of each waveform are treated as zero crossings.

2. For each chan, measure sharpness of each extremum by taking peak_value / npoints around it on that side of 0. Keep the sign. This could later be refined to divide by the FWHM instead of npoints, but would require at least some linear interpolation between the points straddling 50% on either side of the extremum.

3. Find chan with single biggest abs(sharpness) value, choose that as maxchan.

3. Align maxchan to the abs(sharpest) extremum, and store the two (or 3?) sharpest (signed) values. Save sharpness values in s0 and s1 (and s2?). Might consider storing V2 as well as V0 and V1.

4. For each spike, maybe make phaseti0 and phaseti1 (and phaseti2?) all arrays of length == nchans, so you can save the positions of those non-maxchan extrema and use them to easily determine channel weights later for spatial fitting
    - this would also allow you to have a different lockout for each chan in a spike
    - maybe make V0 and V1 and V2, and s0 and s1 and s2 all nchans length arrays too, for easier phase alignment across channels when getting channel weights
    - for manual alignment, rename align max/align min to align +/align - and use signed sharpness instead of amplitude to decide which phase to align each spike to
    - most meaningful spike chans should be those that have the least number of 0 crossing
    - maybe calc sharpness between all 0 crossings on all data, and use that during edge detection. Check the signal amplitude at sharpest point for each segment between each 0 crossings, and if it exceeds threshold, save it as an edge, and pass sharpness values on for spike alignment, max phase choosing, etc. This would also avoid having to constantly deal with waveform edge values, which aren't really 0 crossings.
    - another refinement might be when calculating the width that goes with an extremum, instead of counting the npoints between 0 crossings, count the npoints between where signal drops to say 10% of extremum, ie calc FW1/10M. Wouldn't really need to interpolate like you would for FWHM, since you'd still have a decent number of points to work with, almost as many as FW0M which is what we're currently doing. This would deal with problem where a phase is obviously a phase, and has dropped from its peak back down to almost 0, but it doesn't quite cross, and the crossing doesn't happen til way way later, greatly skewing the width measure for that phase. See first +ve phase of spike at ptc18.14.299719780.


- add a cleaning method, to reject points whose waveforms are quite different from the mean - ask Nick about his criteria - rms error?
    - plot error histogram to aid in setting thresh
    - for each point, find shortest geometric distance between spike and cluster mean, instead of just vertical distance like rms
    - do Gaussian weighting in space and time of errors - care most about errors near the max chan and near the main phases of the spike - maybe use spatial sigma and temporal spread from parameter extraction step, which would give you different G weighting for every spike. Or better yet, just use spatial sigma and temporal spread of the mean, which should be nice and noise-free
         - then, once you've removed the noisy spikes, need to recalculate and replot the error histogram, and make sure there aren't any other noisy spikes remaining
    - try taking integral of spike on main chan (or each chan?), and find peak of the integral to tell you when the biggest phase of the spike has ended
        - he tried this, says it doesn't work too well
    - perhaps best cleaning idea: 2010-07-21 - NVS:
        - use gradient ascent clustering algorithm on maxchan waveform to not just remove noise but actually split 2 or more cells that are stuck in a single cluster, that can't be split in normal parameter space. This is a very high dimensional space (50D in my case, with interpolation to 50 kHz), but there are relatively few spikes in a single cluster that needs splitting (1000s), so this should run fast

- why does neuron.sids field have dtype=int64? Should stick with int32 as spikes['id'] does
    - actually, I'm not sure it is int64. Seems to be int32 after all...

- instead of having a sort.clusteris attrib, add a field to spikes rec array called cid. So each spike would have both an nid and a cid (nid could be -1 after application of density thresh mask, while cid could remain something else). cid could be int16 like nid
- maybe make all spikes span the same amount of time, regardless of lockout during detection? Whatever lockouts existed during detection are ignored when you realign anyway... This would simplify interpretation of wave data in the wave data array
- Nick - use infomax ICA to pull out most informative features across a multitude of spikes (get a couple of mother spikes), and then apply them to all spikes you encounter, pull out the weights of those features, and cluster on those - make sure they come out independent, and also try and compare the ICA features to PCA features. Any difference/improvement?
    - could also try different kinds of ICA (see mdp)
- need to finally implement gaussian spatial fitting to get around the artifactual clusters you get (especially in the y dim) from using spatial mean, especially with a lowish channel count
    - this would let you lower the channel count, save mem, yet still get good clusters
    - will be slower, need to make it as optimal as possible
        - to start, try fitting just the peak and valley (found separately on each channel within reason wrt maxchan, as is currently done for spatial mean method), ie 2 points per chan
        - or maybe just fit one point per chan: check which is the biggest abs(amplitude) of the peak and valley on the maxchan, and just fit the one that's biggest
    - not working due to some spike distribs not being very gaussian (sometimes chans go from big at maxchan, to small, then to big again as you move away from maxchan, end up with solution way far out in x dimension)
        - try removing those chans (usu just one) that violate a 2D gaussian distrib, and fit only the remaining chans
            - how best to automate removal of bad chan(s)
        - instead of fitting a gaussian, try fitting something peakier, like an exponential?
            - or, instead of fitting the voltages, fit the squared voltages? prolly wouldn't help, already using squared errors
        - or, just go back to using openopt, and use constraints
            - slower, but seemed much more reliable
                - won't be modelling nearly as many points this time, speed might not be an issue
            - constraining sigma might be good enough
            - nope, doesn't seem to help
    - go back to spatial mean, but only use subset of chans for each spike to localize - say only the biggest 5 chans (or less if there aren't that many chans in the spike) - would this get rid of the artefactual clustering?
        - weight the chans nonlinearly - maybe do squared weights? Have I tried this already?
            - maybe weight each chan according to its squared distance from the maxchan?
    - build up a set of eigenvectors for each chan separately, get eigenvals for each chan, then use those in combination for each set of chans in a given spike to cluster - problem is you're still in a fairly high dimensional space, something like 3*nchansperspike
        - what about ICA instead?
    - Nick: model gaussian spike sources in 2D space, generate voltages at site locations, then add some noise. Now you know exactly where your "spikes" are in space, and you can test various methods against them to figure out which method works best to localize them, or which works best in certain situations

- tried using V**2 for spatial mean, to give more nonlinear weight to the big chans, so that the inclusion/exclusion of little dinky ones at the periphery wouldn't have such a dramatic effect, but using V**2 seemed to smudge out many of the clusters for some reason, didn't really investigate why
    - should still try some kind of extra nonlinear weighting (on top of channel signal amplitude) based on distance from maxchan, but this might cause issues for spikes with a maxchan that varies between two different chans (somewhere in between)
    - spatial mean seems very sensitive to lockout radius/maxnchansperspike - 9 seems a good num of chans to use for 2 col probes, need to test on 3 col. Using an odd number of chans per spike greatly reduces clusterability, but going down to only 7 also messes things up
- consider making V1 and V2 possible plot params to help distinguish a cluster that has two obviously different shaped spikes in it, that happen to have the same Vpp
- turn off rendering when changing the parameter for a dimension - takes forever to reposition each ellipsoid
- on 'f' down in cluster frame (focus camera), if a neuron is under the mouse, select that neuron in the clist, and maybe in the nlist too
    - or maybe make it a seperate key, like 's' for select - maybe make 's' focus and select, kind of like how 'x' focuses and moves a cluster
- make sort.align_neuron a neuron.align() method instead?
- sometimes spike shapes don't distinguish much on maxchan, but they do on other chans. Try sorting on multichan waveforms, not just maxchan - reduce dimensionality by just picking the peak points (whether simultaneously according to their position on maxchan, or not-so-simultaneously as stored in 2 dimension phasetis of each spike)
    - when picking points, need to take aligni into account. If alignis for two different spikes are different, need to find the dt that would let you keep them aligned and have the same aligni - ie, need to deduce the position of the missing 3rd phase on one of the two spikes
    - maybe best of all is give user ability to select which chan, or which channels, to do waveform clustering on
- add a way for user to indicate if he wants to keep the reclustering results of existing cluster(s), or if he wants to revert back to the previous results. Maybe implement this as a single level undo button. This means that every time you cluster on existing clusters, you need to save a whole bunch of stuff for the undo. Is this doable? Yes! Done!
    - motivation for this is that when you go to recluster, you're messing with sigma (and maybe rmerge), and you don't know what value of sigma will get you the results you want, so you have to try a bunch of different values. And every time you cluster, you can potentially lose spikes due to them falling into a scout that was eventually deleted for having too few spikes. This means you can't completely manually undo by simply rerunning the clustering at the previous sigma, since you've lost some spikes in the forward process, and have no way of gaining them back in the backward process
        - just to make sure, the density thresh doesn't matter while clustering, does it? I recluster based on cids, not nids, right? Yes, but perhaps I should indeed cluster based on nids instead of cids. This would prevent me from getting empty clusters when subclustering, due to some of them being composed of spikes that fall outside the density thresh
        - but overall, I have a feeling that density thresh isn't really necessary any more, if good cleaning can take its place
    - NVS does something like this too (but only for splitting clusters based on waveforms), except he has a sort of preview mode where you have to hit apply to actually apply the new results. This makes more sense for the user, and means potentially less unnecessary data manipulation, but for spyke it would mean you now have the potential for the GUI to be out of synch with the actual state of the clustering, and I think this would be confusing to code, and confusing for the user too.
- maybe mess with how waveform data is normalized before high-D clustering. Normalize all dims by std of chan with biggest std?
- find the set of chans all selected spikes have in common, then for each of those chans find the two peak times in the template, and extract voltages for each spike at those chans and times, and then cluster in 2 x nchans space
    - or better: use KS test to determine which of the timepoints on which of the chans are the least gaussian and hence most useful for clustering - see extract_all_wcs()
    - KS test ain't so great - a distrib can be highly deviant from Guassian and have a high KS value, but can still be unimodal, and hence not very useful for clustering
        - measure multimodality somehow
        - measure entropy: -sum(p*log(p))
            # the higher the entropy, the wider the distrib, since it takes more bits
            # to encode it. But wider distrib doesn't necessarily mean more clusterable...
            # maybe search around first and 2nd point for the
            nbins = intround(10*log2(npoints))
            p, b, l = hist(data[:, :, 25].ravel(), bins=50)
            p, bins = np.histogram(data, nbins)
            p = p[p != 0] # prevent singularities
            p = np.float64(p)
        - measure DJS, but that's quite similar to KS test I think
    - could be that including useless dimensions in waveform clustering allows gradient ascent clustering to use those crappy dimensions for climbing, and erroneously cluster spikes that are quite different on one or two chans, but very similar on other chans (which are the ones providing the useless dims). Need a clusterability measure to choose decent dims, and throw away the rest. Maybe set some information threshold that each dim needs to meet in order to be used for clustering
        - in this spirit, go back to finding peak times in template, but have user choose which chans to cluster upon. So, you're clustering in an even lower dim space, but really only clustering using the best possible dimensions. See if this improves waveform clustering
- capture Del key press in both neuron and cluster lists
- NVS compares the two most similar cluster templates, based I guess on their least squares difference, so you can decide if anything is oversplit, and you can merge them, and then look at the next two most similar cluster templates...
    - could do this more simply by sorting clusters in order of y location, and scrolling through them in order to see if consecutive-ish ones appear too similar
- add simple merge button (^), so you don't have to do a climb with a big sigma to get the same effect
- try reducing number of points during 'wave' cleaning
- should really investigate if I'm freezing points too early/too late. I'm checking amount of movement per dimension, not total Euclidean d**2 movement. This is technically incorrect, although probably a bit faster. Look at usage of "minmove" variable in climbing.pyx
- allow waveform clustering on chans excluding the template maxchan
- could also try using Cauchy distrib for density estimation during the gradient ascent clustering, instead of Gaussian, since it's a drop-in replacement - it's quite a bit faster
- multithread scout movement step (.) if nscouts*ndims are sufficiently large, and hence each step sufficiently slow to make it worthwhile to split its for loop up
- multithread assignment of unclustered points to nearest clustered point, and point density calculations
- BUG: sometimes when undoing a cluster cleaning, I get get back far fewer spikes than were in the original cluster. Hard to reproduce.
    - repeatable: try clustering 156 with 121 spikes with "peaks" on ch30 and ch22, sigma=0.14. Then undo, and you're down to only 28 spikes
        - looks like the problem was the "cid" field was getting out of sync with "nid" field in spikes array, during cluster deletion, merger, etc. Really should just get rid of the "cid" field altogether, or at least stop using it
        - sid 1562 has 58 in its "nid" field but neuron 58 doesn't have 1562 in its sids array
        - strategy to fix existing data: set all nid and cid entries to -1, then iterate over neurons and set nid fields according to sids in each neuron. Leave cid field as -1 always, and eventually, remove it altogether from the spikes array
- when renumbering, should deselect the old number and reselect whatever number that cluster became
- implement multiple undo/redo, saving multiple clusterstates in a stack
    - need to have full old/new symmetry for each cluster state attrib, and then need to reverse meaning of new and old on redo
    - also need to keep an eye on the stack and trim it when it gets too long, to save memory
- have an alternative export for Nick's use: single binary file with alternating spike time, spike channel, and spike cluster assignment values, all int64.
- all message records have both a timestamp (in us), and a datetime stamp. So, I can subtract the timestamp from the datetime stamp of any message record in each .srf file, and get the datetime correspondence of t=0us for that .srf file
- get rid of filelock, which won't really work 100% during multi-file detection, and is often very annoying for the user, and sometimes confusing in the code. Replace with process.acquire() and process.release() to allow a process to acquire and release a lock before and after it grabs data from its stream, to prevent multiple processes from accessing the disk at the same time, regardless of whether they're accessing the same file. This would also work very nicely in the single process case. Acquiring a lock on a single process doesn't do anything, I think. Just get the current process as usual, and call its appropriate methods
    - actually, what I'd really like is a disk lock. Not a file lock, or a process lock. I want other processes to continue crunching while up to 1 process is accessing the disk.
    - it looks like either my W510 disk is fast enough (compared to the desktop quad core's disk), or Win7 is smart enough, that concurrent disk access doesn't seem to happen at all anymore, even without any locking!
- remove direction arg for detection - will simplify code, and never use it anyway
    - maybe do the same for FoundEnoughSpikesError, maxnspikes, findspike, randomsample. Also, no need to update nspikes until detection is done
- get rid of ResampleFileStream too, since it isn't used any more
- simplify Stream.__getitem__ by always returning the full num timepoints between start and end, even for non-contiguous recordings. Fill any timegaps with zeros. This will also remove current TrackStream requirement that all .srf files be contiguous
- add ability to export LFP to .lfp file
    - use np.save of 2D array of all selected LFP chans, include chans field, and maybe float uVperAD field for conversion from int16 to float. Base name should be .srf name
- fix shape mismatch when scrolling to far left or right in TrackStream
- give TrackStream a fname attrib, and same for Stream. These would store the .track and .srf fname respectively. Then, also give each a srffnames attrib, which would be a list of srf fnames (which for a Stream would be a degenerate case with length 1)
- prepend .sort file with .srf or .track fname. Eg: track7c_test.track_2010-10-08_15.31.08_full.sort
- replace tend everywhere with t1, more symmetric with t0
- localization of a very small fraction of spikes is way off for some reason - this makes cluster plot annoying because it's scaled to include them. Why are they so poorly localized? I think, because their signal across chans is weird, and not really localizable. Just force them to use their spatial mean and be done with it
- use ts.tranges to skip over time gaps during detection across a whole track - potentially do the same for plotting raw data
- concatenating results at end of detection results in MemoryError. Results seemed to be about 10GB, and these have to be temporarily copied during concatenate. I guess instead of doing a flat call, I could init an empty final contiguous array, and fill it up one piece at a time by iterating over each 10 sec entry in the results list. Then, before going onto the next, delete that entry in the results list. This way, you do a small allocation, followed by a small deallocation, and you never have to duplicate all memory at any point in time.
- why does LFP now end about 75 or 100 ms too soon in the LFP frame? This resulted from a bug in interpreting NumSamples for lowpassmultichanrecords
- even leaving out the last few problematic recordings, during long track7c detection, it goes to completion, but then it just sits there without closing the pool and proceeding with the concatenation or results. Need to do long detection in single process mode (maybe not full debug to file, to save time), see if there's some error that's being supressed during pool.map()
    - or is it maybe something to do with the external drive? some kinda esata timeout?
    - here's the problem, discovered only when running in single process mode, in file '82 - track 7c predictexps.srf':
        - MainProcess: blockrange: [25057417000 25067419000], cutrange: [25057418000 25067418000]
          File "C:\bzr\spyke\dev\spyke\detect.py", line 330, in searchblock
              wavedata = wavedata[i] # ditto for wavedata
          IndexError: invalid index
    - this was due to not taking within-recording time gaps into account, and in this case, getting no spikes back from a detection blockrange that fell completely within one of these internal time gaps. In multiprocess detection, the error was not visible, because it doesn't halt execution, and it scrolled off the top of the command window buffer by the time detection had psudo-finished.
    - this is fixed by giving each Stream (not just each TrackStream) its own tranges, to allow detection to avoid
- something's wrong with track7c, near the end. The tranges don't work out correctly. Do search from 62513902000 to 63376585960
    - file 95c is the problem
    - looks like timestamps of user message records generated during a period when data wasn't actively being recorded to disk are messed up in Surf, and given a TimeStamp of 0, even when the clock is still running during a pause. Their datetime stamps seem correct though. So, I shoud avoid using user message records to calculate the datetime of the start of acquisition in each .srf file
- create a TrackStream and do one super huge spike sorting on all spikes from all recordings in a given track, but then export them back out one recording at a time, per usual
    - 10 million spikes would be about 10GB of waveform data + 1GB of spike param data - might need more RAM!
- demarcate cluster centers by the median of the positions of the points belonging to them, not by sometimes using the position of the cluster's scout point, or sometimes by calculating its mean (long tails suggest the median might be more appropriate than the mean)
    - add a cluster.updatePosScale() method for updating both raw and normalized pos and size based on median and stdev of up to samplesize spikes belonging to neuron. Call this after clustering, reclustering, and merging
- add a "cluster unsampled spikes" option (default checked) to allow fast test clustering with low sampling using various params, without wasting time assigning unsampled spikes after gradient ascent is done
- add a mode where you can split a cluster purely by chan ids. Often you're in a situation where there's only 1 or 2 chans common to all the spikes in a cluster, yet you want to cluster according to some other chan that looks very clusterable. This would let you do that, at least as a pre-cluster step.
- BUG: for some reason, clusters with only one spike don't display anything in the nslist (they should display the one spike), and hitting R causes spike 0 to be plotted. This is similar to the behaviour you get when returning something other than an int (like a numpy scalar) in model.data() for any neuron with any number of spikes
    - not checking for sids != [] in model.data() fixed it for some reason
- take equal number of samples per cluster when random sampling two clusters
- do best fit alignment of cluster's spikes to its mean - NVS claims this gives more faithful cluster mean waveform representation than just leaving all member spikes aligned to either their max or min peak. Should also help improve cluster cleaning when reclustering by waveform shape
    - calculate stdev of distribution of errors between each spike and the cluster mean, report in uV
    - take each spike and shift it forward and back by only 1 (or at most a few) timepoints (need access to .srf file for this)
        - maybe align each spike by up to -dt/2 < phase0 < phase1 < +dt/2, where dt is time between phases and is specific to each spike. If you let it realign too much, you risk aligning to some feature that was locked out during detection, which would result in reporting the same spike twice
    - for each spike, choose the realignment (if any) that minimizes error between spike and cluster mean
    - repeat until stdev stops changing
- NVS climbing improvement: instead of normalizing gradient by nneighs, normalize by sum of value of  gaussian of all points in the neighbourhood - says it converges much more quickly
- find cluster most similar to currently selected cluster
    - use error of overlapping chans of the two cluster waveforms
        - overlapping chans to consider should only be ones with significant signal on them - NVS uses maxchan + immediate neighbours
        - GUI: most similar clusters: add prev and next buttons to sort toolbar, maybe bind < and > keys
- WISHLIST: allow merging of selected unsorted spikes
- BUG: focus current cluster doesn't work right for new clusters created after a focus event (ie after _dfocus becomes non-zero)
    - after creating and deleting a whole bunch of single point clusters from a fresh detection, spike and cluster focus seems to work fine
    - somewhat related - if sort window is in focus instead of cluster window, hitting X or C doesn't update the glwidget immediately, have to bring glwidget into focus first
    - should really just do the focusing stuff properly, ala VTK. Constantly manipulating the point data is just bad, and very confusing in many cases
        - so this means I need to keep track of current pitch, roll, yaw, and position in glWidget?
            - nope, just need to translate to focus before doing any rotation, and then translate back
- WISHLIST: insert newly split off clusters in nlist
- it would be nice to see the waveforms of spikes thrown away during climbing (for being part of a cluster with too few points)
    - could make a -1 cluster that shows up in the nlist, that only contains the spikes thrown away in the last climb run. Then, could just randomly sample them like any other neuron's spikes.
    - It should be deletable, and undo/redoable, etc. Make it act as much as possible as a genuine cluster.
    - Not sure how the junk spikes will be coloured. -1 doesn't work in the colourdict does it? Maybe expand colourdict to make -1 map to grey.
- handle case where Detector.searchblock() returns 0 spikes (due possibly to very short trange that it searches over), and also case where Detector.detect() returns 0 spikes
- WISHLIST: hitting ESC once clears selected spikes, hitting it a 2nd time clears selected neurons. If no spikes selected, hitting ESC once clears selected neurons right away. Also, if 2 neurons are selected and being compared, hitting ESC should clear the 2ndary neuron selection
- WISHLIST: if no neurons are selected, "R" should random sample unsorted spikes
- on renumbering, if nids are all contiguous and ordered, don't bother renumbering, which means don't clear the undo cache. This makes hitting renumber less risky
- save electrode type to ptcs header descr field
- add ability to only do PCA on selected chans, for both plotting and clustering, would need to add pcchans attrib to sort in addition to pcsids and pcs
- BUG: parsing a .srf file individually, vs as part of a track, and then loading it the other way from the saved .parse file raises a "'File' object has no attribute 'f'" error in surf.py at "self.f.seek(record['dataoffset'])"
    - this was due to fnames being overwritten when loading a srff from a .parse file, and different fnames being used for the normal vs the .track case (.track fnames have ../ prepended)
- use chansplit '/' button manually to do initial maxchan based clustering, don't make this an automatic step when doing maxchan based PCA
- use sx data from each spike to decide what kind of channel inclusion radius to use (relative to maxchan) for choosing significant channels for PCA
    - find inclusion radius for given set of spikes such that the radius you choose is >= 95% of the sx values of those spikes
    - confirmed that this is a good strategy by looking at a few different maxchans/clusters, and plotting the sx distributions of their member spikes
    - but, allow user to override channel selection. If any specific channels are selected, use only those. If not, use those automatically chosen as described above
- allow designation of multiunit clusters, give them increasingly -ve nids, and don't export them by default
    - see spikes on mid depth channels in ptc18.tr1 - you can partition them up by maxchan, but then when you plot them in PC space, there really are no distinct clusters. It's just one big mess, and that's the way it should be labelled. Don't waste time trying to find clusters that aren't there!
    - maybe make the '-' or whatever button toggle whether a cluster is -ve or +ve, and hence MU or SU
        - if the -ve of the current cluster already exists, find the next highest absolute value nid that's available, and take -ve of it
    - would need to make changes in other parts of the code that assume all clusters are >-1. Like cluster renumbering. -ve numbered clusters shouldn't be discarded, only -1 cluster should
    - could conflict a bit with -1 junk cluster, complicated. Might be best to make 0 the junk cluster, +ve values single unit, and -ve values multiunit
- NVS: perhaps Cauchy kernel isn't the best idea, because it has heavy tails, you really should have a bigger rneigh to capture that heavy tail.
    - switched back to Gaussian
- update all saved .sort files
    - do merge on all cluster 0s, and then renumber. Don't allow old cluster 0s to be accidentally treated as junk
    - set all spikes with -1 nids to 0:
    sids, = np.where(self.sort.spikes['nid'] == -1)
    self.sort.spikes['nid'][sids] = 0
    self.DeColourPoints(sids)
    self.UpdateClustersGUI()
- plot only usids that best match current cluster out of all clusters
- force match error hist to close when closing spyke
- compare junk spikes to clusters, if their rms error is small enough, include them in the best fitting cluster. User interface:
    - user works on one cluster at a time, hits a match button that selects and plots all junk spikes (up to a max number, and then subsamples, and maybe plots all matching clusters too?) whose rmserror <= fairly strict user set threshold (AND which match that cluster best, out of all clusters?), in uVrms (editable spin edit). This would allow for merging, or realignment and recalculation of rmserror before merging. Maybe put these two controls on a match tab? Should also plot a histogram of rmserrors for all spikes that best match the current cluster, rmserror thresh control could be a slider under the histogram, using the same scale
        - or thresh control could just be clicking (and optionally holding) on the hist plot, with a vertical line plotted at current thresh, and maybe a printout of the value somewhere, perhaps as a spinedit
        - should this be on a match tab, or have its own match window?
- reinstate freezing of points - disabling it seems to have caused a big slowdown for clusters with > 10k spikes
- use a Gaussian lookup table. Building it might take a little while. Only do this for say > 10k spikes, or whatever the optimal cutoff is
- make sort pane right click clear selected chans
- when clusters are selected, cluster plot just those selected clusters. This is already the case for PCs, but really it should be the case for any dimensions, such as x0 y0 Vpp. When none are selected, (try to) cluster plot all clusters at once. (This generally isn't possible for PCs since all clusters don't have sufficiently overlapping chans)
- when picking number of PCA components to stick into ICA, choose ndims <= sqrt(nspikes) to make sure ICA has good convergence
- make Align best align only according to signal on selected chans - if none selected, select chans according to average mean and radius of selected spikes by calling get_selchans
- add .sort filename, file date time, and file size to ptcs header.descr dict
- big bug: if you've got only a subset of a cluster's spikes selected (say only 20 out of 1000s), and you hit cluster (say that subset of 20 from cluster 2 turns into clusters [0, 56]), you delete everything else that was still in cluster 2!
    - ah, when I hit cluster, the original cluster 2 with all its spikes is deleted, but only the selected 20 live on to turn into other clusters. The balance are set as having nid = 0. I should stop deleting the original, but that would require big changes to both apply_clustering and ApplyClusterChange, both of which are very touchy. Instead, just disallow clustering of only a subset of a cluster's spikes
- make ESC in main window propagate to Sort window, to clear current selection
- make double-click/ENTER of nid in nlist trigger the cluster button
- have a system that marks spike waveforms as "dirty" in the .wave file, so you only go resave those that really need to be resaved, instead of rewriting the whole (potentially multi GB) file each time
    - sort.dirtysids should be a set()
    - writing:
        1. call np.format.read_magic to get .npy format version
        2. assert that .npy format is 1.0
        3. call read_array_header_1_0 to move pointer to start of array in file.  Assert shape and dtype are identical to array in memory. Call f.tell() to get start of array offset.
        4. for each dirty sid (sids should be sorted), calculate offset from start of file (array offset plus sid offset), f.seek() by that amount relative to start of file, write bytes to disk
    - for testing, make some sids dirty, save to existing .wave file by modifying it, then go and write another full one from wavedata in memory. Then check that the two files are indeed identical.
- test dirty spikes functionality for max/min alignment - need to have .srf file open for this
- calculate a cluster pair overlap index using means and stdevs of the two clusters I guess. This would assume normal distribs, which they aren't necessarily.
    - unsure whether a single threshold setting will work well for all cluster pairs
    - this overlap index sounds exactly like discussion with Nick on 2011-10-05 regarding an error measure between pairs of clusters in PC space
    - calc projection of each point onto line connecting centers of the two clusters:
        - find center of first cluster, subtract center coord from points of each cluster, and find center of 2nd cluster wrt first (which means taking 2nd center coord and subtracting the 1st)
        - line connecting the two is now represented by 2nd center coord
        - take dot product of each point with this vector to get projection of each point onto line
        - take stdev of all dot products for this 1st cluster
        - repeat above for 2nd cluster
        - take average of the two stdevs, or maybe just their sum
        - find distance between centers (sqrt(sum(sqr distance vector)))
        - if distance < 2*average stdev, try again in ICA space
            - or, maybe nicer to use the sum of the stdevs, instead of the average, and then check if distance < sum stdevs. Get exact same result either way
        - if it's still the case, maybe they should be merged. stop and leave it to the user
        - overlap index itself could be reported as ratio between sum(stdevs) and the distance. A value >= 1 indicates significant overlap
- make cleaning histogram automatically use nbins = sqrt(npoints). This has worked really nicely in the cluster histogram
- disable cluster dimension list by default with checkbox, and if it's disabled, use the 3 dimensions selected in the plot dimension combo boxes - this will avoid the common situation where I'm plotting in one space, yet clustering in another, which is annoying and a bit dangerous
- for really big clusters >100k, merge step is still too slow
    - might be good to have a feature where you randomly split the one and only one selected cluster in half. Might have to do it a couple of times to get reasonable sizes. Then cluster each one separately, and then merge results appropriately
    - maybe the split should happen automatically if >100k
- make mpl window one of the tool windows, and always plot to it, ie always reuse plots and overwrite what might already be displayed there. Place it in a convenient spot relative to the other tool windows
- if channels are selected, make findMostSimilarCluster compare clusters only on those chans - no more, no less. If not, do as it currently does, which is grab all chans that both clusters have in common
- make SHIFT+Del delete only explicitly selected spikes (technically, split the cluster into a new cluster plus a junk cluster 0)
- closing MPL window doesn't synch up toggle state of MPL window button in toolbar
- replace mpl Lines with LineCollection for speed. See demo/line_collections.py
- bug: open .srf file, and then open a corresponding .sort file - amplitude of LFP drops considerably for some reason. Must be something to do with some gain setting in the stream saved in the sort file?
    - LFP panel was using the wrong AD2uV converter after opening a .sort file
- add std to waveform means - use continuous transparent region around mean, of the same colour. Should be much more quantitative to compare the amount of scatter between two clusters, especially between a cluster and a newly split off junk cluster. The alternative, overplotting say 20 spikes at a time per selected cluster and visually estimating the std, isn't as quantitative.
- add xyz axes to cluster plot
- make xyz axes show the size of the current setting for sigma. Make the length of the axes correspond to sigma. This would even make it feasible to make a good estimate of different sigmas in different dimensions
    - this could save a lot of time having to do lots of iteration trying to find an optimum sigma for really big clusters
- ctrl+scroll in cluster window should modify sigma, step size should be same as scrolling it in the main spyke window
- make SPACE a keyboard shortcut in sort and cluster windows to cluster the data, so as to avoid having to move mouse over to main spyke window
- have a clustering layout toggle button in the View menu and the main toolbar that auto arranges windows in an alternate way, more ideal for clustering. Size the MPL window same as the main spyke window. Put it below the main window. Then put Sort window (full height) at right. Then take whatever horizontal space is left, and make that the cluster window width. Its height is the sum of the heights of the MPL and main windows, and it's positioned at the top. This all keeps the controls as close as possible to the cluster window, keeps the cluster window fairly large and centered in the screen, and it keeps a space below all the windows for the terminal messages to remain visible. 
- add an optional "notes" attrib to each Neuron, so you can jot down stuff about the cluster, like although it looks like it should be split, it's really just one neuron that's drifted over time. This would save you time having to figure out why you hadn't split it previously, saves duplication
    - or better yet, devise a way of marking clusters that you know/feel are good and clean, maybe by changing their text colour in the nlist. Give each neuron a "good" boolean flag, toggled by G key. Don't prevent changes to it, make it purely a reminder from the user to the user
- save actual wavestd values to .ptcs files, now that it's being calculated and displayed during sorting
- make channel selection more visible in sortpanel
- during best fit alignment, message claims fake data is being used on either side, even when the correct .srf file is open. What's going on? This is by design. It's a lot faster to use fake data. Reload manually if desired.
- need a reload button to reload all currently selected spikes from stream
- if any components (PCs/ICs) are selected for plotting, and you plot with no spikes or neurons selected, just return. Otherwise it tries to find common chans for all spikes in sort which can take a long time, and always eventually fails anyway
- add sort slider
    - make spike slider click select spikes at window position zero. Otherwise, you need to slide to at least position 1 before any spikes are selected
- keyboard movement of sliders doesn't seem to trigger sliderMoved(). Same goes for mouse scrolling - use valueChanged instead
- when called during select keypress, pick() should return all points under the brush, not just the first one found. This would help make painting larger areas of points a lot easier
- save and restore chan selection and component analysis selection on .sort file save and load
- make center click random sample
- add ability to align a whole selection of spikes (maybe via cluster selection) to their mean waveform's global min or max, on the selected single channel. This should then simply shift the alignment of all member spikes by a fixed amount left or right, therefore shifting the mean's alignment a fixed amount left or right. Shape shouldn't change. This is for spikes/clusters which are well aligned among themselves, but their max/min isn't centered. Maybe make CTRL+align_min and CTRL+align_max trigger this kind of mean waveform alignment
- save "good" state as an attribute of each ClusterChange
    - just add "good" list to save_old and save_new, and then make use of it in ApplyClusterChange
- key bindings:
    - P: PCA
    - I: PCA+ICA
    - C: c0c1c2
    - T: c0c1t
    - spike slider: ENTER activates plot button
- when checking to see if should use saved PCs/ICs for plotting or clustering, would probably be best to use a hash of the chans and spike ids and whatever else is needed, instead of relying on just the chans and cluster ids, which I think I currently do, to signify that the previous set of calculated components are valid for the currently requested set of spikes and chans
- let user designate just how much of each waveform to consider for dimension reduction. Often, I only want the central 0.25 or 0.5 ms, I don't care about other stuff outside of that (like near conincident spikes). Put a spin box or drop down list in Sort toolbar, use that to control the length of the green selection lines in the sort panel (normal unselected horizontal vref lines can stay full length). Use only timepoints within that window for dimension reduction.
- make alignbest() consider only the selected waveform time span in incltComboBox
- make scroll in spikesortpanel cycle through tis dropdown, ie, just send scroll events to this dropdown
- if more than X number of spikes are selected in nslist or uslist, don't plot them. Then, when it comes time to unselect those spikes, don't raise the error from the plot panel that those plots are missing
    - in fact, maybe all such missing plot errors should be suppressed - they can be very annoying, and don't help the user fix any stray plots that haven't been cleared
- save inclt to .sort file
- make more use of the component analysis cache: make sort.X a dict of arrays (instead of just one), with MD5 hash values as keys. Storing up to say 10 at a time shouldn't have much impact on memory usage. Even making it unlimited shouldn't do much to memory usage.
- make template comparisons only consider the selected waveform time span in incltComboBox
- save window layout to .sort file
- sort window gain combo box
- NDsep calc can take 8 sec for a pair of clusters of 50k points each, even though it's multithreaded. Should NDsep subsample, or does it already?
    - it keeps up to Nmax of the first points - increase speed by calling it with a decreased Nmax of 25000. To reduce bias, make it skip an integer number of points when subsampling, instead of just taking the first Nmax points
- optimize alignbest() - can be very slow (10s of seconds) for very big clusters (100s of thousands of spikes) - switch to Cython and/or multiprocessing/multithreading
rename MEANWAVESAMPLESIZE to MEANWAVEMAXSAMPLES, set to 2000, and subsample by stepping by N // MEANWAVEMAXSAMPLES + 1 points, as in NDsepmetric(). This makes calculating a neuron's mean waveform faster *and* deterministic.
- on merge, instead of the final cluster acquiring the id of the lowest id'd cluster, it should take on the id of the biggest cluster
- density split: lets you split a pair of clusters at the point between their centers where the number of spikes is a minimum
    - problem with GAC sometimes is that when you're splitting a pair of clusters and one is denser than the other, GAC divides them at a point further from the dense cluster than you'd like, ie past the point of lowest total spike density. Although this may be Bayesian optimal, and can be remedied somewhat by using a smaller sigma, but the problem persists. As during many of the calcs done for the cluster histogram in the MPL window, this solution proposes drawing a line between the centers of the two clusters and projecting all of their points onto that line. Then, histogram the projections (use nbins = sqrt(npoints), as usual), then find the smallest bin between the centers, and divide up the spikes according to what side of that division they fall
- use the same "breaking out of search loop along sorted dimension 0" logic from move_scout in merge_scouts on the first merge step, when all the scouts are still ordered along dimension 0, because they haven't moved yet. This could speed up the first merge (which is by far the slowest) significantly
- discovered the bug in naming LFP chans in SURF has hit us again (see 2008-11-13 note): in ptc18.14, and probably lots of others. Crap! This probably means I'll have to go in and manually edit lots of .srf files, and all their copies. I thought I had this problem figured out a long time ago. Some kind of sloppy memory management in surf causes characters from the previous LFP chan to remain for the next LFP chan - think it's a single digit number vs a 2 digit number problem. Maybe I should be putting leading zeros in the LFP chan field names in the surf .cfg files...
    - here's a pretty easy way around it in spyke that doesn't require going through all the .srf files and modifying them:
        - for a given probe type (1a, 2a, whatever the layout happens to be), if you parse out a list of chans that matches a known hard-coded "bad" LFP chan list, then simply replace it with the appropriate correct hard-coded LFP chan list
        - just have to go through a few .srf files, (one per track, or really just one per probe ID, since LFP chan layout stays constant across tracks for a given probe), and create the bad to good chanlist mapping
- to decide which dimension to sort by for clustering, instead of using variance, could use the dimension whose dot product with the view normal vector is a minimum (or whose cross product with it is a max)? Or could just always stick with dimension 0, and make the user swap dimensions as he sees fit. That's what I've settled on.
- consider implementing NVS's measure of cluster overlap. for a pair of clusters:
    o_kl = (1 - p_k)/(1 - e_kl) where p_k = u_k/N_k and u_k is the number of points in cluster k whose nearest neighbour is also in k. Cluster k is the smaller of k and l (N_k < N_l). e_kl = N_k / (N_k + N_l)
    - this wouldn't suffer from the 1D limitation that projecting onto the line connecting the cluster centers has. But, it would be slow, because for each point in each cluster
    - implemented the complement, a separation measure called NDsep
- BUG: make dropping into IPython (1.0.dev) work again
- BUG: make scale bars observe voltage display gain
- BUG: make stdev around the mean in sort panel observe voltage display gain
- asymmetric time windows:
    - check:
        - getting wavedata from xCA
        - findmostsimilar()
        - align()
        - Sort.reloadSpikes()
        - SortWindow.inclt
        - SortWindow.tis
        - Sort.TW
        - Sort.twts
        - Sort.twi
    - rename sort.TW to sort.tw
    - when updating MainWindow.spiketw, make sure to also update Sort.tw
        - when updating Sort.tw, make sure to also update Sort.twts and Sort.twi by calling Sort.calc_twts_twi()
        - reload spikes in existing sorts to new time windows, but first update t0, t1, and tis spikes array fields appropriately
    - add time window as a user adjustable parameter in GUI? No, leave it as an infrequently used command line option (SpykeWindow.update_spiketw)
- add ability to vary the number of PCs to keep per channel before doing ICA. Scholz2004 suggests that more input PCs into ICA isn't always better, so 7 per channel may be too many in some cases.
- make "+" split off selected spikes: same as deleting selected spikes and then promoting them to a cluster
- BUG: for files with multiple experiments in them, multiple .din and .textheader are not being exported. Just the first one is being exported
- add a step that tests for duplicate spikes within each neuron (single or multiunit), and maybe even just duplicate spikes in general in the spikes list
    - do diff on spike times for all spikes in each neuron
    - if diff == 0, of the pair, find the spike whose channels most overlap with those of the neuron template, or whose maxchan is closest to x, y position of the neuron. keep that one, discard the other (set it to nid 0, but there's more to it than that, need to find the right functions)
    - there's already a clean ISIs widget in verify tab, now make it work
- try PCA and FastICA implementations in sklearn instead of MDP - there are indications they might be faster.
    - FastICA is faster in sklearn, PCA seems to be slower
- to help clean out noise events, make one of the cluster dimension rms error wrt to the template, and leave the other two as PCs or ICs
- add direct support for .dat files + some kind of meta data file (.json)
- add "common average referencing" cited in Kilosort paper, ref: Ludwig...Kipke, 2009, J Neurophysiol
- on delete key press in cluster and sort windows, if any spikes are explicitly selected, remove only them, otherwise remove selected neuron(s)
- on ESC, if one of the selected clusters is cluster 0, deselect that one first




end of DONE
-----------
