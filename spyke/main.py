"""Main spyke window"""

from __future__ import division
from __future__ import print_function
from __init__ import __version__

__authors__ = ['Martin Spacek', 'Reza Lotun']

import numpy as np
import pyximport
pyximport.install(build_in_temp=False, inplace=True)
from gac import gac # .pyx file
import util # .pyx file

# instantiate an IPython embedded shell which shows up in the terminal on demand
# and on every exception:
from IPython.terminal.ipapp import load_default_config
from IPython.terminal.embed import InteractiveShellEmbed
config = load_default_config()
# automatically call the pdb debugger after every exception, override default config:
config.TerminalInteractiveShell.pdb = True
ipshell = InteractiveShellEmbed(display_banner=False, config=config)

from PyQt4 import QtCore, QtGui, uic
from PyQt4.QtCore import Qt
getSaveFileName = QtGui.QFileDialog.getSaveFileName
getExistingDirectory = QtGui.QFileDialog.getExistingDirectory
SpykeUi, SpykeUiBase = uic.loadUiType('spyke.ui')

from scipy.misc import comb

import pylab as pl
from matplotlib.backends.backend_qt4agg import FigureCanvasQTAgg as FigureCanvas
from matplotlib.backends.backend_qt4agg import NavigationToolbar2QT as NavigationToolbar
from matplotlib.figure import Figure

import scipy.stats
import os
from os.path import join
import sys
import platform
import time
import datetime
import gc
import cPickle
import random
from copy import copy
from struct import unpack

# seems unnecessary: automatically add spyke to path
#spykepath = os.path.split(os.getcwd())[0] # parent dir of cwd
#sys.path.insert(0, spykepath)

import core
from core import toiter, tocontig, intround, MICRO, ClusterChange, SpykeToolWindow
from core import DJS, g, SimpleStream
import surf
from sort import Sort, SortWindow, MAINSPLITTERPOS, VSPLITTERPOS, NSLISTWIDTH
from sort import MEANWAVEMAXSAMPLES, NPCSPERCHAN
from plot import SpikePanel, ChartPanel, LFPPanel
from detect import Detector
from extract import Extractor

DEFSPIKETW = -400, 600 # spike window temporal window (us)
DEFCHARTTW = -25000, 25000 # chart window temporal window (us)
DEFLFPTW = -500000, 500000 # lfp window temporal window (us)
SLIDERTRES = 100 # slider temporal resoluion (us), slider is limited to 2**32 ticks

SCREENWIDTH = 1920 # TODO: this should be found programmatically
#SCREENHEIGHT = 1080 # TODO: this should be found programmatically
WINDOWTITLEHEIGHT = 26 # TODO: this should be found programmatically
BORDER = 2 # TODO: this should be found programmatically
SPIKEWINDOWWIDTHPERCOLUMN = 80
SPIKEWINDOWHEIGHT = 655 + 2*BORDER # TODO: this should be calculated from SCREENHEIGHT
CHARTWINDOWSIZE = 900+2*BORDER, SPIKEWINDOWHEIGHT
LFPWINDOWSIZE = 250+2*BORDER, SPIKEWINDOWHEIGHT
#SHELLSIZE = CHARTWINDOWSIZE[0], CHARTWINDOWSIZE[1]/2
CLUSTERWINDOWHEIGHT = 700

MAXRECENTFILES = 10 # anything > 10 will probably mess up keyboard accelerators
WINDOWUPDATEORDER = ['Spike', 'LFP', 'Chart'] # chart goes last cuz it's slowest

# if updating at least this many select spikes in .wave file, update them all
# instead for speed:
NDIRTYSIDSTHRESH = 200000


class SpykeWindow(QtGui.QMainWindow):
    """spyke's main window, uses gui layout generated by QtDesigner"""
    def __init__(self):
        QtGui.QMainWindow.__init__(self)
        self.ui = SpykeUi()
        self.ui.setupUi(self) # lay it out
        self.groupMenuSamplingRates()
        self.addRecentFileActions()
        self.updateRecentFiles()
        
        self.move(0, 0) # top left corner, to make space for data windows

        self.streampath = os.getcwd() # init
        self.sortpath = os.getcwd() # init
        for d in ('~/data', '/data'): # use first existing of these paths, if any
            path = os.path.expanduser(d)
            if os.path.exists(path):
                self.streampath = path
                self.sortpath = path
                break
        self.windows = {} # holds child windows
        self.spiketw = DEFSPIKETW # spike window temporal window (us)
        self.charttw = DEFCHARTTW # chart window temporal window (us)
        self.lfptw = DEFLFPTW # lfp window temporal window width (us)
        self.t = None # current time position in recording (us)

        self.hpstream = None
        self.lpstream = None

        self.cchanges = core.Stack() # cluster change stack, for undo/redo
        self.cci = -1 # pointer to cluster change for the next undo (add 1 for next redo)

        self.SetClusteringDims(['c0', 'c1', 'c2'])

        self.dirtysids = set() # sids whose waveforms in .wave file are out of date
        
        # disable most widgets until a .srf or .sort file is opened
        self.EnableStreamWidgets(False)
        self.EnableSortWidgets(False)
        
    def addRecentFileActions(self):
        """Init recent file QActions and insert them into the right place in the
        File menu. Leave them invisible until needed"""
        self.recentFileActions = []
        for i in range(MAXRECENTFILES):
            action = QtGui.QAction(self)
            action.setVisible(False)
            action.triggered.connect(self.OpenRecentFile)
            self.recentFileActions.append(action)
            self.ui.menuFile.insertAction(self.ui.actionSaveSort, action)
        self.ui.menuFile.insertSeparator(self.ui.actionSaveSort)

    def groupMenuSamplingRates(self):
        """Group sampling rates in sampling menu into a QActionGroup such that only
        one is ever active at a time. This isn't possible to do from within
        QtDesigner 4.7, so it's done here manually instead"""
        ui = self.ui
        samplingGroup = QtGui.QActionGroup(self)
        samplingGroup.addAction(ui.action25kHz)
        samplingGroup.addAction(ui.action50kHz)
        samplingGroup.addAction(ui.action100kHz)

    @QtCore.pyqtSlot()
    def on_actionNew_triggered(self):
        self.DeleteSort() # don't create a new one until spikes exist

    @QtCore.pyqtSlot()
    def on_actionOpen_triggered(self):
        getOpenFileName = QtGui.QFileDialog.getOpenFileName
        fname = getOpenFileName(self, caption="Open stream or sort",
                                directory=self.streampath,
                                filter="Surf, track, tsf, mat & sort files "
                                       "(*.srf *.track *.tsf *.mat *.sort );;"
                                       "All files (*.*)")
        fname = str(fname)
        if fname:
            self.OpenFile(fname)

    @QtCore.pyqtSlot()
    def on_actionSaveSort_triggered(self):
        try:
            self.sort
        except AttributeError: # sort doesn't exist
            return
        if self.sort.fname:
            self.SaveSortFile(self.sort.fname) # save to existing sort fname
        else:
            self.on_actionSaveSortAs_triggered()

    @QtCore.pyqtSlot()
    def on_actionSaveSortAs_triggered(self):
        """Save sort to new .sort file"""
        defaultfname = os.path.join(self.sortpath, self.sort.fname)
        if self.sort.fname == '': # sort hasn't been previously saved
            # generate default fname with hpstream.fname and datetime
            fname = self.hpstream.fname.replace(' ', '_')
            dt = str(datetime.datetime.now()) # get an export timestamp
            dt = dt.split('.')[0] # ditch the us
            dt = dt.replace(' ', '_')
            dt = dt.replace(':', '.')
            defaultfname += fname + '_' + dt + '.sort'
        fname = getSaveFileName(self, caption="Save sort As",
                                directory=defaultfname,
                                filter="Sort files (*.sort);;"
                                       "All files (*.*)")
        fname = str(fname)
        if fname:
            base, ext = os.path.splitext(fname)
            if ext != '.sort':
                fname = base + '.sort' # make sure it has .sort extension
            head, tail = os.path.split(fname)
            self.sortpath = head # update sort path
            # make way for new .spike and .wave files
            try: del self.sort.spikefname
            except AttributeError: pass
            try: del self.sort.wavefname
            except AttributeError: pass
            self.SaveSortFile(tail)

    @QtCore.pyqtSlot()
    def on_actionSaveParse_triggered(self):
        self.hpstream.pickle()

    @QtCore.pyqtSlot()
    def on_actionExportPtcsFiles_triggered(self):
        path = getExistingDirectory(self, caption="Export .ptcs file(s) to",
                                    directory=self.sortpath)
        path = str(path)
        if path:
            self.sort.exportptcsfiles(path, self.sortpath)
            # don't update path

    @QtCore.pyqtSlot()
    def on_actionExportGdfFiles_triggered(self):
        path = getExistingDirectory(self, caption="Export .gdf file(s) to",
                                    directory=self.sortpath)
        path = str(path)
        if path:
            self.sort.exportgdffiles(path)
            # don't update path

    @QtCore.pyqtSlot()
    def on_actionExportSpkFiles_triggered(self):
        path = getExistingDirectory(self, caption="Export .spk files to",
                                    directory=self.sortpath)
        path = str(path)
        if path:
            self.sort.exportspikes(path)
            # don't update path

    @QtCore.pyqtSlot()
    def on_actionExportTsChIdFiles_triggered(self):
        path = getExistingDirectory(self, caption="Export .tschid file(s) to",
                                    directory=self.sortpath)
        path = str(path)
        if path:
            self.sort.exporttschid(path)
            # don't update path

    @QtCore.pyqtSlot()
    def on_actionExportDIN_triggered(self):
        path = getExistingDirectory(self, caption="Export .din file(s) to",
                                    directory=self.sortpath)
        path = str(path)
        if path:
            ## TODO: if sort doesn't exist, make a temporary fake with hpstream
            ## as its stream. That's all that's needed.
            self.sort.exportdin(path)
            # don't update path

    @QtCore.pyqtSlot()
    def on_actionExportTextheader_triggered(self):
        path = getExistingDirectory(self, caption="Export .textheader file(s) to",
                                    directory=self.sortpath)
        path = str(path)
        if path:
            ## TODO: if sort doesn't exist, make a temporary fake with hpstream
            ## as its stream. That's all that's needed.
            self.sort.exporttextheader(path)
            # don't update path

    @QtCore.pyqtSlot()
    def on_actionExportAll_triggered(self):
        path = getExistingDirectory(self,
                                    caption="Export .ptcs, .din and .textheader file(s) to",
                                    directory=self.sortpath)
        path = str(path)
        if path:
            self.sort.exportall(basepath=path, sortpath=self.sortpath)
            # don't update path

    @QtCore.pyqtSlot()
    def on_actionExportSpikesZipFile_triggered(self):
        """Save selected spikes on selected channels and timepoints to
        binary .spikes.zip file"""
        self.exportSpikeWaveforms(format='binary')

    @QtCore.pyqtSlot()
    def on_actionExportSpikesCSVFile_triggered(self):
        """Save selected spikes on selected channels and timepoints to
        text .spikes.csv file"""
        self.exportSpikeWaveforms(format='text')

    def exportSpikeWaveforms(self, format):
        """Save selected spikes on selected channels and timepoints to
        binary .spikes.zip file or text .spikes.csv file"""
        if format == 'binary':
            ext = '.spikes.zip'
        elif format == 'text':
            ext = '.spikes.csv'
        else:
            raise ValueError("invalid format: %r" % format)
        defaultfname = os.path.join(self.sortpath, self.sort.fname)
        if defaultfname == '': # sort hasn't been previously saved
            # generate default fname with hpstream.fname and datetime
            fname = self.hpstream.fname.replace(' ', '_')
            dt = str(datetime.datetime.now()) # get an export timestamp
            dt = dt.split('.')[0] # ditch the us
            dt = dt.replace(' ', '_')
            dt = dt.replace(':', '.')
            defaultfname = fname + '_' + dt
        defaultfname = defaultfname + ext
        caption = "Export spike waveforms to %s %s file" % (format, ext)
        filter = "%s spike waveform files (*%s);;All files (*.*)" % (format, ext)
        fname = getSaveFileName(self, caption=caption,
                                directory=defaultfname,
                                filter=filter)
        fname = str(fname)
        if fname:
            before, sep, after = fname.partition(ext)
            if sep != ext:
                fname = before + ext # make sure it has extension
            sids = self.GetAllSpikes()
            selchans = self.get_selchans(sids)
            sw = self.OpenWindow('Sort') # in case it isn't already open
            tis = sw.tis
            self.sort.exportspikewaves(sids, selchans, tis, fname, format)

    @QtCore.pyqtSlot()
    def on_actionExportLFPZipFiles_triggered(self):
        self.exportLFPWaveforms(format='binary')

    @QtCore.pyqtSlot()
    def on_actionExportLFPCSVFiles_triggered(self):
        self.exportLFPWaveforms(format='text')

    def exportLFPWaveforms(self, format='binary'):
        """Export LFP waveform data to binary .lfp.zip file(s) or text .lfp.csv file(s)
        to user-designated basepath"""
        caption = "Export LFP waveforms to binary .lfp.zip files"
        basepath = getExistingDirectory(self, caption=caption, directory=self.sortpath)
        basepath = str(basepath)
        if not basepath:
            return
        try: # self.lpstream is a TrackStream?
            streams = self.lpstream.streams
        except AttributeError: # self.lpstream is a normal Stream
            streams = [self.lpstream]
        if format == 'binary':
            ext = '.lfp.zip'
        elif format == 'text':
            ext = '.lfp.csv'
        else:
            raise ValueError("invalid format: %r" % format)
        print('exporting LFP waveform data to:')
        for stream in streams:
            path = os.path.join(basepath, stream.srcfnameroot)
            try: os.mkdir(path)
            except OSError: pass # path already exists?
            fullfname = os.path.join(path, stream.srcfnameroot+ext)
            s = stream
            wave = s[s.t0:s.t1]
            if format == 'binary':
                chanpos = s.probe.siteloc_arr()
                uVperAD = s.converter.AD2uV(1)
                with open(fullfname, 'wb') as f:
                    np.savez_compressed(f, data=wave.data, chans=wave.chans, t0=s.t0,
                                        t1=s.t1, tres=s.tres, chanpos=chanpos,
                                        uVperAD=uVperAD)
            elif format == 'text':
                np.savetxt(fullfname, wave.data, fmt='%d', delimiter=',') # data should be int
            else:
                raise ValueError('unknown format: %r' % format)
            print(fullfname)

    def update_sort_version(self):
        """Update self.sort to latest version"""
        s = self.sort
        v = float(s.__version__) # sort version
        lv = float(__version__) # latest version
        if v > lv:
            raise RuntimeError('versioning error')
        if v == lv:
            print('no update necessary')
            return
        if v < 0.3:
            print("can't auto update from sort version < 0.3")
            return
        if v == 0.3:
            v = self.update_0_3_to_0_4()
        if v == 0.4:
            v = self.update_0_4_to_0_5()
        if v == 0.5:
            v = self.update_0_5_to_0_6()
        if v == 0.6:
            v = self.update_0_6_to_0_7()
        print('now save me!')
            
    def update_0_3_to_0_4(self):
        """Update sort 0.3 to 0.4:
            - reload all spike waveforms and fix all of their time values
        """        
        print('updating sort from version 0.3 to 0.4')
        s = self.sort
        sids = np.arange(s.nspikes)
        s.reloadSpikes(sids)
        # add sids to the set of dirtysids to be resaved to .wave file:
        self.dirtysids.update(sids)
        s.__version__ = '0.4' # update
        print('done updating sort from version 0.3 to 0.4')
        return float(s.__version__)
        
    def update_0_4_to_0_5(self):
        """Update sort 0.4 to 0.5:
            - rename sort.sortfname to sort.fname
        """
        print('updating sort from version 0.4 to 0.5')
        s = self.sort
        s.fname = s.sortfname
        del s.sortfname
        s.__version__ = '0.5' # update
        print('done updating sort from version 0.4 to 0.5')
        return float(s.__version__)

    def update_0_5_to_0_6(self):
        """Update sort 0.5 to 0.6:
            - rename sort.spikes field names 'phasetis' and 'dphase' to
              'tis' and 'dt' respectively
            - remove unused 'cid', 's0' and 's1' fields from sort.spikes, reorder fields
        """
        print('updating sort from version 0.5 to 0.6')
        s = self.sort
        names = list(s.spikes.dtype.names) # convert from tuple
        phasetis_index = names.index('phasetis')
        dphase_index = names.index('dphase')
        assert (phasetis_index, dphase_index) == (13, 19)
        names[phasetis_index] = 'tis' # rename 'phasetis' to 'tis'
        names[dphase_index] = 'dt' # rename 'dphase' to 'dt'
        s.spikes.dtype.names = names # checks length and auto converts back to tuple
        # also rename fields in detector's SPIKEDTYPE:
        for i in [phasetis_index, dphase_index]:
            field = list(s.detector.SPIKEDTYPE[i])
            field[0] = names[i]
            s.detector.SPIKEDTYPE[i] = tuple(field)

        # new name order, leaves out unused 'cid', 's0' and 's1'
        newnames = ['id', 'nid', 'chan', 'nchans', 'chans', 'chani', 't', 't0', 't1', 'dt',
                    'tis', 'aligni', 'V0', 'V1', 'Vpp', 'x0', 'y0', 'sx', 'sy']
        olddtype = s.detector.SPIKEDTYPE # list of tuples
        oldnames = [ field[0] for field in olddtype ]
        newdtype = []
        for name in newnames:
            newdtype.append(olddtype[oldnames.index(name)])
        s.detector.SPIKEDTYPE = newdtype # replace detector's SPIKEDTYPE
        newspikes = np.empty(s.spikes.shape, dtype=newdtype)
        from numpy.lib import recfunctions as rfn
        newspikes = rfn.recursive_fill_fields(s.spikes, newspikes) # copy from old to new
        s.spikes = newspikes # overwrite

        # in cluster.pos and .normpos, remove 's0' and 's1', and rename 'dphase' to 'dt':
        for c in s.clusters.values():
            c.pos.pop('s0')
            c.pos.pop('s1')
            c.pos['dt'] = c.pos.pop('dphase')
            c.normpos.pop('s0')
            c.normpos.pop('s1')
            c.normpos['dt'] = c.normpos.pop('dphase')

        s.__version__ = '0.6' # update
        print('done updating sort from version 0.5 to 0.6')
        return float(s.__version__)

    def update_0_6_to_0_7(self):
        """Update sort 0.6 to 0.7:
            - replace sort.TW class attribute with sort.tw instance attribute
        """
        print('updating sort from version 0.6 to 0.7')
        s = self.sort
        # Sort.TW class attrib was (-500, 500) in version 0.6
        s.tw = -500, 500
        s.__version__ = '0.7' # update
        print('done updating sort from version 0.6 to 0.7')
        return float(s.__version__)

    @QtCore.pyqtSlot()
    def on_actionCloseSort_triggered(self):
        # TODO: add confirmation dialog if Sort not saved
        self.CloseSortFile()
        print('closed sort')

    @QtCore.pyqtSlot()
    def on_actionCloseStream_triggered(self):
        self.CloseStream()
        print('closed stream')

    @QtCore.pyqtSlot()
    def on_actionQuit_triggered(self):
        self.close()
        #self.destroy() # no longer seems necessary, causes segfault

    def closeEvent(self, event):
        self.on_actionCloseSort_triggered()
        self.on_actionCloseStream_triggered()
        QtGui.QMainWindow.closeEvent(self, event)

    def keyPressEvent(self, event):
        key = event.key()
        try:
            sw = self.windows['Sort']
        except KeyError:
            QtGui.QMainWindow.keyPressEvent(self, event) # pass it on
        if key == Qt.Key_Escape: # deselect all spikes and all clusters in Sort window
            sw.clear()
        elif key == Qt.Key_R: # doesn't fire when certain widgets have focus
            sw.on_actionSelectRandomSpikes_activated()

    @QtCore.pyqtSlot()
    def on_actionUndo_triggered(self):
        """Undo button click. Undo previous cluster change"""
        try:
            cc = self.cchanges[self.cci]
        except IndexError:
            print('nothing to undo')
            return
        print('undoing: %s' % cc.message)
        self.ApplyClusterChange(cc, direction='back')
        self.cci -= 1 # move pointer one change back on the stack
        print('undo complete')

    @QtCore.pyqtSlot()
    def on_actionRedo_triggered(self):
        """Redo button click. Redo next cluster change"""
        try:
            cc = self.cchanges[self.cci+1]
        except IndexError:
            print('nothing to redo')
            return
        print('redoing: %s' % cc.message)
        self.ApplyClusterChange(cc, direction='forward')
        self.cci += 1 # move pointer one change forward on the stack
        print('redo complete')

    @QtCore.pyqtSlot()
    def on_actionSpikeWindow_triggered(self):
        """Spike window toggle menu/button event"""
        self.ToggleWindow('Spike')

    @QtCore.pyqtSlot()
    def on_actionChartWindow_triggered(self):
        """Chart window toggle menu/button event"""
        self.ToggleWindow('Chart')

    @QtCore.pyqtSlot()
    def on_actionLFPWindow_triggered(self):
        """LFP window toggle menu/button event"""
        self.ToggleWindow('LFP')

    @QtCore.pyqtSlot()
    def on_actionSortWindow_triggered(self):
        """Sort window toggle menu/button event"""
        self.ToggleWindow('Sort')

    @QtCore.pyqtSlot()
    def on_actionClusterWindow_triggered(self):
        """Cluster window toggle menu/button event"""
        self.ToggleWindow('Cluster')

    @QtCore.pyqtSlot()
    def on_actionMPLWindow_triggered(self):
        """Matplotlib window toggle menu/button event"""
        self.ToggleWindow('MPL')

    @QtCore.pyqtSlot()
    def on_actionShell_triggered(self):
        """Shell window toggle menu/button event"""
        #self.ToggleWindow('Shell')
        # FIXME: this blocks until you Ctrl-D out of ipython:
        ipshell()

    @QtCore.pyqtSlot()
    def on_actionRasters_triggered(self):
        """Spike rasters toggle menu event"""
        self.ToggleRasters()

    @QtCore.pyqtSlot()
    def on_actionTimeRef_triggered(self):
        """Time reference toggle menu event"""
        self.ToggleRef('TimeRef')

    @QtCore.pyqtSlot()
    def on_actionVoltageRef_triggered(self):
        """Voltage reference toggle menu event"""
        self.ToggleRef('VoltageRef')

    @QtCore.pyqtSlot()
    def on_actionScale_triggered(self):
        """Scale toggle menu event"""
        self.ToggleRef('Scale')

    @QtCore.pyqtSlot()
    def on_actionCaret_triggered(self):
        """Caret toggle menu event"""
        self.ToggleRef('Caret')

    @QtCore.pyqtSlot()
    def on_action25kHz_triggered(self):
        """25kHz menu choice event"""
        self.SetSampfreq(25000)

    @QtCore.pyqtSlot()
    def on_action50kHz_triggered(self):
        """50kHz menu choice event"""
        self.SetSampfreq(50000)

    @QtCore.pyqtSlot()
    def on_action100kHz_triggered(self):
        """100kHz menu choice event"""
        self.SetSampfreq(100000)

    @QtCore.pyqtSlot()
    def on_actionSampleAndHoldCorrect_triggered(self):
        """Sample & hold menu event"""
        enable = self.ui.actionSampleAndHoldCorrect.isChecked()
        self.SetSHCorrect(enable)

    #def onFilePosLineEdit_textChanged(self, text): # updates immediately
    def on_filePosLineEdit_editingFinished(self): # updates on Enter/loss of focus
        text = str(self.ui.filePosLineEdit.text())
        try:
            t = self.str2t[text]
        except KeyError: # convert to float to allow exp notation shorthand
            t = float(text)
        self.seek(t)

    @QtCore.pyqtSlot()
    def on_actionAboutSpyke_triggered(self):
        lf = open('../LICENSE', 'r')
        LICENSE = lf.read()
        lf.close()
        system = """<p>Python %s, Qt %s, PyQt %s<br>
                    %s</p>""" % (platform.python_version(),
                                 QtCore.QT_VERSION_STR, QtCore.PYQT_VERSION_STR,
                                 platform.platform())
        text = """
        <h2><a href=http://swindale.ecc.ubc.ca/spyke>spyke</a> %s</h2>
        <p>A tool for neuronal waveform visualization and spike sorting</p>

        <p>Copyright &copy; 2008-2013 Martin Spacek, Reza Lotun<br>
           <a href=http://swindale.ecc.ubc.ca>Swindale</a> Lab,
           University of British Columbia</p>

        <p>Some functionality inherited from Tim Blanche's Delphi program "SurfBawd".</p>

        <p>Many icons were copied from Ubuntu's <a
        href=http://launchpad.net/humanity>Humanity</a> icon theme.</p>

        <p>%s</p>

        %s""" % (__version__, LICENSE, system)
        QtGui.QMessageBox.about(self, "About spyke", text)

    @QtCore.pyqtSlot()
    def on_actionAboutQt_triggered(self):
        QtGui.QMessageBox.aboutQt(self)

    @QtCore.pyqtSlot()
    def on_filePosStartButton_clicked(self):
        self.seek(self.str2t['start'])

    @QtCore.pyqtSlot()
    def on_filePosEndButton_clicked(self):
        self.seek(self.str2t['end'])

    @QtCore.pyqtSlot(int)
    def on_slider_valueChanged(self, slideri):
        t = slideri * SLIDERTRES
        self.seek(t)

    @QtCore.pyqtSlot()
    def on_detectButton_clicked(self):
        """Detect pane Detect button click"""
        sort = self.CreateNewSort() # create a new Sort
        self.get_detector() # update Sort's current detector with new one from widgets
        if sort.detector.extractparamsondetect:
            self.init_extractor() # init the Extractor
        sort.spikes, sort.wavedata = sort.detector.detect() # struct array of spikes, 3D array
        sort.update_usids()
        sort.sampfreq = sort.stream.sampfreq # lock down sampfreq and shcorrect attribs
        sort.shcorrect = sort.stream.shcorrect

        self.ui.progressBar.setFormat("%d spikes" % sort.nspikes)
        self.EnableSortWidgets(True)
        sw = self.OpenWindow('Sort') # ensure it's open
        if sort.nspikes > 0:
            self.on_plotButton_clicked()

    def init_extractor(self):
        """Initialize Extractor"""
        #XYmethod = self.XY_extract_radio_box.GetStringSelection()
        XYmethod = 'Gaussian fit' # hard code for now, don't really need extract pane
        ext = Extractor(self.sort, XYmethod) # or eventually, self.get_extractor()
        self.sort.extractor = ext
        #self.update_extractor(ext) # eventually, update extractor from multiple Extract pane widgets

    def OnXYExtract(self, evt=None):
        """Extract pane XY Extract button click. Extracts (or re-extracts and
        overwrites) XY parameters from all sort.spikes, and stores
        them as spike attribs"""
        try:
            self.sort.extractor
        except AttributeError:
            self.init_extractor()

        #import cProfile
        #cProfile.runctx('self.sort.extractor.extract_all_XY()', globals(), locals())

        self.sort.extractor.extract_all_XY() # adds extracted XY params to sort.spikes
        self.windows['Sort'].uslist.updateAll() # update any columns showing param values
        self.EnableSpikeWidgets(True) # enable cluster_pane

    def OnWaveletExtract(self, evt=None):
        """Extract pane wavelet Extract button click. Extracts (or re-extracts and
        overwrites) wavelet coefficients from all sort.spikes, and stores
        them as spike attribs"""
        try:
            self.sort.extractor
        except AttributeError:
            self.init_extractor()

        #import cProfile
        #cProfile.runctx('self.sort.extractor.extract_all_XY()', globals(), locals())

        # extract coeffs of selected wavelet type, add coeffs to sort.spikes
        wavelet = self.wavelet_extract_radio_box.GetStringSelection()
        self.sort.extractor.extract_all_wcs(wavelet)
        self.windows['Sort'].uslist.updateAll() # update any columns showing param values
        self.EnableSpikeWidgets(True) # enable cluster_pane

    def OnTemporalExtract(self, evt=None):
        """Extract pane temporal Extract button click. Extracts (or re-extracts and
        overwrites) temporal params from all sort.spikes, and stores
        them as spike attribs"""
        try:
            self.sort.extractor
        except AttributeError:
            self.init_extractor()

        self.sort.extractor.extract_all_temporal()
        self.windows['Sort'].uslist.updateAll() # update any columns showing param values
        self.EnableSpikeWidgets(True) # enable cluster_pane

    @QtCore.pyqtSlot()
    def on_clusterButton_clicked(self):
        """Cluster pane Cluster button click"""
        s = self.sort
        spikes = s.spikes
        #sids = self.GetAllSpikes() # all selected spikes
        # always cluster all spikes in existing clusters, don't just cluster some subset,
        # since existing clusters are always deleted in apply_clustering and
        # ApplyClusterChange, and spikes that aren't in that subset would inadvertantly
        # become unsorted
        sids = np.concatenate([self.GetClusterSpikes(), self.GetUnsortedSpikes()])
        sids.sort()
        oldclusters = self.GetClusters() # all selected clusters
        if len(sids) == 0: # nothing selected
            sids = spikes['id'] # all spikes (sorted)
            oldclusters = s.clusters.values() # all clusters
        dims = self.GetClusteringDims()
        comps = np.any([ dim.startswith('c') and dim[-1].isdigit() for dim in dims ])
        subsidss = [] # sids grouped into subclusters, each to be clustered separately
        msgs = []
        t0 = time.time()
        if comps and np.all(sids == spikes['id']): # doing PCA/ICA on all spikes
            if not oldclusters:
                print("no existing clusters to sequentially do PCA/ICA on and subcluster")
                return
            # partition data by existing clusters before clustering,
            # restrict to only clustered spikes:
            for oldcluster in oldclusters:
                subsidss.append(oldcluster.neuron.sids)
                msgs.append('oldcluster %d' % oldcluster.id)
            sids = np.concatenate(subsidss) # update
            sids.sort()
        else: # just the selected spikes
            subsidss.append(sids)
            msgs.append('%d selected sids' % len(sids))
        nids = self.subcluster(sids, subsidss, msgs, dims)
        print('clustering took %.3f sec' % (time.time()-t0))
        self.apply_clustering(oldclusters, sids, nids, verb='GAC')

    def subcluster(self, sids, subsidss, msgs, dims):
        """Perform (sub)clustering according to subsids in subsidss. Incorporate results
        from each (sub)clustering into a single nids output array"""
        # init nids output array to be all unclustered:
        nids = np.zeros(len(sids), dtype=np.int32)
        for subsids, msg in zip(subsidss, msgs):
            print('clustering %s on dims %r' % (msg, dims))
            subnids = self.gac(subsids, dims) # subclustering result
            ci = subnids > 0 # consider only the clustered sids
            subsids = subsids[ci]
            subnids = subnids[ci]
            nidoffset = max(nids) + 1
            nidsi = sids.searchsorted(subsids)
            nids[nidsi] = subnids + nidoffset
        return nids

    def chansplit(self):
        """Split spikes into clusters of unique channel combinations"""
        s = self.sort
        spikes = s.spikes
        sids = self.GetAllSpikes() # all selected spikes
        oldclusters = self.GetClusters() # all selected clusters
        if len(sids) == 0: # nothing selected
            sids = spikes['id'] # all spikes (sorted)
            oldclusters = s.clusters.values() # all clusters
        t0 = time.time()
        chans = spikes[sids]['chans']
        chans = tocontig(chans) # string view won't work without contiguity
        # each row becomes a string:
        strchans = chans.view('S%d' % (chans.itemsize*chans.shape[1]))
        # each row in uchancombos is a unique combination of chans:
        uchancombos = np.unique(strchans).view(chans.dtype).reshape(-1, chans.shape[1])
        if len(uchancombos) == 1:
            print("selected spikes all share the same set of channels, can't chansplit")
            return
        # init to unclustered, shouldn't be any once done:
        nids = np.zeros(len(sids), dtype=np.int32)
        for comboi, uchancombo in enumerate(uchancombos):
            nids[(chans == uchancombo).all(axis=1)] = comboi + 1
        if (nids == 0).any():
            raise RuntimeError("there shouldn't be any unclustered points from chansplit")
        print('chansplit took %.3f sec' % (time.time()-t0))
        self.apply_clustering(oldclusters, sids, nids, verb='channel split')

    def densitysplit(self):
        """Split cluster pair by density along line between their centers in current
        cluster space"""
        s = self.sort
        spikes = s.spikes
        oldclusters = self.GetClusters() # all selected clusters
        if len(oldclusters) != 2:
            print("need to select exactly 2 clusters to split them by density")
            return
        dims = self.GetClusterPlotDims()
        try:
            X, sids = self.get_param_matrix(dims=dims)
        except RuntimeError, errmsg:
            print(errmsg)
            return

        nids = s.spikes['nid'][sids] # copy
        unids = np.unique(nids)
        assert len(unids) == 2
        # centers of both clusters, use median:
        i0 = nids == unids[0]
        i1 = nids == unids[1]
        c0 = np.median(X[i0], axis=0) # ndims vector
        c1 = np.median(X[i1], axis=0)
        # line connecting the centers of the two clusters, wrt c0
        line = c1-c0
        line /= np.linalg.norm(line) # make it unit length
        #print('c0=%r, c1=%r, line=%r' % (c0, c1, line))
        proj = np.dot(X-c0, line) # projection of each point onto line
        nbins = max(intround(np.sqrt(len(proj))), 2) # good heuristic
        #print('nbins = %d' % nbins)
        hist, edges = np.histogram(proj, bins=nbins)
        ei0, ei1 = edges.searchsorted((np.median(proj[i0]), np.median(proj[i1])))
        # find histogram min between cluster medians:
        threshi = hist[ei0:ei1].argmin()
        thresh = edges[ei0:ei1][threshi]
        #print('thresh is %.3f' % thresh)
        #print('ei0, ei1: %d, %d' % (ei0, ei1))
        assert ei0 < ei1 # think this is always the case because projections are wrt c0
        nids[proj < thresh] = unids[0] # overwrite nid values in nids, since it's a copy
        nids[proj >= thresh] = unids[1]
        self.apply_clustering(oldclusters, sids, nids, verb='density split')

    def randomsplit(self):
        """Randomly split each selected cluster in half. This is done to increase
        gac() speed"""
        oldclusters = self.GetClusters() # all selected clusters
        subsidss = []
        for cluster in oldclusters:
            subsidss.append(cluster.neuron.sids)
        sids = np.concatenate(subsidss)
        sids.sort()
        destsubsidss = []
        for subsids in subsidss:
            np.random.shuffle(subsids) # shuffle in-place
            spliti = len(subsids) // 2
            destsubsids0 = subsids[:spliti]
            destsubsids0.sort() # sids should always go out sorted
            destsubsidss.append(destsubsids0)
            destsubsids1 = subsids[spliti:]
            destsubsids1.sort()
            destsubsidss.append(destsubsids1)
        # init to unclustered, shouldn't be any once done:
        nids = np.zeros(len(sids), dtype=np.int32)
        for i, destsubsids in enumerate(destsubsidss):
            nids[sids.searchsorted(destsubsids)] = i + 1
        if (nids == 0).any():
            raise RuntimeError("there shouldn't be any unclustered points from randomsplit")
        self.apply_clustering(oldclusters, sids, nids, verb='randomly split')

    def gac(self, sids, dims):
        """Cluster sids along dims, using NVS's gradient ascent algorithm"""
        s = self.sort
        waveclustering = np.any([ dim.startswith('pk') for dim in dims ])
        if waveclustering: # do waveform clustering
            if len(dims) > 1:
                raise RuntimeError("Can't do high-D clustering of spike waveforms in tandem with any other spike parameters as dimensions")
            wctype = dims[0]
            try:
                data = self.get_waveclustering_data(sids, wctype=wctype)
            except RuntimeError as msg:
                print(msg)
                return
        else: # do spike parameter (non-wavefrom) clustering
            data, sids = self.get_param_matrix(sids=sids, dims=dims, scale=True)
        data = tocontig(data) # ensure it's contiguous for gac()
        # grab gac() params and run it
        self.update_sort_from_cluster_pane()
        npoints, ndims = data.shape
        s.sigmasqrtndims = s.sigma * np.sqrt(ndims) # scale sigma with dimensionality
        print('clustering %d points in %d-D space' % (npoints, ndims))
        t0 = time.time()
        nids = gac(data, sigma=s.sigmasqrtndims, rmergex=s.rmergex, rneighx=s.rneighx,
                   alpha=s.alpha, maxgrad=s.maxgrad,
                   maxnnomerges=1000, minpoints=s.minpoints)
        # nids from gac() are 0-based, but we want our single unit nids to be 1-based,
        # to leave room for junk cluster at 0 and multiunit clusters at nids < 0. So add 1:
        nids += 1
        print('GAC took %.3f sec' % (time.time()-t0))
        return nids
    
    def get_selchans(self, sids):
        """Return user selected chans. If none, automatically select and
        return chans within a radius encompassing 95% percent of sx values in sids,
        centered on average position of sids. Could also use a multiple of this
        derived sx to select more or fewer chans"""
        spikes = self.sort.spikes
        panel = self.windows['Sort'].panel
        selchans = panel.chans_selected # a list
        selchans.sort()
        if selchans and panel.manual_selection:
            return selchans # always return whatever's manually selected
        sxs = spikes['sx'][sids]
        sxs = np.sort(sxs) # get a sorted copy
        sxi = int(len(sxs) * 0.95) # round down, index > ~95% percent of values
        sx = sxs[sxi]
        dm = self.sort.detector.dm # DistanceMatrix
        spos = np.vstack((spikes['x0'][sids], spikes['y0'][sids])).T # sids x 2
        meanpos = spos.mean(axis=0) # mean spike position
        chanpos = np.asarray(dm.coords) # positions of enabled chans
        d = np.sqrt(np.sum((chanpos - meanpos)**2, axis=1)) # Euclidean chan distances from meanpos
        selchans = sorted(dm.chans[d <= sx]) # chans within sx of meanpos
        print('selection center: %.1f, %.1f um' % (meanpos[0], meanpos[1]))
        print('selection radius: %.1f um' % sx)
        panel.chans_selected = selchans
        panel.update_selvrefs()
        panel.draw_refs()
        panel.manual_selection = False
        return selchans

    def apply_clustering(self, oldclusters, sids, nids, autogen=True, verb=''):
        """Delete old clusters and replace the existing clustering of the desired sids
        with their new nids"""
        s = self.sort
        spikes = s.spikes
        sw = self.windows['Sort']
        cw = self.windows['Cluster']
        
        # deselect all clusters before potentially deleting any unselected
        # clusters, to avoid lack of Qt selection event when selection values
        # (not rows) change. Also, deselect usids while we're at it:
        self.SelectClusters(s.clusters, on=False)
        sw.uslist.clearSelection()

        # delete junk cluster if it exists and isn't in oldclusters,
        # add this deletion to cluster change stack
        if 0 in s.clusters and 0 not in [ c.id for c in oldclusters ]:
            # save some undo/redo stuff
            message = 'delete junk cluster 0'
            cc = ClusterChange(s.neurons[0].sids, spikes, message)
            cc.save_old([s.clusters[0]], s.norder, s.good)
            # delete it
            s.remove_neuron(0)
            # save more undo/redo stuff
            cc.save_new([], s.norder, s.good)
            self.AddClusterChangeToStack(cc)
            print(cc.message)

        # save some undo/redo stuff
        message = '%s clusters %r' % (verb, [ c.id for c in oldclusters ])
        cc = ClusterChange(sids, spikes, message)
        cc.save_old(oldclusters, s.norder, s.good)

        # start insertion indices of new clusters from first selected cluster, if any
        unids = np.unique(nids)
        nnids = len(unids)
        insertis = [None] * nnids
        if len(oldclusters) > 0:
            startinserti = s.norder.index(oldclusters[0].id)
            insertis = range(startinserti, startinserti+nnids)

        if autogen:
            # delete old clusters
            self.DelClusters(oldclusters, update=False)

        # apply new clusters
        newclusters = []
        for nid, inserti in zip(unids, insertis):
            ii, = np.where(nids == nid)
            nsids = sids[ii] # sids belonging to this nid
            if autogen: # auto generate a new cluster
                if nid != 0:
                    nid = None # auto generate a new nid
                cluster = self.CreateCluster(update=False, id=nid, inserti=inserti)
            else: # try and keep current cluster
                try:
                    cluster = s.clusters[nid]
                except KeyError:
                    if nid != 0:
                        nid = None # auto generate a new nid
                    cluster = self.CreateCluster(update=False, id=nid, inserti=inserti)
            newclusters.append(cluster)
            neuron = cluster.neuron
            sw.MoveSpikes2Neuron(nsids, neuron, update=False)
            if len(nsids) == 0:
                raise RuntimeError('WARNING: neuron %d has no spikes for some reason'
                                   % neuron.id)
            cluster.update_pos()

        # save more undo/redo stuff
        cc.save_new(newclusters, s.norder, s.good)
        self.AddClusterChangeToStack(cc)

        # now do some final updates
        self.UpdateClustersGUI()
        if not np.all(sids == spikes['id']): # if clustering only some spikes,
            self.SelectClusters(newclusters) # select all newly created cluster(s)
        if np.all(sids == cw.glWidget.sids):
            self.ColourPoints(newclusters) # just recolour
        else:
            self.on_plotButton_clicked() # need to do a full replot
        cc.message += ' into %r' % [c.id for c in newclusters]
        print(cc.message)

    def get_waveclustering_data(self, sids, wctype='wave'):
        s = self.sort
        spikes = s.spikes

        # find which chans are common to all sids
        commonchans, chanslist = s.get_common_chans(sids)

        # get selected chans
        chans = self.get_selchans(sids)
        for chan in chans:
            if chan not in commonchans:
                raise RuntimeError("chan %d not common to all spikes, pick from %r"
                                   % (chan, list(commonchans)))
        nchans = len(chans)
        if nchans == 0:
            raise RuntimeError("no channels selected")
        print('clustering on chans %r' % list(chans))

        # collect data from chans from all spikes:
        nspikes = len(sids)
        nt = s.wavedata.shape[2]
        data = np.zeros((nspikes, nchans, nt), dtype=np.float32)
        for sii, sid in enumerate(sids):
            spikechans = chanslist[sii]
            spikechanis = np.searchsorted(spikechans, chans)
            data[sii] = s.wavedata[sid, spikechanis]

        # find mean waveform of selected spikes, evenly sampling for speed
        # if nspikes exceeds a threshold:
        if nspikes > MEANWAVEMAXSAMPLES:
            step = nspikes // MEANWAVEMAXSAMPLES + 1 
            print('get_waveclustering_data() sampling every %d spikes instead of all %d'
                  % (step, nspikes))
            siis = np.arange(0, nspikes, step) # eq'v to: np.arange(nspikes)[::step]
            template = data[siis].mean(axis=0)
        else:
            template = data.mean(axis=0)

        # TODO: add stdev2, stdev6 and stdev10 that use the most variable points
        # per chan, or somehow, the most non-gaussian points per chan

        if wctype == 'pk2':
            # use data at peaks of template
            ntis = 2
            tis = np.zeros((nchans, ntis), dtype=int)
            for chani in range(nchans):
                t0, t1 = np.sort([template[chani].argmin(), template[chani].argmax()])
                tis[chani] = t0, t1
        elif wctype == 'pk6':
            # use data at peaks of template, and before and after each peak
            ntis = 6
            tis = np.zeros((nchans, ntis), dtype=int)
            for chani in range(nchans):
                t1, t4 = np.sort([template[chani].argmin(), template[chani].argmax()])
                dt3 = max((t4 - t1) / 3.0, 1) # 1/3 the distance between peaks
                t0 = max(t1-dt3, 0)
                t2 = min(t1+dt3, nt-1)
                t3 = max(t4-dt3, 0)
                t5 = min(t4+dt3, nt-1)
                tis[chani] = intround([t0, t1, t2, t3, t4, t5])
        elif wctype == 'pk10':
            # use data at peaks of template, and before and after each peak
            ntis = 10
            tis = np.zeros((nchans, ntis), dtype=int)
            for chani in range(nchans):
                t2, t7 = np.sort([template[chani].argmin(), template[chani].argmax()])
                dt5 = max((t7 - t2) / 5.0, 1) # 1/5 the distance between peaks
                t0 = max(t2-2*dt5, 0)
                t1 = max(t2-dt5, 0)
                t3 = min(t2+dt5, nt-1)
                t4 = min(t2+2*dt5, nt-1)
                t5 = max(t7-2*dt5, 0)
                t6 = max(t7-dt5, 0)
                t8 = min(t7+dt5, nt-1)
                t9 = min(t7+2*dt5, nt-1)
                tis[chani] = intround([t0, t1, t2, t3, t4, t5, t6, t7, t8, t9])
        else:
            raise RuntimeError('unknown wctype %r' % wctype)

        print('tis =')
        print(tis)
        for chani in range(nchans):
            assert core.is_unique(tis[chani])
            # TODO: if it isn't unique, throw out the ti repeats per channel, and have
            # potentially a different number of tis per chan. Would have to change the
            # fancy indexing operation below...

        # grab each spike's data at tis, using fancy indexing
        # see core.rowtake() or util.rowtake_cy() for indexing explanation
        data = data[:, np.arange(nchans)[:, None], tis] # shape = nspikes, nchans, ntis
        data.shape = nspikes, -1 # reshape to 2D, ie flatten across chans

        # normalize by the std of the dim with the biggest std - this allows use of reasonable
        # value of sigma (~0.15), similar to param clustering, and independent of what the
        # amplifier gain was during recording
        norm = data.std(axis=0).max()
        data /= norm
        print('normalized waveform data by %f' % norm)
        return data

    @QtCore.pyqtSlot()
    def on_x0y0VppButton_clicked(self):
        """Cluster pane x0y0Vpp button click. Set plot dims to x0, y0, and Vpp"""
        self.SetPlotDims('x0', 'y0', 'Vpp')

    @QtCore.pyqtSlot()
    def on_c0c1c2Button_clicked(self):
        """Cluster pane c0c1c2 button click. Set plot dims to c0, c1, and c2"""
        s = self.sort
        if QtGui.QApplication.instance().keyboardModifiers() == Qt.ControlModifier:
            try:
                del s.X[s.get_Xhash(*self.get_Xhash_args())] # force recalc
            except (AttributeError, KeyError): pass
        self.SetPlotDims('c0', 'c1', 'c2')

    @QtCore.pyqtSlot()
    def on_c0c1tButton_clicked(self):
        """Cluster pane c0c1t button click. Set plot dims to c0, c1, and t"""
        s = self.sort
        if QtGui.QApplication.instance().keyboardModifiers() == Qt.ControlModifier:
            try:
                del s.X[s.get_Xhash(*self.get_Xhash_args())] # force recalc
            except (AttributeError, KeyError): pass
        self.SetPlotDims('c0', 'c1', 't')

    def SetPlotDims(self, x, y, z):
        """Set plot dimensions to x, y, z, and replot"""
        xi = self.ui.xDimComboBox.findText(x)
        yi = self.ui.yDimComboBox.findText(y)
        zi = self.ui.zDimComboBox.findText(z)        
        self.ui.xDimComboBox.setCurrentIndex(xi)
        self.ui.yDimComboBox.setCurrentIndex(yi)
        self.ui.zDimComboBox.setCurrentIndex(zi)
        self.on_plotButton_clicked() # replot

    def get_param_matrix(self, sids=None, dims=None, scale=True):
        """Given list of dims, get clustering parameter matrix according to
        current selection of sids and channels"""
        s = self.sort
        sw = self.OpenWindow('Sort') # in case it isn't already open
        cw = self.OpenWindow('Cluster') # in case it isn't already open
        comps = np.any([ dim.startswith('c') and dim[-1].isdigit() for dim in dims ])
        if sids == None:
            sids = self.GetAllSpikes() # only selected spikes
        if len(sids) == 0: # if none selected
            if comps: # if component analysis selected
                raise RuntimeError('need non-empty spike selection to do component analysis')
            else: # return all spike ids
                sids = self.sort.spikes['id']
        kind = None
        tis = None
        selchans = None
        if comps: # only do kind, tis and selchans grab if doing CA:
            kind = str(self.ui.componentAnalysisComboBox.currentText())
            tis = sw.tis # waveform time indices to include, centered on spike
            selchans = self.get_selchans(sids)
        X = s.get_param_matrix(kind=kind, sids=sids, tis=tis, selchans=selchans,
                               dims=dims, scale=scale)
        return X, sids

    def get_Xhash_args(self):
        """Return currently selected clustering paramater that would be used to generate the
        identifying hash for the dimension reduced matrix if it were to be calculated at this
        point in time"""
        sw = self.OpenWindow('Sort') # in case it isn't already open
        kind = str(self.ui.componentAnalysisComboBox.currentText())
        sids = self.GetAllSpikes() # only selected spikes
        tis = sw.tis # waveform time indices to include, centered on spike
        selchans = np.asarray(self.get_selchans(sids))
        chans = self.sort.get_common_chans(sids, selchans)[0]
        try:
            npcsperchan = self.sort.npcsperchan
        except AttributeError: # hasn't been set yet
            npcsperchan = NPCSPERCHAN
        return kind, sids, tis, chans, npcsperchan

    @QtCore.pyqtSlot()
    def on_plotButton_clicked(self):
        """Cluster pane plot button click. Plot points and colour them
        according to their clusters."""
        s = self.sort
        if QtGui.QApplication.instance().keyboardModifiers() == Qt.ControlModifier:
            try:
                del s.X[s.get_Xhash(*self.get_Xhash_args())] # force recalc
            except (AttributeError, KeyError): pass
        cw = self.OpenWindow('Cluster') # in case it isn't already open
        dims = self.GetClusterPlotDims()
        try:
            X, sids = self.get_param_matrix(dims=dims)
        except RuntimeError, errmsg:
            print(errmsg)
            return
        if len(X) == 0:
            return # nothing to plot
        nids = s.spikes['nid'][sids]
        cw.plot(X, sids, nids)
        sw = self.OpenWindow('Sort') # in case it isn't already open
        sw.PlotClusterHistogram(X, nids) # auto update cluster histogram plot

    @QtCore.pyqtSlot()
    def get_cleaning_density_hist(self):
        """Calculate histogram of point densities of selected spikes over selected
        clustering dimensions from origin"""
        dims = self.GetClusteringDims()
        X, sids = self.get_param_matrix(dims=dims)
        # each dim in X has 0 mean, so X is centered on origin
        X = np.float64(X) # convert to double precision
        ndims = X.shape[1]
        r = np.sqrt(np.square(X).sum(axis=1)) # all +ve values
        r /= r.std() # normalize to unit variance
        nbins = intround(np.sqrt(len(X))) # good heuristic
        rhist, edges = np.histogram(r, nbins) # distance hist, edges includes the right edge
        ledges = edges[:-1] # keep just the left edges, discard the last right edge
        assert len(ledges) == nbins
        binwidth = ledges[1] - ledges[0]
        # density histogram: npoints / fractional volume        
        dhist = np.float64(rhist) / np.diff(edges**ndims)
        dhist /= (dhist * binwidth).sum() # normalize to unit area
        return dhist, ledges, binwidth, ndims, sids, r

    @QtCore.pyqtSlot()
    def on_cleanHistButton_clicked(self):
        """Cluster pane cleaning hist button click. Plot histogram of point
        densities of selected spikes over selected clustering dimensions from origin,
        compare to Gaussian. Note that each time you reject points > nstds away
        from origin, the distrib may get less and less Gaussian, and more and more
        uniform"""
        dhist, ledges, binwidth, ndims, sids, r = self.get_cleaning_density_hist()
        ris = ledges + (binwidth / 2) # center values of bins
        gauss = g(0, 1, ris)
        gauss /= (gauss * binwidth).sum() # normalize to unit area
        djs = DJS(dhist, gauss)
        mplw = self.OpenWindow('MPL')
        a = mplw.ax
        a.clear()
        mplw.setWindowTitle('Density Histogram')
        a.bar(ledges, dhist, width=binwidth)
        a.plot(ris, gauss, '-') # plot Gaussian on top of density histogram
        a.set_title('%dD cluster density histogram, DJS = %.3f' % (ndims, djs))
        a.set_xlabel('nstdevs')
        a.set_ylabel('normalized density')
        mplw.f.tight_layout(pad=0.3) # crop figure to contents
        mplw.figurecanvas.draw()

    @QtCore.pyqtSlot()
    def on_cleanButton_clicked(self):
        """Cluster pane clean button click. Set as unsorted those points that fall outside
        of nstds distance away in the cluster density histogram plotted above"""
        # r vals are in nstds units:
        dhist, ledges, binwidth, ndims, sids, r = self.get_cleaning_density_hist()
        nstds = self.ui.cleanNstdsSpinBox.value()
        nids = self.sort.spikes[sids]['nid']
        unids = np.unique(nids)
        oldclusters = [ self.sort.clusters[unid] for unid in unids ]
        nids[r > nstds] = 0 # set some sids to cluster 0, ie unclustered
        self.apply_clustering(oldclusters, sids, nids, verb='clean')

    @QtCore.pyqtSlot()
    def on_calcMatchErrorsButton_clicked(self):
        """Match pane calc button click. Calculate rmserror between all clusters and
        all unsorted spikes. Also calculate which cluster each unsorted spike matches best"""
        spikes = self.sort.spikes
        wavedata = self.sort.wavedata
        cids = np.sort(self.sort.clusters.keys())
        sids = self.sort.usids.copy()
        ncids, nsids = len(cids), len(sids)
        print('calculating rmserror between all %d clusters and all %d unsorted spikes'
              % (ncids, nsids))
        errs = np.empty((ncids, nsids), dtype=np.float32)
        errs.fill(np.inf) # TODO: replace with sparse matrix with np.inf as default value
        for cidi, cid in enumerate(cids):
            neuron = self.sort.neurons[cid]
            for sidi, sid in enumerate(sids):
                chan = spikes['chan'][sid]
                nchans = spikes['nchans'][sid]
                chans = spikes['chans'][sid][:nchans]
                # TODO: this is a bit wasteful if no chans are in common:
                sdata = wavedata[sid, :nchans]
                try:
                    ndata, sdata = neuron.getCommonWaveData(chan, chans, sdata)
                except ValueError: # not comparable
                    continue
                errs[cidi, sidi] = core.rms(ndata - sdata)
        errs = self.sort.converter.AD2uV(errs) # convert from AD units to uV, np.infs are OK
        self.match = Match(cids, sids, errs)
        print('done calculating rmserror between all %d clusters and all %d unsorted spikes'
              % (ncids, nsids))
        return self.match
        
    @QtCore.pyqtSlot()
    def on_plotMatchErrorsButton_clicked(self):
        """Match pane plot match errors button click. Plot histogram of rms error between
        current cluster and all unclustered spikes that best fit the current cluster"""
        cluster = self.GetCluster()
        cid = cluster.id
        if not hasattr(self, 'match') or self.match == None:
            self.match = self.on_calcMatchErrorsButton_clicked() # (re)calc
        errs = self.match.get_best_errs(cid)
        if len(errs) == 0:
            print('no unsorted spikes fit cluster %d' % cid)
            return
        f = pl.gcf()
        pl.clf()
        f.canvas.parent().setWindowTitle('cluster %d rmserror histogram' % cid)
        binsize = self.ui.matchErrorPlotBinSizeSpinBox.value()
        pl.hist(errs, bins=np.arange(0, 50, binsize))
        pl.title('rmserrors between cluster %d and %d unsorted spikes' %
                 (cid, len(errs)))
        pl.xlabel('rmserror (uV)')
        pl.ylabel('count')
        
    @QtCore.pyqtSlot()
    def on_matchButton_clicked(self):
        """Deselect any selected unsorted spikes in uslist, and then select
        unsorted spikes that fall below match error threshold and fit the
        current cluster best"""
        cluster = self.GetCluster()
        cid = cluster.id
        if not hasattr(self, 'match') or self.match == None:
            self.match = self.on_calcMatchErrorsButton_clicked() # (re)calc
        errs = self.match.get_best_errs(cid)
        if len(errs) == 0:
            print('no unsorted spikes fit cluster %d' % cid)
            return
        bestsids = self.match.best[cid]
        thresh = self.ui.matchThreshSpinBox.value()
        sids = bestsids[errs <= thresh]
        sidis = self.sort.usids.searchsorted(sids)
        # clear uslist selection, select sidis rows in uslist
        sw = self.windows['Sort']
        sw.uslist.clearSelection()
        sw.uslist.selectRows(sidis, on=True, scrollTo=False)
        print('matched %d spikes to cluster %d' % (len(sids), cid))

    @QtCore.pyqtSlot()
    def on_plotXcorrsButton_clicked(self):
        """Plot all cross/auto correlograms for all selected neurons, and display
        them in an upper or lower triangle configuration"""
        ## TODO: for now, just plot a single autocorrelogram
        clusters = self.GetClusters()
        xsids = clusters[0].neuron.sids
        if len(clusters) == 1:
            autocorr = True
            ysids = xsids # x and y are identical
        elif len(clusters) == 2:
            autocorr = False
            ysids = clusters[1].neuron.sids
        else:
            raise NotImplementedError("can't deal with more than one xcorr for now")
        xspikets = self.sort.spikes['t'][xsids]
        yspikets = self.sort.spikes['t'][ysids]

        ## TODO: spikes['t'][sids] is very different from spikes[sids]['t'] !
        ## The first is C contig, the second is not! The first probably makes a copy,
        ## while the second does not. First is much much faster for array ops, while
        ## the second conserves memory, and avoids needless copying, which might be faster
        ## if no array ops are involved. Should check all the code that pulls stuff out of
        ## the spikes recarray, and choose the best one more carefully!
        
        trange = self.ui.xcorrsRangeSpinBox.value() * 1000 # convert to us
        trange = max(1000, trange) # enforce min trange, in us
        trange = np.array([-trange, trange]) # convert to a +/- array, in us
        
        t0 = time.time()
        dts = util.xcorr(xspikets, yspikets, trange=trange) # in us
        print('xcorr calc took %.3f sec' % (time.time()-t0))
        if autocorr:
            dts = dts[dts != 0] # remove 0s for autocorr
        #print(dts)

        dts = dts / 1000 # in ms, converts to float64 array
        trange = trange / 1000 # in ms, converts to float64 array
        nbins = intround(np.sqrt(len(dts))) # good heuristic
        nbins = max(20, nbins) # enforce min nbins
        nbins = min(100, nbins) # enforce max nbins
        t = np.linspace(start=trange[0], stop=trange[1], num=nbins, endpoint=True)
        n = np.histogram(dts, bins=t, density=False)[0]
        binwidth = t[1] - t[0] # all should be equal width

        # plot:
        mplw = self.OpenWindow('MPL')
        a = mplw.ax
        a.clear()
        # omit last right edge in t:
        a.bar(t[:-1], height=n, width=binwidth, color='k', edgecolor='k')
        a.set_xlim(t[0], t[-1])
        a.set_xlabel('ISI (ms)')
        a.set_ylabel('count')
        if autocorr:
            windowtitle = "n%d autocorr" % clusters[0].id
        else:
            windowtitle = "n%d xcorr with n%d" % (clusters[0].id, clusters[1].id)
        mplw.setWindowTitle(windowtitle)
        title = windowtitle + ', binwidth: %.2f ms' % binwidth
        print(title)
        a.set_title(title)
        #a.set_ylabel('ISI rate (Hz)')
        mplw.f.tight_layout(pad=0.3) # crop figure to contents
        mplw.figurecanvas.draw()

    @QtCore.pyqtSlot()
    def on_compareAllAutoCorrPairs_clicked(self, RP=750):
        """For each neuron pair, calculate autocorr of each neuron, find number of ISIs <=
        RP, sum this up for both neurons, then temporarily merge the spike trains of the two
        neurons, calculate autocorr of merger, and find its number of ISIs <= RP. Take
        difference between this and the previous sum, normalize by total number of intervals
        to get change in refractory period contamination drpc. Sort neuron pairs in
        decreasing order of drpc. Those with the biggest drpc have an autocorr that's most
        affected by the incorrect merger. This should help demonstrate that even in the
        worst case scenario, the autocorr is a poor diagnostic of contaminated spike
        trains."""
        sort = self.sort
        nids = sort.good
        nn = len(nids)
        npairs = comb(nn, 2, exact=True)
        print('RP = %d' % RP)
        print('npairs = %d' % npairs)
        # refractor period contamination array, one row per neuron pair:
        # each row: [nid0, nid1, rpc0, rpc1, drpc]
        rpc = np.zeros((npairs, 5), dtype=np.float64)
        # convert RP to half trange array, in us
        trange = np.array([0, RP])
        pairi = 0
        for nidi0 in range(nn):
            for nidi1 in range(nidi0+1, nn): # for all neuron pairs
                nid0 = nids[nidi0]
                nid1 = nids[nidi1]
                sids0 = sort.neurons[nid0].sids
                sids1 = sort.neurons[nid1].sids
                s0 = sort.spikes['t'][sids0]
                s1 = sort.spikes['t'][sids1]
                dts0 = util.xcorr(s0, s0, trange)
                dts0 = dts0[dts0 != 0] # remove 0s for autocorr
                dts1 = util.xcorr(s1, s1, trange)
                dts1 = dts1[dts1 != 0] # remove 0s for autocorr
                # now do it for the merged spike trains:
                s = np.concatenate([s0, s1])
                s.sort() # necessary for xcorr
                dts = util.xcorr(s, s, trange)
                dts = dts[dts != 0] # remove 0s for autocorr
                nISI0 = len(s0) - 1
                nISI1 = len(s1) - 1
                nISI = len(s) - 1
                rpc0 = len(dts0) / nISI0
                rpc1 = len(dts1) / nISI1
                drpc = (len(dts) - (len(dts0) + len(dts1))) / nISI
                rpc[pairi] = nid0, nid1, rpc0, rpc1, drpc
                pairi += 1
                print('.', end='')
        sortis = np.argsort(rpc[:, 4])
        sortis = sortis[::-1] # reverse
        rpc = rpc[sortis] # sorted by decreasing drpc values
        print(rpc)
        print('first <= 50 pairs:')
        print(rpc[:50])
        f = pl.figure()
        pl.get_current_fig_manager().set_window_title('delta RPC')
        pl.plot(rpc[:, 4], 'k-', linewidth=4)
        pl.xlabel('pair number')
        #pl.ylabel(r'$\Delta$RPC for ISI $\leq$ %.2f ms' % (RP/1000))
        pl.ylabel(r'$\Delta$RPC')
        import pdb; pdb.set_trace()

    @QtCore.pyqtSlot()
    def on_ISICleanButton_clicked(self):
        """Remove any duplicate spikes within each neuron, according to an ISI threshold"""
        minISI = self.ui.minISISpinBox.value()
        spikes = self.sort.spikes
        rmsidss = [] # list of lists
        print('removing duplicate spikes:')
        for nid in sorted(self.sort.neurons):
            if nid == 0:
                # skip any duplicate spikes that are already unclustered:
                continue
            # for each pair of duplicate spikes, keep whichever has the most channel overlap
            # with neuron template. If they have same amount of overlap, keep the first one
            neuron = self.sort.neurons[nid]
            rmsids = [] # reset
            # pick out the first sid of each pair of duplicate sids, if any:
            sidis = np.where(np.diff(spikes['t'][neuron.sids]) <= minISI)[0]
            if len(sidis) == 0:
                continue # skip to next nid
            sids = neuron.sids[sidis]
            #x0, y0 = neuron.cluster.pos['x0'], neuron.cluster.pos['y0']
            for sid in sids:
                if spikes['nid'][sid+1] != nid:
                    # second spike in this "duplicate" pair belongs to another neuron,
                    # or maybe even junk neuron 0, skip to next pair
                    continue
                nchans0 = spikes['nchans'][sid]
                chans0 = spikes['chans'][sid][:nchans0]
                nchans1 = spikes['nchans'][sid+1]
                chans1 = spikes['chans'][sid+1][:nchans0]
                ncommon0 = len(np.intersect1d(neuron.chans, chans0))
                ncommon1 = len(np.intersect1d(neuron.chans, chans1))
                if ncommon0 >= ncommon1:
                    #keepsid = sid
                    rmsid = sid + 1
                else:
                    #keepsid = sid + 1
                    rmsid = sid
                #print('  removing sid %d:' % rmsid, ncommon0, ncommon1)
                
                """
                # code for choosing the one closest to template mean position, not as robust:
                d02 = (spikes['x0'][sid] - x0)**2 + (spikes['y0'][sid] - y0)**2
                d12 = (spikes['x0'][sid+1] - x0)**2 + (spikes['y0'][sid+1] - y0)**2
                if d02 <= d12:
                    keep = sid
                else:
                    keep = sid + 1
                print('kept sid %d:' % keep, d02, d12)
                """
                rmsids.append(rmsid)
            print('neuron %d: %r' % (nid, rmsids))
            #print()
            rmsidss.append(rmsids) # list of lists
        # do the actual removal
        rmsids = np.concatenate(rmsidss) # flat array
        nrm = len(rmsids)
        if nrm > 0:
            self.SplitSpikes(sids=rmsids, delete=True, autogen=False)
        #self.apply_clustering([], rmsids, np.zeros(nrm, dtype=np.int16), verb='clean')
        # recalculate template means
        print('removed %d duplicate spikes' % nrm)

    def GetSortedSpikes(self):
        """Return IDs of selected sorted spikes"""
        sw = self.windows['Sort']
        srows = sw.nslist.selectedRows()
        return sw.nslist.sids[srows]

    def GetUnsortedSpikes(self):
        """Return IDs of selected unsorted spikes"""
        sw = self.windows['Sort']
        srows = sw.uslist.selectedRows()
        return self.sort.usids[srows]

    def GetClusterSpikes(self):
        """Return sorted IDs of all spikes of selected clusters"""
        clusters = self.GetClusters()
        if len(clusters) == 0:
            return np.array([], dtype=np.int64)
        sids = []
        for cluster in clusters:
            sids.append(cluster.neuron.sids)
        sids = np.concatenate(sids)
        sids.sort()
        return sids

    def GetSpikes(self):
        """Return IDs of explicitly selected spikes"""
        sw = self.windows['Sort']
        return np.concatenate([ self.GetSortedSpikes(), self.GetUnsortedSpikes() ])

    def GetSpike(self):
        """Return ID of just one selected spike, from nslist or uslist"""
        sids = self.GetSpikes()
        nselected = len(sids)
        if nselected != 1:
            raise RuntimeError("can't figure out which of the %d selected spike IDs you want"
                               % nselected)
        return sids[0]

    def GetAllSpikes(self):
        """Return sorted IDs of all selected spikes, whether explicitly or implicitly
        selected"""
        sids = []
        ssids = self.GetSortedSpikes()
        sids.append(ssids)
        # if no sorted spikes explicitly selected, check if any clusters are:
        if len(ssids) == 0:
            sids.append(self.GetClusterSpikes())
        # include any selected usids as well
        sids.append(self.GetUnsortedSpikes())
        sids = np.concatenate(sids)
        sids.sort()
        return sids

    def GetClusterIDs(self):
        """Return list of IDs of currently selected clusters, in norder"""
        sw = self.windows['Sort']
        cids = [ i.data().toInt()[0] for i in sw.nlist.selectedIndexes() ]
        #cids.sort() # don't do regular sort, sort by norder
        ciis = np.argsort([ self.sort.norder.index(cid) for cid in cids ])
        return [ cids[cii] for cii in ciis ] # in norder

    def GetClusters(self):
        """Return list of currently selected clusters, in norder"""
        cids = self.GetClusterIDs() # already in norder
        return [ self.sort.clusters[cid] for cid in cids ]

    def GetCluster(self):
        """Return just one selected cluster"""
        clusters = self.GetClusters()
        nselected = len(clusters)
        if nselected != 1:
            raise RuntimeError("can't figure out which of the %d selected clusters you want"
                               % nselected)
        return clusters[0]

    def SelectClusters(self, clusters, on=True):
        """Select/deselect clusters"""
        clusters = toiter(clusters)
        try:
            selnids = [ cluster.id for cluster in clusters ]
        except AttributeError: # assume they're ints
            selnids = [ cluster for cluster in clusters ]
        rows = [ self.sort.norder.index(selnid) for selnid in selnids ]
        nlist = self.windows['Sort'].nlist
        nlist.selectRows(rows, on)
        #print('set rows %r to %r' % (rows, on))

    def ToggleCluster(self, cluster):
        """Toggle selection of given cluster"""
        sw = self.windows['Sort']
        try:
            nid = cluster.id
        except AttributeError: # assume it's an int
            nid = cluster
        row = self.sort.norder.index(nid)
        on = not sw.nlist.rowSelected(row)
        sw.nlist.selectRows(row, on=on)
        return on

    def SelectSpikes(self, sids, on=True):
        """Set selection state of given spikes, as well as their current clusters, if any"""
        sw = self.windows['Sort']
        nids = self.sort.spikes['nid'][sids]

        # select/deselect any unclustered spikes:
        usids = sids[nids == 0]
        if len(usids) > 0:
            usrows = self.sort.usids.searchsorted(usids)
            sw.uslist.selectRows(usrows, on=on)

        # select/deselect any clustered spikes, as well as their clusters:
        csids = sids[nids != 0] # clustered spike ids
        unids = np.unique(nids)
        unids = unids[unids != 0] # remove cluster 0
        # get currently selected sids in nslist, and the unids they belong to:
        selsids = sw.nslist.sids[sw.nslist.selectedRows()] # hopefully don't need a copy
        selunids = sw.nslist.nids
        if on == True: # find clustered spikes to add to selection:
            # add csids to selsids (get values in csids that aren't in selsids):
            csids = np.setdiff1d(csids, selsids, assume_unique=True) # to add
            allcsids = np.union1d(csids, selsids) # final
        elif on == False: # find clustered spikes to remove from selection:
            # remove csids from selsids:
            csids = np.intersect1d(csids, selsids, assume_unique=True) # to remove
            allcsids = np.setdiff1d(csids, selsids, assume_unique=True) # final
        else:
            raise ValueError("invalid 'on' value: %r" % on)
        if len(csids) == 0:
            return # no clustered spikes to add or remove
        newunids = np.unique(self.sort.spikes['nid'][allcsids]) # excludes cluster 0
        # select any new clusters so nslist has correct contents, this
        # changes contents of nslist and hence clears any currently selected sids:
        addunids = np.setdiff1d(newunids, selunids)
        if len(addunids) > 0:
            # all nids will be in sort.norder list, find their positions
            addnlistrows = [ self.sort.norder.index(unid) for unid in addunids ]
            sw.nlist.selectRows(addnlistrows, on=True)
        # now do the clustered spike selection:
        nslistrows = sw.nslist.sids.searchsorted(csids) # nslist.sids is sorted
        #t0 = time.time()
        sw.nslist.selectRows(nslistrows, on=on)
        #print('nslist.selectRows took %.3f sec' % (time.time()-t0))

    def CreateCluster(self, update=True, id=None, inserti=None):
        """Create a new cluster, add it to the GUI, return it"""
        s = self.sort
        neuron = s.create_neuron(id, inserti=inserti)
        sw = self.windows['Sort']
        if update:
            sw.nlist.updateAll()
        from cluster import Cluster # can't delay this any longer
        cluster = Cluster(neuron)
        s.clusters[cluster.id] = cluster
        neuron.cluster = cluster
        try:
            cw = self.windows['Cluster'] # don't force its display by default
        except KeyError:
            cw = self.OpenWindow('Cluster')
        return cluster

    def DelClusters(self, clusters, update=True):
        """Delete clusters from the GUI, and delete clusters
        and their neurons from the Sort."""
        clusters = toiter(clusters)
        self.SelectClusters(clusters, on=False) # first deselect them all
        sw = self.windows['Sort']
        cw = self.windows['Cluster']
        self.ColourPoints(clusters, setnid=0) # decolour before clusters lose their sids
        for cluster in clusters:
            sw.RemoveNeuron(cluster.neuron, update=update)
        cw.glWidget.updateGL()
        if update:
            self.UpdateClustersGUI()

    def UpdateClustersGUI(self):
        """Update lots of stuff after modifying clusters,
        here as a separate method for speed, only call when really needed"""
        s = self.sort
        sw = self.windows['Sort']
        sw.nlist.updateAll()
        s.update_usids()
        sw.uslist.updateAll()

    def ColourPoints(self, clusters, setnid=None):
        """Colour the points that fall within each cluster (as specified
        by cluster.neuron.sids) the same colour as the cluster itself. Or, if
        setnid != None, colour all points in clusters according to setnid value"""
        clusters = toiter(clusters)
        gw = self.windows['Cluster'].glWidget
        for cluster in clusters:
            neuron = cluster.neuron
            # not all (or any) of neuron.sids may currently be plotted
            commonsids = np.intersect1d(neuron.sids, gw.sids)
            if len(commonsids) > 0:
                sidis = gw.sids.searchsorted(commonsids)
                # set new nids for commonsids in glWidget:
                if setnid == None:
                    gw.nids[sidis] = neuron.id
                else:
                    gw.nids[sidis] = setnid
                gw.colour(commonsids) # recolour commonsids according to their nids
        gw.updateGL()

    def SetClusteringDims(self, dims):
        dimlist = self.ui.dimlist
        alldims = [ str(dimlist.item(rowi).text()) for rowi in range(dimlist.count()) ]
        rowis = [ alldims.index(dim) for dim in dims ]
        for rowi in rowis:
            # there really should be an easier way, but .setSelection(QRect, ...) doesn't work?
            #dimlist.setCurrentRow(rowi, QtGui.QItemSelectionModel.Select)
            dimlist.item(rowi).setSelected(True) # a little nicer

    def GetClusteringDims(self):
        """Get selected clustering dimensions in dimlist. If dimlist disabled, use plot
        dimensions instead"""
        if not self.ui.dimlist.isEnabled():
            return self.GetClusterPlotDims()
        items = self.ui.dimlist.selectedItems()
        if len(items) == 0:
            raise RuntimeError('No clustering dimensions selected')
        dims = [ str(item.text()) for item in items ] # dim names
        return dims

    def GetClusterPlotDims(self):
        """Return 3-tuple of strings of cluster dimension names, in (x, y, z) order"""
        x = str(self.ui.xDimComboBox.currentText())
        y = str(self.ui.yDimComboBox.currentText())
        z = str(self.ui.zDimComboBox.currentText())
        return x, y, z

    def AddClusterChangeToStack(self, cc):
        """Adds cc to the cluster change stack, removing any potential redo changes"""
        self.cci += 1
        del self.cchanges[self.cci::] # remove any existing redo cluster changes
        self.cchanges.append(cc) # add to stack
        # TODO: check if stack has gotten too long, if so, remove some from the start
        # and update self.cci appropriately

    def ApplyClusterChange(self, cc, direction):
        """Apply cluster change described in cc, in either the forward or backward
        direction, to the current set of clusters"""
        s = self.sort
        spikes = s.spikes
        sw = self.windows['Sort']
        cw = self.windows['Cluster']
        sids = cc.sids

        # reverse meaning of 'new' and 'old' if direction == 'forward', ie if redoing
        if direction == 'back':
            #newnids = cc.newnids # not needed
            oldnids = cc.oldnids
            newunids = cc.newunids
            oldunids = cc.oldunids
            poss = cc.oldposs
            normposs = cc.oldnormposs
            norder = cc.oldnorder
            good = cc.oldgood
        else: # direction == 'forward'
            #newnids = cc.oldnids # not needed
            oldnids = cc.newnids
            newunids = cc.oldunids
            oldunids = cc.newunids
            poss = cc.newposs
            normposs = cc.newnormposs
            norder = cc.newnorder
            good = cc.newgood

        # delete newly added clusters
        newclusters = [ s.clusters[nid] for nid in newunids ]
        self.SelectClusters(newclusters, on=False) # deselect new clusters
        # temporarily deselect any bystander clusters to get around fact that
        # selections are row-based in Qt, not value-based, which means selection
        # changes happen without a selectionChanged event when the rowCount changes
        bystanders = self.GetClusters()
        self.SelectClusters(bystanders, on=False)
        self.DelClusters(newclusters, update=False) # del new clusters

        # restore relevant spike fields
        spikes['nid'][sids] = oldnids

        # restore the old clusters
        oldclusters = []
        dims = self.GetClusterPlotDims()
        t0 = time.time()
        # NOTE: oldunids are not necessarily sorted
        for nid, pos, normpos in zip(oldunids, poss, normposs):
            nsids = sids[oldnids == nid] # sids belonging to this nid
            cluster = self.CreateCluster(update=False, id=nid)
            oldclusters.append(cluster)
            neuron = cluster.neuron
            sw.MoveSpikes2Neuron(nsids, neuron, update=False)
            cluster.pos = pos
            cluster.normpos = normpos
        # restore norder and good
        s.norder = copy(norder)
        s.good = copy(good)

        # now do some final updates
        self.UpdateClustersGUI()
        self.ColourPoints(oldclusters)
        #print('applying clusters to plot took %.3f sec' % (time.time()-t0))
        # select newly recreated oldclusters
        self.SelectClusters(oldclusters)
        # restore bystander selections
        self.SelectClusters(bystanders)
        #print('oldclusters: %r' % [c.id for c in oldclusters])
        #print('newclusters: %r' % [c.id for c in newclusters])
        #print('bystanders: %r' % [c.id for c in bystanders])

    def SplitSpikes(self, sids=None, delete=False, autogen=True):
        """Split off explicitly selected spikes from their clusters (if any). More accurately,
        split selected cluster(s) into new cluster(s) plus a destination cluster, whose ID
        depends on the delete arg. This process is required to allow undo/redo.
        If sids arg isn't None, use them instead of getting them from selected clusters"""
        s = self.sort
        spikes = s.spikes
        if sids == None: # splitting via GUI
            sids = self.GetSpikes() # explicitly selected spikes (sorted and unsorted)
            # all spikes of all affected clusters:
            supersids = np.unique(np.concatenate([sids, self.GetClusterSpikes()]))
            oldclusters = self.GetClusters()
        else: # splitting programmatically, ignore any spike selection
            unids = np.unique(spikes['nid'][sids]) # associated set of neuron ids
            # sort by norder:
            uniis = np.argsort([ s.norder.index(unid) for unid in unids ])
            unids = unids[uniis] # in norder
            oldclusters = [ s.clusters[unid] for unid in unids ]
            # all spikes of all affected clusters:
            supersids = np.concatenate([ oc.neuron.sids for oc in oldclusters ])
            supersids.sort()
        sids.sort()
        if len(sids) == 0:
            return # do nothing
        if delete:
            newnid = 0 # junk cluster
        else:
            newnid = s.nextnid
        supersidis = supersids.searchsorted(sids) # where sids fit into supersids
        nids = spikes[supersids]['nid'] # seems to return a copy
        nids[supersidis] = newnid # doesn't seem to overwrite nid values in spikes recarray
        self.apply_clustering(oldclusters, supersids, nids, autogen=autogen, verb='split')

    def updateTitle(self):
        """Update main spyke window title based on open stream and sort, if any"""
        if hasattr(self.hpstream, 'fname'):
            title = self.hpstream.fname
            if hasattr(self, 'sort') and self.sort.fname:
                title += ', ' + self.sort.fname
        elif hasattr(self, 'sort') and self.sort.fname:
            title = self.sort.fname
        else:
            title = 'spyke'
        self.setWindowTitle(title) # update the title

    def OpenRecentFile(self):
        """Open a filename from the clicked recent file in the File menu"""
        action = self.sender()
        if action:
            fullfname = str(action.data().toString())
            self.OpenFile(fullfname)

    def updateRecentFiles(self, fullfname=None):
        """Update list of recent files in File menu, optionally specifying the
        last fname opened or closed, which should hence go to the top of the list.
        Some of this code is taken from PySide's examples/mainwindows/recentfiles.py"""
        settings = QtCore.QSettings('spyke', 'spyke') # retrieve setting
        fullfnames = settings.value('recentFileList').toList()
        for i in range(len(fullfnames)): # convert each entry from QVariant to QString
            fullfnames[i] = fullfnames[i].toString()
        if fullfname:
            try:
                fullfnames.remove(fullfname)
            except ValueError:
                pass
            fullfnames.insert(0, fullfname)
        del fullfnames[MAXRECENTFILES:]
        settings.setValue('recentFileList', fullfnames) # update setting

        # update menu to match fullfnames:
        nrecent = len(fullfnames)
        for i, fullfname in enumerate(fullfnames):
            text = "&%d %s" % (i, fullfname) # add keyboard accelerator
            self.recentFileActions[i].setText(text)
            self.recentFileActions[i].setData(fullfname)
            self.recentFileActions[i].setVisible(True)

        for j in range(nrecent, MAXRECENTFILES):
            self.recentFileActions[j].setVisible(False)

    def OpenFile(self, fname):
        """Open a stream or sort file. fname in this case must contain a full path"""
        head, tail = os.path.split(fname)
        assert head # make sure fname has a path to it
        ext = os.path.splitext(tail)[1]
        if ext in ['.srf', '.track', '.tsf', '.mat']:
            self.streampath = head
            self.OpenStreamFile(tail)
        elif ext == '.sort':
            self.sortpath = head
            self.OpenSortFile(tail)
        else:
            critical = QtGui.QMessageBox.critical
            critical(self, "Error", "%s is not a .srf, .track, .tsf, or .sort file" % fname)

    def OpenStreamFile(self, fname):
        """Open a stream (.srf, .track, or .tsf file) and update display accordingly.
        fname is assumed to be relative to self.streampath"""
        if self.hpstream != None:
            self.CloseStream() # in case a stream is already open
        ext = os.path.splitext(fname)[1]
        if ext == '.srf':
            srff = surf.File(fname, self.streampath)
            srff.parse() # TODO: parsing progress dialog
            self.hpstream = srff.hpstream # highpass record (spike) stream
            self.lpstream = srff.lpstream # lowpassmultichan record (LFP) stream
        elif ext == '.track':
            srffs = []
            with open(join(self.streampath, fname), 'r') as trackfile:
                for line in trackfile: # one srf filename per line
                    if line.startswith('#'): # it's a comment line
                        continue # skip it
                    srffname = line.rstrip('\n')
                    srff = surf.File(srffname, self.streampath)
                    srff.parse()
                    srffs.append(srff) # build up list of open and parsed surf File objects
            self.hpstream = core.TrackStream(srffs, fname, kind='highpass')
            self.lpstream = core.TrackStream(srffs, fname, kind='lowpass')
        elif ext == '.tsf':
            self.hpstream = self.OpenTSFFile(fname)
        elif ext == '.mat':
            self.hpstream = self.OpenQuirogaMATFile(fname)
        else:
            raise ValueError('unknown extension %r' % ext)

        try:
            self.sort.stream = self.hpstream # restore newly opened stream to sort
        except AttributeError: # no sort yet
            pass

        self.updateTitle()
        self.updateRecentFiles(join(self.streampath, fname))

        self.ui.__dict__['action%dkHz' % (self.hpstream.sampfreq / 1000)].setChecked(True)
        self.ui.actionSampleAndHoldCorrect.setChecked(self.hpstream.shcorrect)

        self.set_chans_enabled(self.hpstream.chans, enable=True)
        tww = self.spiketw[1]-self.spiketw[0] # window width
        self.t = intround(self.hpstream.t0 + tww/2) # set current timepoint (us)

        self.SPIKEWINDOWWIDTH = self.hpstream.probe.ncols * SPIKEWINDOWWIDTHPERCOLUMN
        self.OpenWindow('Spike')

        self.str2t = {'start': self.hpstream.t0,
                      'now': self.t, # FIXME: this won't track self.t automatically
                      'end': self.hpstream.t1}
        self.range = (self.hpstream.t0, self.hpstream.t1) # us
        self.ui.filePosLineEdit.setText(str(self.t))
        self.ui.filePosStartButton.setText(str(self.hpstream.t0))
        self.ui.filePosEndButton.setText(str(self.hpstream.t1))
        # set all slider values in multiples of SLIDERTRES
        self.ui.slider.setRange(self.range[0] // SLIDERTRES,
                                self.range[1] // SLIDERTRES) # no need to round
        self.ui.slider.setValue(self.t // SLIDERTRES)
        self.ui.slider.setSingleStep(1)
        self.ui.slider.setPageStep((self.spiketw[1]-self.spiketw[0]) // SLIDERTRES)
        self.ui.slider.setInvertedControls(True)

        self.EnableStreamWidgets(True)

    def OpenQuirogaMATFile(self, fname):
        """Open Quiroga's .mat files containing single channel synthetic spike data.
        Return a SimpleStream"""
        import scipy.io
        fname = join(self.streampath, fname)
        d = scipy.io.loadmat(fname, squeeze_me=True)
        #chan = d['chan'] # this field isn't always present
        #assert chan == 1
        nchans = 1
        wavedata = d['data'] # float64, mV
        wavedata = wavedata * 1000 # uV
        assert wavedata.ndim == 1
        nt = len(wavedata)
        wavedata.shape = nchans, -1 # enforce 2D
        # convert to int16, assume ADC resolution for this data was <= 16 bits,
        # use some reasonable gain values, check they don't saturate 16 bits:
        intgain = 1
        extgain = 2000
        converter = core.Converter(intgain=intgain, extgain=extgain)
        wavedata = converter.uV2AD(wavedata, inttype=np.int64)
        # check for saturation:
        wdmin, wdmax = wavedata.min(), wavedata.max()
        print('gain = %d' % (intgain*extgain))
        print('wavedata.min() = %d, wavedata.max() = %d' % (wdmin, wdmax))
        if wdmin <= -2**15 or wdmax >= 2**15-1:
            raise RuntimeError("wavedata has saturated int16. Try reducing gain")
        wavedata = np.int16(wavedata) # downcast to int16
        siteloc = np.empty((nchans, 2))
        siteloc[0] = 0, 0
        rawtres = float(d['samplingInterval']) # ms
        rawtres = rawtres / 1000 # sec
        rawsampfreq = intround(1 / rawtres) # Hz
        masterclockfreq = None
        stream = SimpleStream(fname, wavedata, siteloc, rawsampfreq, masterclockfreq,
                              intgain, extgain, bitshift=None)
        truth = core.EmptyClass()
        truth.spiketis = d['spike_times']
        assert truth.spiketis[-1] < nt
        truth.spikets = truth.spiketis * rawtres
        # unsure what the other arrays in this field are for:
        truth.sids = d['spike_class'][0]
        assert int(d['startData']) == 0
        stream.truth = truth
        return stream

    def OpenTSFFile(self, fname):
        """Open NVS's "test spike file" .tsf format for testing spike sorting
        performance. This describes a single 2D contiguous array of raw waveform
        data, within which are embedded a number of spikes from a number of neurons.
        The ground truth is typically listed at the end of the file. Return a SimpleStream.

        fname is assumed to be relative to self.streampath.

        .tsf file TODO:

            - make data column-major for better seeking in time
            - move nchans field before siteloc field
            - make maxchans 0 based, ie same as labelled on probe design by UMich
            - would be better to keep spikes sorted in time, instead of by cluster id
            - no need for 64 extgain values, they're all the same, whether you're exporting
              spike or LFP data. And if for some reason they could've been different, length
              of extgains vector should be nchans, not fixed 64. Also, if extgains is a
              vector, then so should intgains
            - number cluster ids in vertically spatial order, by mean of their template's
              vertical spatial position, not just by their maxchan - subtle difference
            - are .tsf spike times all aligned to +ve 0 crossing? One difference from .sort
              is that they're all truncated to the nearest 25kHz sample point. Maybe it
              would be best to save the spike time in us instead of in 25kHz sample point
              indices
            - add some kind of datetime stamp, ala .srf. Maybe datetime the .tsf file was
              generated
            - increment format number. Maybe we should ultimately make a .nvs file
              type, similar to .tsf format, for sharing with others, as a simplified
              .srf file. Would require adding an LFP channel field to the end, or just make
              the LFP chans look like normal spike chans, way oversampled
            - add more cells, make some fraction of them bursting, give bursting cells
              some prob distrib over number of spikes per burst, make each spike in a
              burst say 5 or 10% smaller than the previous spike adaptation
            - maybe even simulate spatial drift? That would be more difficult
            - need far more spikes. Enforce a power law distribution in number spikes
              per cell
            - main thing is to look at how close in space and time spikes can be seeded
              and still be detected and clustered correctly
    
        """
        try: f = open(join(self.streampath, fname), 'rb')
        except IOError:
            print("can't find file %r" % fname)
            return
        header = f.read(16)
        assert header == 'Test spike file '
        format, = unpack('i', f.read(4))
        assert format == 1000
        nchans = 54 # assumed
        siteloc = np.fromfile(f, dtype=np.int16, count=nchans*2)
        siteloc.shape = nchans, 2
        rawsampfreq, = unpack('i', f.read(4)) # 25k
        masterclockfreq, = unpack('i', f.read(4)) # 1M
        extgains = np.fromfile(f, dtype=np.uint16, count=64)
        extgain = extgains[0]
        intgain, = unpack('H', f.read(2))
        # this nchans field should've been above siteloc field:
        nchans2, = unpack('i', f.read(4))
        assert nchans == nchans2 # make sure above assumption was right
        nt, = unpack('i', f.read(4)) # 7.5M, eq'v to 300 sec data total
        # read row major data, ie, chan loop is outer loop
        wavedata = np.fromfile(f, dtype=np.int16, count=nchans*nt)
        wavedata.shape = nchans, nt
        stream = SimpleStream(fname, wavedata, siteloc, rawsampfreq, masterclockfreq,
                              intgain, extgain)
        # not all .tsf files have ground truth data at end:
        pos = f.tell()
        groundtruth = f.read()
        if groundtruth == '': # reached EOF
            nbytes = f.tell()
            print('read %d bytes, %s is %d bytes long' % (pos, fname, nbytes))
            f.close()
            return stream
        else:
            f.seek(pos) # go back and parse ground truth data
        truth = core.EmptyClass()
        # something to do with how spikes were seeded vertically in space:
        truth.vspacing, = unpack('i', f.read(4))
        truth.nspikes, = unpack('i', f.read(4))
        # sample index of each spike:
        spiketis = np.fromfile(f, dtype=np.uint32, count=truth.nspikes)
        sids = spiketis.argsort() # indices that sort spikes in time
        truth.spikets = spiketis[sids] * stream.rawtres # in us
        truth.nids = np.fromfile(f, dtype=np.uint32, count=truth.nspikes)[sids]
        truth.chans = np.fromfile(f, dtype=np.uint32, count=truth.nspikes)[sids]
        assert truth.chans.min() >= 1 # NVS stores these as 1-based
        truth.chans -= 1 # convert to proper 0-based maxchan ids
        self.renumber_tsf_truth(truth, stream)
        stream.truth = truth
        pos = f.tell()
        f.seek(0, 2)
        nbytes = f.tell()
        print('read %d bytes, %s is %d bytes long' % (pos, fname, nbytes))
        return stream

    def renumber_tsf_truth(self, truth, stream):
        """Renumber .tsf ground truth nids according to vertical spatial order of their
        max chan, similar to what's done in .sort. Differences in labelling can still
        arise because in a .sort, nids are ordered by the mean vertically modelled
        position of each neuron's member spikes, not strictly by the maxchan of its
        mean template"""
        oldnid2sids = {}
        nids = truth.nids
        oldunids = np.unique(nids)
        nnids = len(oldunids)
        oldchans = np.zeros(nnids, dtype=truth.chans.dtype)
        assert (oldunids == np.arange(1, nnids+1)).all()
        # find maxchan of each nid, store in oldchans:
        for chani, oldnid in enumerate(oldunids):
            sids = nids == oldnid
            oldnid2sids[oldnid] = sids # save these for next loop
            chans = truth.chans[sids]
            chan = chans[0]
            assert (chans == chan).all() # check for surprises
            oldchans[chani] = chan
        # convert maxchans to y positions:
        ypos = np.asarray([ stream.probe.SiteLoc[chan][1] for chan in oldchans ])
        # as in sort.on_actionRenumberClusters_triggered(), this is a bit confusing:
        # find indices that would sort old ids by y pos, but then what you really want
        # is to find the y pos *rank* of each old id, so you need to take argsort again:
        sortiis = ypos.argsort().argsort()
        newunids = oldunids[sortiis] # sorted by vertical position
        for oldnid, newnid in zip(oldunids, newunids):
            sids = oldnid2sids[oldnid]
            nids[sids] = newnid # overwrite old nid values with new ones

    def OpenSortFile(self, fname):
        """Open a Sort from a .sort and .spike file, try and open a .wave file
        with the same name, restore the (closed) stream"""
        self.DeleteSort() # delete any existing Sort
        print('opening sort file %r' % fname)
        t0 = time.time()
        f = open(join(self.sortpath, fname), 'rb')
        sort = cPickle.load(f)
        print('done opening sort file, took %.3f sec' % (time.time()-t0))
        print('sort file was %d bytes long' % f.tell())
        f.close()
        self.sort = sort

        sortProbeType = type(sort.probe)
        if self.hpstream != None:
            sort.stream = self.hpstream # restore open stream to sort
            streamProbeType = type(self.hpstream.probe)
            if sortProbeType != streamProbeType:
                self.CreateNewSort() # overwrite the failed Sort
                raise RuntimeError(".sort file's probe type %r doesn't match .srf file's "
                                   "probe type %r" % (sortProbeType, streamProbeType))

        self.OpenSpikeFile(sort.spikefname)

        # try auto-updating sort to latest version:
        if float(sort.__version__) < float(__version__):
            self.update_sort_version()
        
        # restore Sort's tw to self and to spike and sort windows, if applicable:
        #print('sort.tw is %r' % (sort.tw,))
        self.update_spiketw(sort.tw)
        # restore sampling variables:
        self.SetSampfreq(sort.sampfreq)
        self.SetSHCorrect(sort.shcorrect)
        self.ui.progressBar.setFormat("%d spikes" % sort.nspikes)

        self.SPIKEWINDOWWIDTH = sort.probe.ncols * SPIKEWINDOWWIDTHPERCOLUMN
        sw = self.OpenWindow('Sort') # ensure it's open
        sw.uslist.updateAll() # restore unsorted spike listview
        self.restore_clustering_selections()
        self.RestoreClusters2GUI()
        self.updateTitle()
        self.updateRecentFiles(join(self.sortpath, fname))
        self.update_gui_from_sort()
        self.EnableSortWidgets(True)

    def restore_clustering_selections(self):
        """Restore state of last user-selected clustering parameters, specifically those
        that are otherwise not bound to the sort outside of saving it to file. Performs
        reverse of save_clustering_selections()"""
        s = self.sort
        sw = self.OpenWindow('Sort')
        cw = self.OpenWindow('Cluster')
        # try and restore saved component analysis selection
        try:
            i = self.ui.componentAnalysisComboBox.findText(s.selCA)
            self.ui.componentAnalysisComboBox.setCurrentIndex(i)
        except AttributeError: pass # wasn't saved, loading from old .sort file
        # try and restore saved cluster selection
        try: self.SelectClusters(s.selnids)
        except AttributeError: pass # wasn't saved, loading from old .sort file
        # try and restore saved sort window channel selection, and manual selection flag
        try:
            sw.panel.chans_selected = s.selchans
            sw.panel.manual_selection = s.selchansmanual
            # don't save x, y, z dimension selection, leave it at default xyVpp
            # for maximum speed when loading sort file
        except AttributeError: pass # wasn't saved, loading from old .sort file
        # try and restore saved inclt selection
        try:
            i = sw.incltComboBox.findText(s.selinclt)
            sw.incltComboBox.setCurrentIndex(i)
        except AttributeError: pass # wasn't saved, loading from old .sort file

        sw.panel.update_selvrefs()
        sw.panel.draw_refs() # update

        self.on_plotButton_clicked() # create glyph on first open
        # try and restore saved camera view
        try: cw.glWidget.MV, cw.glWidget.focus = s.MV, s.focus
        except AttributeError: pass

    def OpenSpikeFile(self, fname):
        sort = self.sort
        print('loading spike file %r' % fname)
        t0 = time.time()
        f = open(join(self.sortpath, fname), 'rb')
        spikes = np.load(f)
        print('done opening spike file, took %.3f sec' % (time.time()-t0))
        print('spike file was %d bytes long' % f.tell())
        f.close()
        sort.spikes = spikes
        # when loading a spike file, make sure the nid field is overwritten
        # in the spikes array. The nids in sort.neurons are always the definitive ones
        for neuron in sort.neurons.values():
            spikes['nid'][neuron.sids] = neuron.id
        sort.update_usids()
        # try loading .wave file of the same name
        wavefname = os.path.splitext(fname)[0] + '.wave'
        sort.wavedata = self.OpenWaveFile(wavefname)

    def OpenWaveFile(self, fname):
        """Open a .wave file and return wavedata array"""
        sort = self.sort
        print('opening wave file %r' % fname)
        t0 = time.time()
        try: f = open(join(self.sortpath, fname), 'rb')
        except IOError:
            print("can't find file %r" % fname)
            return
        try:
            del sort.wavedata
            #gc.collect() # ensure memory is freed up to prepare for new wavedata, necessary?
        except AttributeError: pass
        wavedata = np.load(f)
        print('done opening wave file, took %.3f sec' % (time.time()-t0))
        print('wave file was %d bytes long' % f.tell())
        f.close()
        if len(wavedata) != sort.nspikes:
            critical = QtGui.QMessageBox.critical
            critical(self, "Error",
                     ".wave file has a different number of spikes from the current Sort")
            raise RuntimeError
        return wavedata

    def CreateNewSort(self):
        """Create a new Sort, bind it to self, and return it"""
        self.DeleteSort()
        self.sort = Sort(detector=None, # detector is assigned in on_detectButton_clicked
                         stream=self.hpstream,
                         tw=self.spiketw)
        self.EnableSortWidgets(True)
        return self.sort

    def SaveSortFile(self, fname):
        """Save sort to a .sort file. fname is assumed to be relative to self.sortpath"""
        s = self.sort
        try: s.spikes
        except AttributeError: raise RuntimeError("Sort has no spikes to save")
        if not os.path.splitext(fname)[1]: # if it doesn't have an extension
            fname = fname + '.sort'
        try: s.spikefname
        except AttributeError: # corresponding .spike filename hasn't been gemerated yet
            s.spikefname = os.path.splitext(fname)[0] + '.spike'
        self.SaveSpikeFile(s.spikefname) # always (re)save .spike when saving .sort
        print('saving sort file %r' % fname)
        t0 = time.time()
        self.save_clustering_selections()
        self.save_window_states()
        s.fname = fname # bind it now that it's about to be saved
        f = open(join(self.sortpath, fname), 'wb')
        cPickle.dump(s, f, protocol=-1) # pickle with most efficient protocol
        f.close()
        print('done saving sort file, took %.3f sec' % (time.time()-t0))
        self.updateTitle()
        self.updateRecentFiles(join(self.sortpath, fname))

    def save_clustering_selections(self):
        """Save state of last user-selected clustering parameters. Unlike parameters such as
        sort.sigma, these parameters aren't bound to the sort during normal operation
        yet they're useful to restore when .sort file is reopened"""
        s = self.sort
        sw = self.windows['Sort'] # should be open if s.spikes exists
        s.selCA = str(self.ui.componentAnalysisComboBox.currentText())
        s.selnids = self.GetClusterIDs() # save current cluster selection
        s.selchans = sw.panel.chans_selected
        s.selchansmanual = sw.panel.manual_selection
        s.selinclt = str(sw.incltComboBox.currentText())
        try:
            cw = self.windows['Cluster']
            s.MV, s.focus = cw.glWidget.MV, cw.glWidget.focus # save camera view
        except KeyError:
            # cw hasn't been opened yet, no camera view to save
            pass

    def save_window_states(self):
        """Save window geometries and states (toolbar positions, etc.) to .sort file"""
        s = self.sort
        s.windowGeometries = {}
        s.windowStates = {}
        for wintype, window in self.windows.items():
            #print('saving state of %s window' % wintype)
            s.windowGeometries[wintype] = window.saveGeometry()
            s.windowStates[wintype] = window.saveState()

    def SaveSpikeFile(self, fname):
        """Save spikes to a .spike file. fname is assumed to be relative to self.sortpath"""
        s = self.sort
        try: s.spikes
        except AttributeError: raise RuntimeError("Sort has no spikes to save")
        if not os.path.splitext(fname)[1]: # if it doesn't have an extension
            fname = fname + '.spike'
        try: s.wavefname
        except AttributeError: # corresponding .wave file hasn't been created yet
            wavefname = os.path.splitext(fname)[0] + '.wave'
            self.SaveWaveFile(wavefname) # only write whole .wave file if missing s.wavefname attrib
            self.dirtysids.clear() # shouldn't be any, but clear anyway just in case
        if len(self.dirtysids) > 0:
            self.SaveWaveFile(s.wavefname, sids=self.dirtysids)
            self.dirtysids.clear() # no longer dirty
        print('saving spike file %r' % fname)
        t0 = time.time()
        f = open(join(self.sortpath, fname), 'wb')
        np.save(f, s.spikes)
        f.close()
        print('done saving spike file, took %.3f sec' % (time.time()-t0))
        s.spikefname = fname # used to indicate that the spikes have been saved

    def SaveWaveFile(self, fname, sids=None):
        """Save waveform data to a .wave file. Optionally, update only sids
        in existing .wave file. fname is assumed to be relative to self.sortpath"""
        s = self.sort
        try: s.wavedata
        except AttributeError: return # no wavedata to save
        if not os.path.splitext(fname)[1]: # if it doesn't have an extension
            fname = fname + '.wave'
        print('saving wave file %r' % fname)
        t0 = time.time()
        if sids != None and len(sids) >= NDIRTYSIDSTHRESH:
            sids = None # resave all of them for speed
        if sids == None: # write the whole file
            print('updating all %d spikes in wave file %r' % (s.nspikes, fname))
            f = open(join(self.sortpath, fname), 'wb')
            np.save(f, s.wavedata)
            f.close()
        else: # write only sids
            print('updating %d spikes in wave file %r' % (len(sids), fname))
            core.updatenpyfilerows(join(self.sortpath, fname), sids, s.wavedata)
        print('done saving wave file, took %.3f sec' % (time.time()-t0))
        s.wavefname = fname

    def DeleteSort(self):
        """Delete any existing Sort"""
        try:
            # TODO: if Save button is enabled, check if Sort is saved,
            # if not, prompt to save
            #print('deleting existing Sort and entries in list controls')
            #self.sort.spikes.resize(0, recheck=False) # doesn't work, doesn't own memory
            del self.sort
        except AttributeError:
            pass
        if 'Sort' in self.windows:
            sw = self.windows['Sort']
            sw.nlist.reset()
            sw.nslist.reset()
            sw.nslist.neurons = []
            sw.uslist.reset()
            sw.panel.removeAllItems()
        if 'Cluster' in self.windows:
            cw = self.windows['Cluster']
            cw.glWidget.reset()
        if 'MPL' in self.windows:
            mplw = self.windows['MPL']
            mplw.ax.clear()
            mplw.figurecanvas.draw()
        del self.cchanges[:]
        self.cci = -1
        self.ui.progressBar.setFormat('0 spikes')
        # make sure self.sort and especially self.sort.spikes is really gone
        # TODO: check if this is necessary once everything works with new streamlined
        # (no objects) spikes struct array
        gc.collect()

    def get_chans_enabled(self):
        return np.asarray([ chan for (chan, enable) in self._chans_enabled.iteritems()
                            if enable ], dtype=np.uint8)

    def set_chans_enabled(self, chans, enable=None):
        """Updates which chans are enabled in ._chans_enabled dict and in the
        plot panels, and in the highpass stream. If enable is set, chans specifies
        which chans should have their enable flag overwritten. Otherwise,
        chans specifies all the chans we want enabled.
        The code for the 2nd case is quite elaborate, such that the visibility
        state of any given plot in all plotpanels isn't needlessly toggled,
        which slows things down and causes flicker, I think"""

        # inits and checks
        try:
            allchans = self.hpstream.chans # not sure if this needs to be copy()'d or not
        except AttributeError: # no hpstream yet
            allchans = []
        if chans == None: # None means all chans
            chans = allchans
        chans = toiter(chans) # need not be contiguous
        try:
            self._chans_enabled
        except AttributeError:
            self._chans_enabled = {} #dict(zip(allchans, [ True for chan in allchans ]))

        # overwrite enable flag of chans...
        if enable != None:
            for chan in chans:
                self._chans_enabled[chan] = enable
        # ...or, leave only chans enabled
        else:
            enabledchans = [ chan for (chan, enabled) in self._chans_enabled.iteritems() if enabled==True ]
            disabledchans = [ chan for (chan, enabled) in self._chans_enabled.iteritems() if enabled==False ]
            notchans = set(allchans).difference(chans) # chans we don't want enabled
            # find the difference between currently enabled chans and the chans we want enabled
            chans2disable = set(enabledchans).difference(chans)
            # find the difference between currently disabled chans and the chans we want disabled
            chans2enable = set(disabledchans).difference(notchans)
            for chan in chans2enable:
                self._chans_enabled[chan] = True
            for chan in chans2disable:
                self._chans_enabled[chan] = False

        # now set chans in plotpanels to reset colours:
        for wintype in WINDOWUPDATEORDER:
            try:
                self.windows[wintype].panel.set_chans(self.chans_enabled)
            except KeyError: # wintype hasn't been opened yet
                pass

        # update stream
        if self.hpstream != None:
            self.hpstream.chans = self.chans_enabled
        if self.lpstream != None:
            # take intersection of lpstream.layout.chans and chans_enabled,
            # conserving ordering in lpstream.layout.chans
            self.lpstream.chans = [ chan for chan in self.lpstream.layout.chans if
                                    chan in  self.chans_enabled ]

        self.plot() # replot

    chans_enabled = property(get_chans_enabled, set_chans_enabled)

    def CloseStream(self):
        """Close data windows and stream (both hpstream and lpstream)"""
        # need to specifically get a list of keys, not an iterator,
        # since self.windows dict changes size during iteration
        for wintype in self.windows.keys():
            if wintype in ['Spike', 'Chart', 'LFP']:
                self.CloseWindow(wintype) # deletes from dict
        for stream in [self.hpstream, self.lpstream]:
            if stream: stream.close()
        self.hpstream = None
        self.lpstream = None
        self.chans_enabled = []
        self.t = None
        self.spiketw = DEFSPIKETW # reset
        self.charttw = DEFCHARTTW
        self.lfptw = DEFLFPTW
        self.ShowRasters(False) # reset
        self.updateTitle()
        self.EnableStreamWidgets(False)
        
    def CloseSortFile(self):
        self.DeleteSort()
        self.updateTitle()
        self.EnableSortWidgets(False)
        
    def RestoreClusters2GUI(self):
        """Stuff that needs to be done to synch the GUI with newly imported clusters"""
        self.UpdateClustersGUI() # restore nlist and uslist
        try:
            self.sort.spikes
            # colour points for all clusters in one shot:
            self.ColourPoints(self.sort.clusters.values())
        except AttributeError: pass # no spikes
        self.OpenWindow('Sort')

    def OpenWindow(self, wintype):
        """Create and bind a window, show it, plot its data if applicable. Much of this
        BORDER stuff is just an empirically derived hack"""
        new = wintype not in self.windows
        if new:
            if wintype == 'Spike':
                x = self.pos().x()
                y = self.pos().y() + self.size().height() + WINDOWTITLEHEIGHT
                window = SpikeWindow(parent=self, tw=self.spiketw, pos=(x, y),
                                     size=(self.SPIKEWINDOWWIDTH, SPIKEWINDOWHEIGHT))
            elif wintype == 'Chart':
                x = self.pos().x() + self.SPIKEWINDOWWIDTH + 2*BORDER
                y = self.pos().y() + self.size().height() + WINDOWTITLEHEIGHT
                window = ChartWindow(parent=self, tw=self.charttw, cw=self.spiketw,
                                     pos=(x, y), size=CHARTWINDOWSIZE)
            elif wintype == 'LFP':
                x = self.pos().x() + self.SPIKEWINDOWWIDTH + CHARTWINDOWSIZE[0] + 4*BORDER
                y = self.pos().y() + self.size().height() + WINDOWTITLEHEIGHT
                window = LFPWindow(parent=self, tw=self.lfptw, cw=self.charttw,
                                   pos=(x, y), size=LFPWINDOWSIZE)
            elif wintype == 'Sort':
                x = self.pos().x() + self.size().width() + 2*BORDER
                y = self.pos().y()
                #print('sort x: %d' % x)
                window = SortWindow(parent=self, pos=(x, y))
            elif wintype == 'Cluster':
                x = self.pos().x() + self.size().width() + self.windows['Sort'].size().width() + 4*BORDER
                y = self.pos().y()
                from cluster import ClusterWindow # can't delay this any longer
                size = (SCREENWIDTH - x - 2*BORDER, CLUSTERWINDOWHEIGHT)
                #print('cluster x: %d' % x)
                #print('cluster size: %r' % (size,))
                window = ClusterWindow(parent=self, pos=(x, y), size=size)
            elif wintype == 'MPL':
                x = self.pos().x()
                y = self.pos().y() + self.size().height() + WINDOWTITLEHEIGHT
                window = MPLWindow(parent=self, pos=(x, y),
                                   size=(self.size().width(), self.size().width()))
            self.windows[wintype] = window
            try: # try and load saved window geometry and state from sort
                window.restoreGeometry(self.sort.windowGeometries[wintype])
                window.restoreState(self.sort.windowStates[wintype])
            except(AttributeError, KeyError):
                pass
        self.ShowWindow(wintype) # just show it
        if new: # do stuff that only works after first show
            if wintype not in ['Cluster', 'MPL']:
                window.panel.draw_refs() # prevent plot artifacts
            # should be unnecessary after restoring window state above, but vsplitter
            # and hsplitter aren't restored properly, set them manually:
            if wintype == 'Sort':
                #window.mainsplitter.moveSplitter(MAINSPLITTERPOS, 1)
                window.vsplitter.moveSplitter(VSPLITTERPOS, 1)
                window.hsplitter.moveSplitter(window.hsplitter.width()-NSLISTWIDTH, 1)
        return self.windows[wintype] # 'window' isn't necessarily in local namespace

    def ShowWindow(self, wintype, enable=True):
        """Show/hide a window, force menu and toolbar states to correspond"""
        window = self.windows[wintype]
        if enable:
            window.show()
        else:
            window.hide()
        self.ui.__dict__['action%sWindow' % wintype].setChecked(enable)
        if enable and isinstance(window, DataWindow):
            # update the newly shown data window's data, in case self.t changed since
            # it was last visible
            self.plot(wintype)

    def HideWindow(self, wintype):
        self.ShowWindow(wintype, False)

    def ToggleWindow(self, wintype):
        """Toggle visibility of a data window"""
        try:
            window = self.windows[wintype]
            self.ShowWindow(wintype, not window.isVisible()) # toggle it
        except KeyError: # window hasn't been opened yet
            self.OpenWindow(wintype)

    def CloseWindow(self, wintype):
        """Hide window, remove it from windows dict, destroy it"""
        self.HideWindow(wintype)
        window = self.windows.pop(wintype)
        window.destroy()

    def ToggleRasters(self):
        """Toggle visibility of rasters"""
        enable = self.ui.actionRasters.isChecked()
        self.ShowRasters(enable)

    def ShowRasters(self, enable=True):
        """Show/hide rasters for all applicable windows. Force menu states to correspond"""
        self.ui.actionRasters.setChecked(enable)
        for wintype, window in self.windows.iteritems():
            if wintype in ['Spike', 'Chart', 'LFP']:
                window.panel.show_rasters(enable=enable)
                self.plot(wintype)

    def ToggleRef(self, ref):
        """Toggle visibility of TimeRef, VoltageRef, Scale, or the Caret"""
        enable = self.ui.__dict__['action%s' % ref].isChecked()
        self.ShowRef(ref, enable)

    def ShowRef(self, ref, enable=True):
        """Show/hide a TimeRef, VoltageRef, Scale, or the Caret. Force menu states to correspond"""
        self.ui.__dict__['action%s' % ref].setChecked(enable)
        for wintype, window in self.windows.items():
            if wintype in ['Spike', 'Chart', 'LFP', 'Sort']:
                window.panel.show_ref(ref, enable=enable)

    def SetSampfreq(self, sampfreq):
        """Set highpass stream sampling frequency, update widgets"""
        if self.hpstream != None:
            self.hpstream.sampfreq = sampfreq
            # since slider is in multiples of SLIDERTRES, doesn't need to be updated
            self.plot()
        self.ui.__dict__['action%dkHz' % (sampfreq / 1000)].setChecked(True)

    def SetSHCorrect(self, enable):
        """Set highpass stream sample & hold correct flag, update widgets"""
        if self.hpstream != None:
            self.hpstream.shcorrect = enable
        self.ui.actionSampleAndHoldCorrect.setChecked(enable)
        self.plot()

    def EnableStreamWidgets(self, enable):
        """Enable/disable all widgets that require an open stream"""
        self.ui.filePosStartButton.setEnabled(enable)
        self.ui.filePosLineEdit.setEnabled(enable)
        self.ui.filePosEndButton.setEnabled(enable)
        self.ui.slider.setEnabled(enable)
        '''
        self.menubar.Enable(wx.ID_NEW, enable)
        self.menubar.Enable(wx.ID_SPIKEWIN, enable)
        self.menubar.Enable(wx.ID_CHARTWIN, enable)
        self.menubar.Enable(wx.ID_LFPWIN, enable)
        self.menubar.Enable(wx.ID_TREF, enable)
        self.menubar.Enable(wx.ID_VREF, enable)
        self.menubar.Enable(wx.ID_CARET, enable)
        self.menubar.Enable(wx.ID_SAMPLING, enable)
        self.menubar.Enable(wx.ID_WAVEFORMS, enable)
        self.toolbar.EnableTool(wx.ID_NEW, enable)
        self.toolbar.EnableTool(wx.ID_SPIKEWIN, enable)
        self.toolbar.EnableTool(wx.ID_CHARTWIN, enable)
        self.toolbar.EnableTool(wx.ID_LFPWIN, enable)
        self.file_pos_control_panel.Show(enable)
        self.notebook.Show(enable)
        self.detect_button.Enable(enable)
        self.file_min_label.Show(enable)
        self.file_max_label.Show(enable)
        '''
    def EnableSortWidgets(self, enable):
        """Enable/disable all widgets that require a sort"""
        self.EnableSamplingMenu(not enable)
        self.ui.actionRasters.setEnabled(enable)
        self.ShowRasters(enable)
        self.ui.tabWidget.setCurrentIndex(int(enable)) # select cluster or detect tab
        '''
        self.menubar.Enable(wx.ID_SORTWIN, enable)
        self.toolbar.EnableTool(wx.ID_SORTWIN, enable)
        self.menubar.Enable(wx.ID_CLUSTERWIN, enable)
        self.toolbar.EnableTool(wx.ID_CLUSTERWIN, enable)
        self.menubar.Enable(wx.ID_SAVE, enable)
        self.toolbar.EnableTool(wx.ID_SAVE, enable)
        self.menubar.Enable(wx.ID_RASTERS, enable)
        '''
        self.EnableSpikeWidgets(enable)

    def EnableSamplingMenu(self, enable):
        """Enable/disable all items in Sampling menu, while still allowing
        the menu to be opened and its contents viewed"""
        self.ui.action25kHz.setEnabled(enable)
        self.ui.action50kHz.setEnabled(enable)
        self.ui.action100kHz.setEnabled(enable)
        self.ui.actionSampleAndHoldCorrect.setEnabled(enable)

    def EnableSpikeWidgets(self, enable):
        """Enable/disable all widgets that require the current Sort to have spikes"""
        return # do nothing for now
        '''
        try:
            if len(self.sort.spikes) == 0: enable = False # no spikes
        except AttributeError: enable = False # self.sort doesn't exist yet
        self.extract_pane.Enable(enable)
        try: self.sort.extractor
        except AttributeError: enable = False # no params extracted, or .sort doesn't exist
        self.cluster_pane.Enable(enable)
        try:
            if len(self.sort.clusters) == 0: enable = False # no clusters exist yet
        except AttributeError: enable = False
        self.cluster_params_pane.Enable(enable)
        try:
            if len(self.sort.neurons) == 0: enable = False # no neurons
        except AttributeError: enable = False # self.sort doesn't exist yet
        self.validate_pane.Enable(enable)
        '''
    def get_detector(self):
        """Create and bind Detector object, update sort from gui"""
        self.sort.detector = Detector(sort=self.sort)
        self.update_sort_from_gui()

    def update_spiketw(self, spiketw):
        """Update tw of self.sort and of Spike and Sort windows. For efficiency,
        only update sort and windows when necessary. This is appropriate
        for the user to call directly from the command line."""
        assert len(spiketw) == 2
        assert spiketw[0] < 0 and spiketw[1] > 0
        self.spiketw = spiketw
        if hasattr(self, 'sort'):
            if self.sort.tw != spiketw:
                self.sort.update_tw(spiketw)
        for wintype in ['Spike', 'Sort']:
            if wintype in self.windows:
                panel = self.windows[wintype].panel
                if panel.tw != spiketw:
                    panel.update_tw(spiketw)
 
    def update_sort_from_gui(self):
        self.update_sort_from_detector_pane()
        self.update_sort_from_cluster_pane()

    def update_sort_from_detector_pane(self):
        ui = self.ui
        det = self.sort.detector
        det.chans = self.chans_enabled
        if ui.globalFixedRadioButton.isChecked():
            threshmethod = 'GlobalFixed'
        elif ui.channelFixedRadioButton.isChecked():
            threshmethod = 'ChanFixed'
        elif ui.dynamicRadioButton.isChecked():
            threshmethod = 'Dynamic'
        else:
            raise ValueError
        det.threshmethod = threshmethod
        det.fixedthreshuV = ui.globalFixedSpinBox.value()
        det.noisemult = ui.dynamicNoiseXSpinBox.value()
        det.noisemethod = str(ui.noiseMethodComboBox.currentText())
        det.ppthreshmult = ui.vppThreshXSpinBox.value()
        det.dt = ui.dtSpinBox.value()
        det.trange = self.get_detectortrange()
        det.blocksize = int(float(ui.blockSizeLineEdit.text())) # allow exp notation
        det.lockr = ui.lockRSpinBox.value()
        det.inclr = ui.inclRSpinBox.value()

    def update_sort_from_cluster_pane(self):
        ui = self.ui
        s = self.sort
        s.sigma = ui.sigmaSpinBox.value()
        s.rmergex = ui.rmergeXSpinBox.value()
        s.rneighx = ui.rneighXSpinBox.value()
        s.alpha = ui.alphaSpinBox.value()
        s.maxgrad = ui.maxgradSpinBox.value()
        s.minpoints = ui.minpointsSpinBox.value()

    def update_gui_from_sort(self):
        ui = self.ui
        s = self.sort
        det = s.detector
        self.chans_enabled = det.chans
        # update detector pane
        meth2widget = {'GlobalFixed': ui.globalFixedRadioButton,
                       'ChanFixed': ui.channelFixedRadioButton,
                       'Dynamic': ui.dynamicRadioButton}
        meth2widget[det.threshmethod].setChecked(True)
        ui.globalFixedSpinBox.setValue(det.fixedthreshuV)
        ui.dynamicNoiseXSpinBox.setValue(det.noisemult)
        ui.noiseMethodComboBox.setCurrentIndex(ui.noiseMethodComboBox.findText(det.noisemethod))
        ui.vppThreshXSpinBox.setValue(det.ppthreshmult)
        ui.dtSpinBox.setValue(det.dt)
        ui.rangeStartLineEdit.setText(str(det.trange[0]))
        ui.rangeEndLineEdit.setText(str(det.trange[1]))
        ui.blockSizeLineEdit.setText(str(det.blocksize))
        ui.lockRSpinBox.setValue(det.lockr)
        ui.inclRSpinBox.setValue(det.inclr)
        # update cluster pane
        ui.sigmaSpinBox.setValue(s.sigma)
        ui.rmergeXSpinBox.setValue(s.rmergex)
        ui.rneighXSpinBox.setValue(s.rneighx)
        ui.alphaSpinBox.setValue(s.alpha)
        ui.maxgradSpinBox.setValue(s.maxgrad)
        ui.minpointsSpinBox.setValue(s.minpoints)

    def get_detectortrange(self):
        """Get detector time range from combo boxes, and convert
        start, now, and end to appropriate vals"""
        t0 = str(self.ui.rangeStartLineEdit.text())
        t1 = str(self.ui.rangeEndLineEdit.text())
        try:
            t0 = self.str2t[t0]
        except KeyError:
            t0 = int(float(t0)) # convert to float to allow exp notation shorthand
        try:
            t1 = self.str2t[t1]
        except KeyError:
            t1 = int(float(t1))
        return t0, t1

    def get_nearest_timepoint(self, t):
        """Round t to nearest (possibly interpolated) sample timepoint"""
        t = intround(t / self.hpstream.tres) * self.hpstream.tres
        t = min(max(t, self.range[0]), self.range[1]) # constrain to within .range
        return t

    def seek(self, t=0):
        """Seek to position in stream. t is time in us"""
        # for some reason, sometimes seek is called during spyke's shutdown process,
        # after hpstream has been removed. This prevents raising an error:
        if self.hpstream == None:
            return
        oldt = self.t
        self.t = self.get_nearest_timepoint(t)
        self.str2t['now'] = self.t # update
        # only plot if t has actually changed, though this doesn't seem to improve
        # performance, maybe mpl is already doing something like this?
        if self.t != oldt: # update controls first so they don't lag
            self.ui.filePosLineEdit.setText(str(self.t))
            if self.t % SLIDERTRES == 0: # only update slider if at a SLIDERTRES tick
                self.ui.slider.setValue(self.t // SLIDERTRES)
            self.plot()
    
    def step(self, direction):
        """Step one timepoint left or right"""
        self.seek(self.t + direction*self.hpstream.tres)

    def tell(self):
        """Return current position in surf file"""
        return self.t

    def plot(self, wintypes=None):
        """Update the contents of all the data windows, or just specific ones.
        Center each data window on self.t"""
        if wintypes == None: # update all visible windows
            wintypes = self.windows.keys()
        else: # update only specific windows, if visible
            wintypes = toiter(wintypes)
        wintypes = [ wintype for wintype in WINDOWUPDATEORDER if wintype in wintypes ] # reorder
        windows = [ self.windows[wintype] for wintype in wintypes ] # get windows in order
        for wintype, window in zip(wintypes, windows):
            if window.isVisible(): # for performance, only update if window is shown
                if wintype == 'Spike':
                    wave = self.hpstream(self.t+self.spiketw[0], self.t+self.spiketw[1])
                elif wintype == 'Chart':
                    wave = self.hpstream(self.t+self.charttw[0], self.t+self.charttw[1])
                elif wintype == 'LFP':
                    wave = self.lpstream(self.t+self.lfptw[0], self.t+self.lfptw[1])
                window.panel.plot(wave, tref=self.t) # plot it


class DataWindow(SpykeToolWindow):
    """Base data window to hold a custom spyke panel widget"""
    def setupUi(self, pos, size):
        self.setCentralWidget(self.panel)
        self.resize(*size)
        self.move(*pos)

    def step(self, direction):
        """Step left or right one caret width"""
        panelwidth = self.panel.cw[1] - self.panel.cw[0]
        spw = self.parent()
        spw.seek(spw.t + direction * panelwidth)

    def page(self, direction):
        """Page left or right one panel width"""
        panelwidth = self.panel.tw[1] - self.panel.tw[0]
        spw = self.parent()
        spw.seek(spw.t + direction * panelwidth)

    def keyPressEvent(self, event):
        spw = self.parent()
        key = event.key()
        if key == Qt.Key_Left:
            self.step(-1)
        elif key == Qt.Key_Right:
            self.step(+1)
        elif key == Qt.Key_PageUp:
            self.page(-1)
        elif key == Qt.Key_PageDown:
            self.page(+1)
        else:
            SpykeToolWindow.keyPressEvent(self, event) # pass it on


class SpikeWindow(DataWindow):
    """Window to hold the custom spike panel widget"""
    def __init__(self, parent=None, tw=None, cw=None, pos=None, size=None):
        DataWindow.__init__(self, parent)
        self.panel = SpikePanel(self, tw=tw, cw=cw)
        self.setupUi(pos, size)
        self.setWindowTitle("Spike Window")

    def step(self, direction):
        """Step left or right one sample timepoint"""
        spw = self.parent()
        spw.step(direction)

    def keyPressEvent(self, event):
        spw = self.parent()
        key = event.key()
        ctrl = event.modifiers() == Qt.ControlModifier # only modifier is ctrl
        if ctrl and key in [Qt.Key_Enter, Qt.Key_Return]:
            self.panel.reloadSelectedSpike()
        else:
            DataWindow.keyPressEvent(self, event) # pass it on


class ChartWindow(DataWindow):
    """Window to hold the custom chart panel widget"""
    def __init__(self, parent=None, tw=None, cw=None, pos=None, size=None):
        DataWindow.__init__(self, parent)
        self.panel = ChartPanel(self, tw=tw, cw=cw)
        self.setupUi(pos, size)
        self.setWindowTitle("Chart Window")


class LFPWindow(DataWindow):
    """Window to hold the custom LFP panel widget"""
    def __init__(self, parent=None, tw=None, cw=None, pos=None, size=None):
        DataWindow.__init__(self, parent)
        self.panel = LFPPanel(self, tw=tw, cw=cw)
        self.setupUi(pos, size)
        self.setWindowTitle("LFP Window")


class MPLWindow(SpykeToolWindow):
    """Matplotlib window"""
    def __init__(self, parent=None, pos=None, size=None):
        SpykeToolWindow.__init__(self, parent)
        figure = Figure()
        self.f = figure
        self.figurecanvas = FigureCanvas(figure)
        self.setCentralWidget(self.figurecanvas)
        self.toolbar = NavigationToolbar(self.figurecanvas, self, False)
        self.toolbar.setObjectName('toolbar')
        self.addToolBar(self.toolbar)
        QtCore.QObject.connect(self.toolbar, QtCore.SIGNAL("message"),
                               self.statusBar().showMessage)
        self.resize(*size)
        self.move(*pos)
        self.setWindowTitle("MPL Window")
        self.ax = figure.add_subplot(111)

class Match(object):
    """Just an object to store rmserror calculations between all clusters
    and all unsorted spikes, and also to store which cluster each spike
    matches best"""
    def __init__(self, cids=None, sids=None, errs=None):
        self.cids = cids # row labels
        self.sids = sids # column labels
        self.errs = errs # len(cids) x len(sids) error array
        self.best = {} # dict with cluster ids as keys and sids as values
        bestcidis = errs.argmin(axis=0) # of length len(sids)
        for cidi, cid in enumerate(cids):
            sidis, = np.where(bestcidis == cidi)
            self.best[cid] = sids[sidis]

    def get_best_errs(self, cid):
        """Get rmserror values between cluster cid and all the unsorted spikes
        in self.sids that match it best"""
        cidi = self.cids.searchsorted(cid)
        bestsids = self.best[cid]
        bestsidis = self.sids.searchsorted(bestsids)
        return self.errs[cidi, bestsidis]
        

if __name__ == '__main__':
    # prevents "The event loop is already running" messages when calling ipshell():
    QtCore.pyqtRemoveInputHook()
    app = QtGui.QApplication(sys.argv)
    spykewindow = SpykeWindow()
    spykewindow.show()
    sys.exit(app.exec_())
